{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Nombre del Ã¡rea de trabajo",
			"defaultValue": "asa-tfm-pgc"
		},
		"asa-tfm-pgc-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"connectionString\"de \"asa-tfm-pgc-WorkspaceDefaultSqlServer\"",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:asa-tfm-pgc.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"ls_COLI_ERP_password": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"password\"de \"ls_COLI_ERP\""
		},
		"asa-tfm-pgc-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalake1pgc.dfs.core.windows.net"
		},
		"ls_ADLG2_Bronze_Landing_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalake1pgc.dfs.core.windows.net/"
		},
		"ls_COLI_ERP_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "@{linkedService().NombreServidor}"
		},
		"ls_COLI_ERP_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "@{linkedService().NombreBaseDatos}"
		},
		"ls_COLI_ERP_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "sa"
		},
		"ls_Severless_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "asa-tfm-pgc-ondemand.sql.azuresynapse.net"
		},
		"ls_Severless_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "@{linkedService().DBName}"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/0 - Get ExecutionID')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Set year",
						"type": "SetVariable",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "",
							"value": {
								"value": "@formatDateTime(utcnow(), 'yyyy')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set day",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set month",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set month_copy1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "day",
							"value": {
								"value": "@formatDateTime(utcnow(), 'dd')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set hour",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set day_copy1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "hour",
							"value": {
								"value": "@formatDateTime(utcnow(), 'HHmmss')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set month",
						"type": "SetVariable",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [
							{
								"activity": "Set year",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "month",
							"value": {
								"value": "@formatDateTime(utcnow(), 'MM')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set executionID",
						"type": "SetVariable",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [
							{
								"activity": "Set hour",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "executionID",
							"value": {
								"value": "@concat(variables('day'), '_', variables('hour'))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set variable1",
						"type": "SetVariable",
						"dependsOn": [],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "pipelineReturnValue",
							"value": [
								{
									"key": "year",
									"value": {
										"type": "Expression",
										"content": "@formatDateTime(utcnow(), 'yyyy')"
									}
								}
							],
							"setSystemVariable": true
						}
					},
					{
						"name": "Set month_copy1",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set variable1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "pipelineReturnValue",
							"value": [
								{
									"key": "month",
									"value": {
										"type": "Expression",
										"content": "@formatDateTime(utcnow(), 'MM')"
									}
								}
							],
							"setSystemVariable": true
						}
					},
					{
						"name": "Set day_copy1",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set day",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "pipelineReturnValue",
							"value": [
								{
									"key": "day",
									"value": {
										"type": "Expression",
										"content": "@formatDateTime(utcnow(), 'dd')"
									}
								}
							],
							"setSystemVariable": true
						}
					},
					{
						"name": "Set hour_copy1",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set hour",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "pipelineReturnValue",
							"value": [
								{
									"key": "hour",
									"value": {
										"type": "Expression",
										"content": "@formatDateTime(utcnow(), 'HHmmss')"
									}
								}
							],
							"setSystemVariable": true
						}
					},
					{
						"name": "Set executionID_copy1",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set hour_copy1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "pipelineReturnValue",
							"value": [
								{
									"key": "executionID",
									"value": {
										"type": "Expression",
										"content": "@concat(variables('day'), '_', variables('hour'))"
									}
								}
							],
							"setSystemVariable": true
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"variables": {
					"year": {
						"type": "String"
					},
					"month": {
						"type": "String"
					},
					"day": {
						"type": "String"
					},
					"hour": {
						"type": "String"
					},
					"executionID": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/1 - SqlToLanding')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "List of Tables to Load",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "csvConfiguration",
								"type": "DatasetReference",
								"parameters": {
									"NombreCSV": "ConfiguracionOrigenes.csv"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Save Landing Files",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "List of Tables to Load",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('List of tables to load').output.value",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Active For Load",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@not(equals(trim(item().Load), 'N'))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Load Landing",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "SqlServerSource",
														"sqlReaderQuery": {
															"value": "SELECT *, @{formatDateTime(pipeline().TriggerTime,'yyyyMMdd')} AS fechaCarga, '@{pipeline().RunId}' AS pipelineID \nFROM @{item().SchemaName}.@{item().TableName} \nWHERE @{item().IncrementalColumn} >= CAST(CAST('@{item().UpdateDate}' AS nvarchar) AS datetime2);",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings",
															"copyBehavior": "FlattenHierarchy"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"parallelCopies": 1,
													"dataIntegrationUnits": 4,
													"translator": {
														"type": "TabularTranslator",
														"typeConversion": true,
														"typeConversionSettings": {
															"allowDataTruncation": true,
															"treatBooleanAsNumber": false
														}
													}
												},
												"inputs": [
													{
														"referenceName": "dsSQLGenerico",
														"type": "DatasetReference",
														"parameters": {
															"NombreServidor": "@item().ServerName",
															"NombreBD": "@item().DataBaseName",
															"NombreEsquema": "@item().SchemaName",
															"NombreTabla": "@item().TableName",
															"FechaActualizacion": "@item().UpdateDate",
															"ColumnaIncremental": {
																"value": "@item().IncrementalColumn",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "dsParquetRaw",
														"type": "DatasetReference",
														"parameters": {
															"NombreFichero": "@concat( item().FileName, '_', formatDateTime(utcnow(), 'yyyyMMdd'),'.', item().Format)"
														}
													}
												]
											}
										]
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-16T00:20:34Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/csvConfiguration')]",
				"[concat(variables('workspaceId'), '/datasets/dsSQLGenerico')]",
				"[concat(variables('workspaceId'), '/datasets/dsParquetRaw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/2 - BronzeToSilver')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get Metadata Landing",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "connectionString",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsRawEntidades",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "ParquetReadSettings"
							}
						}
					},
					{
						"name": "UPSERT Silver",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "landingFiles",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "MergeLandingFilesToSilver",
								"type": "NotebookReference"
							},
							"parameters": {
								"data_lake_container": {
									"value": {
										"value": "@pipeline().parameters.ADLG2",
										"type": "Expression"
									},
									"type": "string"
								},
								"silver_folder": {
									"value": {
										"value": "@pipeline().parameters.SilverFolder",
										"type": "Expression"
									},
									"type": "string"
								},
								"landing_files": {
									"value": {
										"value": "@string(variables('landingFiles'))",
										"type": "Expression"
									},
									"type": "string"
								},
								"connection_string": {
									"value": {
										"value": "@variables('connectionString')",
										"type": "Expression"
									},
									"type": "string"
								},
								"bronze_error": {
									"value": {
										"value": "@pipeline().parameters.BronzeFolderError",
										"type": "Expression"
									},
									"type": "string"
								},
								"bronze_processed": {
									"value": {
										"value": "@pipeline().parameters.BronzeFolderProcessed",
										"type": "Expression"
									},
									"type": "string"
								},
								"bronze_logs": {
									"value": {
										"value": "@pipeline().parameters.BronzeFolderLogs",
										"type": "Expression"
									},
									"type": "string"
								},
								"silver_logs": {
									"value": {
										"value": "@pipeline().parameters.SilverFolderLogs",
										"type": "Expression"
									},
									"type": "string"
								},
								"container_name": {
									"value": {
										"value": "@pipeline().parameters.ContainerName",
										"type": "Expression"
									},
									"type": "string"
								},
								"bronze_pending": {
									"value": {
										"value": "@pipeline().parameters.BronzeFolderPending",
										"type": "Expression"
									},
									"type": "string"
								},
								"metadata_folder": {
									"value": {
										"value": "@pipeline().parameters.MetadataFolder",
										"type": "Expression"
									},
									"type": "string"
								},
								"bronze_landing": {
									"value": {
										"value": "@pipeline().parameters.BronzeFolderLanding",
										"type": "Expression"
									},
									"type": "string"
								},
								"year": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'yyyy')",
										"type": "Expression"
									},
									"type": "string"
								},
								"month": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'MM')",
										"type": "Expression"
									},
									"type": "string"
								},
								"day": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'dd')",
										"type": "Expression"
									},
									"type": "string"
								},
								"hour": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'HHmmss')",
										"type": "Expression"
									},
									"type": "string"
								},
								"executionID": {
									"value": {
										"value": "@concat(formatDateTime(utcnow(), 'dd'), '_', formatDateTime(utcnow(), 'HHmmss'))",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkTFM",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Small",
							"numExecutors": 2
						}
					},
					{
						"name": "landingFiles",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get Metadata Landing",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "landingFiles",
							"value": {
								"value": "@activity('Get Metadata Landing').output.childItems",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Get ConnectionString",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "dsConnectionString",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "connectionString",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get ConnectionString",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "connectionString",
							"value": {
								"value": "@activity('Get ConnectionString').output.value[0].conn\n",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"ADLG2": {
						"type": "string",
						"defaultValue": "abfss://mdw@datalake1pgc.dfs.core.windows.net"
					},
					"BronzeFolderPending": {
						"type": "string",
						"defaultValue": "mdw/bronze/Landing/Pending"
					},
					"SilverFolder": {
						"type": "string",
						"defaultValue": "silver/DeltaTables"
					},
					"BronzeFolderError": {
						"type": "string",
						"defaultValue": "mdw/bronze/Landing/Error"
					},
					"BronzeFolderProcessed": {
						"type": "string",
						"defaultValue": "mdw/bronze/Processed"
					},
					"BronzeFolderLogs": {
						"type": "string",
						"defaultValue": "mdw/bronze/Logs"
					},
					"SilverFolderLogs": {
						"type": "string",
						"defaultValue": "mdw/silver/Logs"
					},
					"ContainerName": {
						"type": "string",
						"defaultValue": "mdw"
					},
					"MetadataFolder": {
						"type": "string",
						"defaultValue": "mdw/bronze/Configuration"
					},
					"BronzeFolderLanding": {
						"type": "string",
						"defaultValue": "bronze/Landing/Pending"
					},
					"year": {
						"type": "string"
					},
					"month": {
						"type": "string"
					},
					"day": {
						"type": "string"
					},
					"hour": {
						"type": "string"
					},
					"executionID": {
						"type": "string"
					}
				},
				"variables": {
					"landingFiles": {
						"type": "Array",
						"defaultValue": [
							"a"
						]
					},
					"connectionString": {
						"type": "String",
						"defaultValue": "a"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsRawEntidades')]",
				"[concat(variables('workspaceId'), '/notebooks/MergeLandingFilesToSilver')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkTFM')]",
				"[concat(variables('workspaceId'), '/datasets/dsConnectionString')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/3 - Load SCD2 and Fact')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "LoadSCD2DimsAndFact",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "tablesToLoad",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "LoadSCD2AndFact",
								"type": "NotebookReference"
							},
							"parameters": {
								"data_lake_container": {
									"value": {
										"value": "@pipeline().parameters.ADLG2",
										"type": "Expression"
									},
									"type": "string"
								},
								"connection_string": {
									"value": {
										"value": "@variables('connectionString')",
										"type": "Expression"
									},
									"type": "string"
								},
								"gold_folder_logs": {
									"value": {
										"value": "@pipeline().parameters.GoldFolderLogs",
										"type": "Expression"
									},
									"type": "string"
								},
								"container_name": {
									"value": {
										"value": "@pipeline().parameters.ContainerName",
										"type": "Expression"
									},
									"type": "string"
								},
								"metadata_folder": {
									"value": {
										"value": "@pipeline().parameters.MetadataFolder",
										"type": "Expression"
									},
									"type": "string"
								},
								"tables_to_load": {
									"value": {
										"value": "@string(variables('tablesToLoad'))",
										"type": "Expression"
									},
									"type": "string"
								},
								"golden_folder_dimensions": {
									"value": {
										"value": "@pipeline().parameters.GoldenFolderDimensions",
										"type": "Expression"
									},
									"type": "string"
								},
								"golden_folder_fact": {
									"value": {
										"value": "@pipeline().parameters.GoldenFolderfact",
										"type": "Expression"
									},
									"type": "string"
								},
								"year": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'yyyy')",
										"type": "Expression"
									},
									"type": "string"
								},
								"month": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'MM')",
										"type": "Expression"
									},
									"type": "string"
								},
								"day": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'dd')",
										"type": "Expression"
									},
									"type": "string"
								},
								"hour": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'HHmmss')",
										"type": "Expression"
									},
									"type": "string"
								},
								"executionID": {
									"value": {
										"value": "@concat(formatDateTime(utcnow(), 'dd'), '_', formatDateTime(utcnow(), 'HHmmss'))",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkTFM",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Small",
							"numExecutors": 2
						}
					},
					{
						"name": "Search ActiveForLoad",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "connectionString",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "csvConfiguration",
								"type": "DatasetReference",
								"parameters": {
									"NombreCSV": "ConfiguracionReporting.csv"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "tablesToLoad",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Search ActiveForLoad",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "tablesToLoad",
							"value": {
								"value": "@activity('Search ActiveForLoad').output.value",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Get ConnectionString",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "dsConnectionString",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "connectionString",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get ConnectionString",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "connectionString",
							"value": {
								"value": "@activity('Get ConnectionString').output.value[0].conn\n",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"ADLG2": {
						"type": "string",
						"defaultValue": "abfss://mdw@datalake1pgc.dfs.core.windows.net"
					},
					"GoldFolderLogs": {
						"type": "string",
						"defaultValue": "gold/Logs"
					},
					"ContainerName": {
						"type": "string",
						"defaultValue": "mdw"
					},
					"MetadataFolder": {
						"type": "string",
						"defaultValue": "mdw/bronze/Configuration"
					},
					"GoldenFolderDimensions": {
						"type": "string",
						"defaultValue": "gold/Dimensions"
					},
					"GoldenFolderfact": {
						"type": "string",
						"defaultValue": "gold/Fact"
					},
					"year": {
						"type": "string"
					},
					"month": {
						"type": "string"
					},
					"day": {
						"type": "string"
					},
					"hour": {
						"type": "string"
					},
					"executionID": {
						"type": "string"
					}
				},
				"variables": {
					"tablesToLoad": {
						"type": "Array",
						"defaultValue": [
							"a"
						]
					},
					"connectionString": {
						"type": "String",
						"defaultValue": "a"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/LoadSCD2AndFact')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkTFM')]",
				"[concat(variables('workspaceId'), '/datasets/csvConfiguration')]",
				"[concat(variables('workspaceId'), '/datasets/dsConnectionString')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Orchestrator')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Bronze",
						"type": "ExecutePipeline",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [
							{
								"activity": "Get exeuctionID",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "1 - SqlToLanding",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "Silver",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Bronze",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "2 - BronzeToSilver",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"executionID": {
									"value": "@activity('Get exeuctionID').output.pipelineReturnValue.executionID",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "Gold",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Silver",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "3 - Load SCD2 and Fact",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"executionID": {
									"value": "@activity('Get exeuctionID').output.pipelineReturnValue.executionID",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "Get exeuctionID",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "0 - Get ExecutionID",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/1 - SqlToLanding')]",
				"[concat(variables('workspaceId'), '/pipelines/2 - BronzeToSilver')]",
				"[concat(variables('workspaceId'), '/pipelines/3 - Load SCD2 and Fact')]",
				"[concat(variables('workspaceId'), '/pipelines/0 - Get ExecutionID')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvConfiguration')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreCSV": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreCSV",
							"type": "Expression"
						},
						"folderPath": "bronze/Configuration",
						"fileSystem": "mdw"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ServerName;DataBaseName;SchemaName;TableName;PathName;FileName;Load",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsConfiguration')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "bronze/Configuration",
						"fileSystem": "mdw"
					},
					"columnDelimiter": ";",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ServerName;DataBaseName;SchemaName;TableName;PathName;FileName;Load",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsConnectionString')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "conn.csv",
						"folderPath": "bronze/Configuration",
						"fileSystem": "mdw"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "conn",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsGoldDimensions')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "gold/Dimensions",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsGoldFact')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "gold/Fact",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetError')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					},
					"NombreCarpeta": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@concat('bronze/Error/', dataset().NombreCarpeta)",
							"type": "Expression"
						},
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetProcessed')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					},
					"NombreCarpeta": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@concat('bronze/Processed/', dataset().NombreCarpeta)",
							"type": "Expression"
						},
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetRaw')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": "bronze/Landing/Pending",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsRawEntidades')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "bronze/Landing/Pending",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSQLGenerico')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_COLI_ERP",
					"type": "LinkedServiceReference",
					"parameters": {
						"NombreServidor": {
							"value": "@dataset().NombreServidor",
							"type": "Expression"
						},
						"NombreBaseDatos": {
							"value": "@dataset().NombreBD",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"NombreServidor": {
						"type": "string"
					},
					"NombreBD": {
						"type": "string"
					},
					"NombreEsquema": {
						"type": "string"
					},
					"NombreTabla": {
						"type": "string"
					},
					"FechaActualizacion": {
						"type": "string"
					},
					"ColumnaIncremental": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServerTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().NombreEsquema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().NombreTabla",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_COLI_ERP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSilverFact')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "silver/Fact",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSilverSCD')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "silver/SCD",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-tfm-pgc-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('asa-tfm-pgc-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-tfm-pgc-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('asa-tfm-pgc-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_ADLG2_Bronze_Landing')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ls_ADLG2_Bronze_Landing_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_COLI_ERP')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"NombreServidor": {
						"type": "string"
					},
					"NombreBaseDatos": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServer",
				"typeProperties": {
					"server": "[parameters('ls_COLI_ERP_properties_typeProperties_server')]",
					"database": "[parameters('ls_COLI_ERP_properties_typeProperties_database')]",
					"encrypt": "mandatory",
					"trustServerCertificate": true,
					"authenticationType": "SQL",
					"userName": "[parameters('ls_COLI_ERP_properties_typeProperties_userName')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('ls_COLI_ERP_password')]"
					}
				},
				"connectVia": {
					"referenceName": "IR-Coli-ERP",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/IR-Coli-ERP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_Severless')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"server": "[parameters('ls_Severless_properties_typeProperties_server')]",
					"database": "[parameters('ls_Severless_properties_typeProperties_database')]",
					"encrypt": "mandatory",
					"trustServerCertificate": false,
					"authenticationType": "SystemAssignedManagedIdentity"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Carga 04_00')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Orchestrator",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2024-10-24T00:44:00",
						"timeZone": "Romance Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								4
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Orchestrator')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IR-Coli-ERP')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential1')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {
					"resourceId": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.ManagedIdentity/userAssignedIdentities/asa-tfm-pgc"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure BackuplessSTG')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Configuration"
				},
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity' \nGO\n\nCREATE USER [asa-tfm-pgc] FROM EXTERNAL PROVIDER;\nGO\nALTER ROLE db_datareader ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_datawriter ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_owner ADD MEMBER [asa-tfm-pgc];\nGO\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA bk AUTHORIZATION dbo\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "BackuplessDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure GoldelessSTG')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Configuration"
				},
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity' \nGO\nCREATE DATABASE SCOPED CREDENTIAL [https://datalake1pgc.dfs.core.windows.net] \nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'SAS Token';\nGO\nCREATE USER [asa-tfm-pgc] FROM EXTERNAL PROVIDER;\nGO\nALTER ROLE db_datareader ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_datawriter ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_owner ADD MEMBER [asa-tfm-pgc];\nGO\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA dim AUTHORIZATION dbo\nGO\nCREATE SCHEMA fact AUTHORIZATION dbo\nGO\nCREATE SCHEMA gold AUTHORIZATION dbo\n\nCREATE SCHEMA etl AUTHORIZATION dbo",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure ReportingDB')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Configuration"
				},
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity'\nGO\nSELECT * FROM sys.database_scoped_credentials;\n\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA dim AUTHORIZATION dbo;\nGO\nCREATE SCHEMA fact AUTHORIZATION dbo;\nGO\n\n\n\nCREATE USER [powerBIdata] from EXTERNAL PROVIDER\nALTER ROLE db_datareader ADD MEMBER [powerBI];\n-- 1. Crear un login a nivel de servidor para el usuario\nCREATE LOGIN powerBIuser WITH PASSWORD = 'Laburo99.';\n\n-- 2. Cambiar el contexto a la base de datos donde deseas crear el usuario\n\n\n-- 3. Crear el usuario dentro de la base de datos basada en el login creado\nCREATE USER powerBI FOR LOGIN powerBIuser;\nCREATE USER powerBIdata FOR LOGIN powerBIuser;\n\n-- 4. Asignar permisos especÃ­ficos o roles al usuario\n-- Por ejemplo, otorgar permisos de lectura y escritura\nALTER ROLE db_datareader ADD MEMBER powerBI;\nALTER ROLE db_datawriter ADD MEMBER powerBI;\n\n-- O para darle permisos mÃ¡s amplios como propietario de la base de datos:\n-- ALTER ROLE db_owner ADD MEMBER nuevo_usuario;\n\nSELECT * FROM sys.database_principals WHERE name = 'powerBI';\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ReportingDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Dimensions Gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Views Silver"
				},
				"content": {
					"query": "---------------\n-- Dim Date --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Date] AS \nSELECT  idFechas AS idDate,\n        ClaveFecha AS dateKey,\n        Fecha AS date,\n        COALESCE(Mes, 'D')  AS month,\n        COALESCE(NumeroMes, -1)  AS monthNumber,\n        COALESCE(Ano, -1)  AS year,\n        COALESCE(NumeroSemana, -1)  AS weekNumber,\n        COALESCE(DiaSemana, 'D') AS dayWeek,\n        COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\n        COALESCE(DiaAno, -1) AS yearDay,\n        CASE WHEN Trimestre = 1 THEN 'Q1'\n             WHEN Trimestre = 2 THEN 'Q2'\n             WHEN Trimestre = 3 THEN 'Q3'\n             ELSE '-1' END AS quarter,\n        CASE WHEN Cuatrimestre = 1 THEN 'Q1'\n             WHEN Cuatrimestre = 2 THEN 'Q2'\n             WHEN Cuatrimestre = 3 THEN 'Q3'\n             WHEN Cuatrimestre = 4 THEN 'Q4'\n             ELSE '-1' \n        END                                       AS quadrimester,\n        CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\n            WHEN Cuatrimestre IN (2,4) THEN 'S2'\n            ELSE '-1' \n        END                                       AS semester,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[fechas]\n\n\n------------------\n-- Dim Articles --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Articles] AS\nSELECT  idArticulos                                                                         AS idArticles,\n        COALESCE(nombre, 'D')                                                               AS name,\n        COALESCE(descripcion, 'D')                                                          AS description,\n        COALESCE(codigoReferencia, 'D')                                                     AS externalCode,\n        COALESCE(t.talla, 'D')                                                              AS size,\n        COALESCE( t.numeroTalla, -1)                                                        AS numSize,\n        COALESCE(co.color, 'D')                                                             AS colour,\n        COALESCE(ca.categoria, 'D')                                                         AS category,\n        COALESCE(l.codigoLinea, 'D')                                                        AS codLine,\n        COALESCE(l.Linea, 'D')                                                              AS line, \n        CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN 'OI'+CAST((a.idTemporada) AS nvarchar)\n             WHEN a.idCategoria IN (2,4,6,7,9) THEN 'OI'+CAST(a.idTemporada AS nvarchar)\n             ELSE 'PV'+CAST(a.idTemporada AS nvarchar)\n        END                                                                                 AS season,\n        a.fechaCarga                                                                        AS loadDate,\n        a.fechaDelta                                                                        AS deltaDate\nFROM [default].[dbo].[articulos] AS a\nLEFT JOIN [default].[dbo].[talla] AS t\n    ON t.idTalla = a.idTalla\nLEFT JOIN [default].[dbo].[color] AS co\n   ON co.idColor = a.idColor\nLEFT JOIN [default].[dbo].[categoria] AS ca -- select * from [default].[dbo].[categoria]\n    ON ca.idCategoria = a.idCategoria\nLEFT JOIN [default].[dbo].[Linea] AS l\n    ON l.idLinea = a.idLinea\n--where  a.fechaCarga  < '20241009'\n\n\n-------------------\n-- Dim Warehouse --\n-------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Warehouse] AS\nSELECT  idAlmacenes AS idWarehouse,\n        COALESCE(a.Nombre, 'D') AS warehouse,\n        COALESCE(a.codigoAlmacen, 'D') AS externalCode,\n        COALESCE(p.codigoPais, 'D') AS countryCode, \n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(a.ciudad, 'D') AS city,\n        COALESCE(a.Direccion, 'D') AS address,\n        COALESCE(a.descripcion, 'D') AS description,\n        a.fechaCarga AS loadDate,\n        a.fechaDelta AS deltaDate\nFROM [default].[dbo].[almacenes] AS a\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.idpais = a.idPais\n  -- WHERE a.fechaDelta <> 20241011\n--UNION \n--select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\n--UNION \n--select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\n\n\n---------------------\n-- Dim Postal Code --\n---------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_PostalCodes] AS\nSELECT  idCodigoPostal AS idPostalCodes,\n        COALESCE(codigoPostal, 'D') AS postalCode,\n        COALESCE(region, 'D') AS region,\n        COALESCE(c.codigoPais, 'D') AS countryCode,\n        COALESCE(nombre, 'D') AS country,\n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[codigoPostal] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.codigoPais = c.codigoPais\n\n------------------\n-- Dim Currency --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Currency] AS\nSELECT  idDivisa AS idCurrency,\n        COALESCE(nombre, 'D') AS name,\n        COALESCE(Divisa, 'D') AS currency,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[divisa]\n\n---------------\n-- Dim Hours --\n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Hours] AS\nSELECT  idHoras AS idHours,\n        COALESCE(Hora, -1) AS hour,\n        COALESCE(Minuto, -1) AS minute,\n        COALESCE(HoraCompleta, 'D') as fullHour,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[horas]\n\n\n----------------\n-- Dim Tariff --\n----------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Tariff] AS\nSELECT  idTarifa AS idTariff,\n        COALESCE(Tarifa, 'D') AS tariff,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate       \nFROM [default].[dbo].[tarifa]\n\n\n------------------------\n-- Dim Operation Type --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_OperationType] AS\nSELECT  idTipoOperacion AS idOperationType,\n        COALESCE(Operacion, 'D') AS operation,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[tipoOperacion]\n\n\n------------------------\n-- Dim Client --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Client] AS\nSELECT  idCliente AS idClient,\n        COALESCE(c.nombre, 'D') AS name,\n        COALESCE(apellido1, 'D') AS lastName1,\n        COALESCE(apellido2, 'D') AS lastName2, \n        COALESCE(email, 'D') AS email,\n        COALESCE(telefono, 'D') AS phoneNumber,\n        COALESCE(cumpleanos, '1900-01-01') AS birthDay,       \n        DATEDIFF(YEAR, cumpleanos, GETDATE()) \n        - CASE \n            WHEN MONTH(GETDATE()) < MONTH('1983-12-04') \n                 OR (MONTH(GETDATE()) = MONTH('1983-12-04') AND DAY(GETDATE()) < DAY('1983-12-04'))\n            THEN 1\n            ELSE 0\n        END AS age,\n        CASE WHEN hombre = 1 THEN 'Hombre'\n        ELSE 'Mujer' \n        END AS gender,\n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(p.codigoPais, 'D') AS countryCode,\n        COALESCE(cp.region, 'D') AS region,\n        COALESCE(c.direcion, 'D') AS address,\n        COALESCE(cp.codigoPostal, 'D') AS postalCode,\n        COALESCE(activo, 0) AS active,       \n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[cliente] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    On c.idPais = c.idPais\nINNER JOIN [default].[dbo].[codigoPostal] AS cp\n    ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Dimensions Silver')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Views Silver"
				},
				"content": {
					"query": "---------------\n-- Dim Date --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Date] AS \nSELECT  idFechas AS idDate,\n        ClaveFecha AS dateKey,\n        Fecha AS date,\n        COALESCE(Mes, 'D')  AS month,\n        COALESCE(NumeroMes, -1)  AS monthNumber,\n        COALESCE(Ano, -1)  AS year,\n        COALESCE(NumeroSemana, -1)  AS weekNumber,\n        COALESCE(DiaSemana, 'D') AS dayWeek,\n        COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\n        COALESCE(DiaAno, -1) AS yearDay,\n        CASE WHEN Trimestre = 1 THEN 'Q1'\n             WHEN Trimestre = 2 THEN 'Q2'\n             WHEN Trimestre = 3 THEN 'Q3'\n             ELSE '-1' END AS quarter,\n        CASE WHEN Cuatrimestre = 1 THEN 'Q1'\n             WHEN Cuatrimestre = 2 THEN 'Q2'\n             WHEN Cuatrimestre = 3 THEN 'Q3'\n             WHEN Cuatrimestre = 4 THEN 'Q4'\n             ELSE '-1' \n        END                                       AS quadrimester,\n        CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\n            WHEN Cuatrimestre IN (2,4) THEN 'S2'\n            ELSE '-1' \n        END                                       AS semester,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[fechas]\n\n\n------------------\n-- Dim Articles --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Articles] AS\nSELECT  idArticulos                                                                         AS idArticles,\n        COALESCE(nombre, 'D')                                                               AS name,\n        COALESCE(descripcion, 'D')                                                          AS description,\n        COALESCE(codigoReferencia, 'D')                                                     AS externalCode,\n        COALESCE(t.talla, 'D')                                                              AS size,\n        COALESCE( t.numeroTalla, -1)                                                        AS numSize,\n        COALESCE(co.color, 'D')                                                             AS colour,\n        COALESCE(ca.categoria, 'D')                                                         AS category,\n        COALESCE(l.codigoLinea, 'D')                                                        AS codLine,\n        COALESCE(l.Linea, 'D')                                                              AS line, \n        CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN 'OI'+CAST((a.idTemporada) AS nvarchar)\n             WHEN a.idCategoria IN (2,4,6,7,9) THEN 'OI'+CAST(a.idTemporada AS nvarchar)\n             ELSE 'PV'+CAST(a.idTemporada AS nvarchar)\n        END                                                                                 AS season,\n        a.fechaCarga                                                                        AS loadDate,\n        a.fechaDelta                                                                        AS deltaDate\nFROM [default].[dbo].[articulos] AS a\nLEFT JOIN [default].[dbo].[talla] AS t\n    ON t.idTalla = a.idTalla\nLEFT JOIN [default].[dbo].[color] AS co\n   ON co.idColor = a.idColor\nLEFT JOIN [default].[dbo].[categoria] AS ca -- select * from [default].[dbo].[categoria]\n    ON ca.idCategoria = a.idCategoria\nLEFT JOIN [default].[dbo].[Linea] AS l\n    ON l.idLinea = a.idLinea\n--where  a.fechaCarga  < '20241009'\n\n\n-------------------\n-- Dim Warehouse --\n-------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Warehouse] AS\nSELECT  idAlmacenes AS idWarehouse,\n        COALESCE(a.Nombre, 'D') AS warehouse,\n        COALESCE(a.codigoAlmacen, 'D') AS externalCode,\n        COALESCE(p.codigoPais, 'D') AS countryCode, \n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(a.ciudad, 'D') AS city,\n        COALESCE(a.Direccion, 'D') AS address,\n        COALESCE(a.descripcion, 'D') AS description,\n        a.fechaCarga AS loadDate,\n        a.fechaDelta AS deltaDate\nFROM [default].[dbo].[almacenes] AS a\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.idpais = a.idPais\n  -- WHERE a.fechaDelta <> 20241011\n--UNION \n--select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\n--UNION \n--select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\n\n\n---------------------\n-- Dim Postal Code --\n---------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_PostalCodes] AS\nSELECT  idCodigoPostal AS idPostalCodes,\n        COALESCE(codigoPostal, 'D') AS postalCode,\n        COALESCE(region, 'D') AS region,\n        COALESCE(c.codigoPais, 'D') AS countryCode,\n        COALESCE(nombre, 'D') AS country,\n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[codigoPostal] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.codigoPais = c.codigoPais\n\n------------------\n-- Dim Currency --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Currency] AS\nSELECT  idDivisa AS idCurrency,\n        COALESCE(nombre, 'D') AS name,\n        COALESCE(Divisa, 'D') AS currency,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[divisa]\n\n---------------\n-- Dim Hours --\n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Hours] AS\nSELECT  idHoras AS idHours,\n        COALESCE(Hora, -1) AS hour,\n        COALESCE(Minuto, -1) AS minute,\n        COALESCE(HoraCompleta, 'D') as fullHour,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[horas]\n\n\n----------------\n-- Dim Tariff --\n----------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Tariff] AS\nSELECT  idTarifa AS idTariff,\n        COALESCE(Tarifa, 'D') AS tariff,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate       \nFROM [default].[dbo].[tarifa]\n\n\n------------------------\n-- Dim Operation Type --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_OperationType] AS\nSELECT  idTipoOperacion AS idOperationType,\n        COALESCE(Operacion, 'D') AS operation,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[tipoOperacion]\n\n\n------------------------\n-- Dim Client --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Client] AS\nSELECT  idCliente AS idClient,\n        COALESCE(c.nombre, 'D') AS name,\n        COALESCE(apellido1, 'D') AS lastName1,\n        COALESCE(apellido2, 'D') AS lastName2, \n        COALESCE(email, 'D') AS email,\n        COALESCE(telefono, 'D') AS phoneNumber,\n        COALESCE(cumpleanos, '1900-01-01') AS birthDay,       \n        DATEDIFF(YEAR, cumpleanos, GETDATE()) \n        - CASE \n            WHEN MONTH(GETDATE()) < MONTH('1983-12-04') \n                 OR (MONTH(GETDATE()) = MONTH('1983-12-04') AND DAY(GETDATE()) < DAY('1983-12-04'))\n            THEN 1\n            ELSE 0\n        END AS age,\n        CASE WHEN hombre = 1 THEN 'Hombre'\n        ELSE 'Mujer' \n        END AS gender,\n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(p.codigoPais, 'D') AS countryCode,\n        COALESCE(cp.region, 'D') AS region,\n        COALESCE(c.direcion, 'D') AS address,\n        COALESCE(cp.codigoPostal, 'D') AS postalCode,\n        COALESCE(activo, 0) AS active,       \n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[cliente] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    On c.idPais = c.idPais\nINNER JOIN [default].[dbo].[codigoPostal] AS cp\n    ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Fact Gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Views Silver"
				},
				"content": {
					"query": "---------------\n-- Fact Sales --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_fact_Sales] AS \nSELECT  idDesgloseVenta AS idSales,\n        d.idArticulo AS idArticles,\n        c.idAlmacen AS idWarehouse,\n        c.idCliente AS idClient,\n        c.idCodigoPostal AS idPostalCodes,\n        c.idDivisa AS idCurrency,\n        c.idTarifa AS idTariff,\n        c.idTipoOperacion AS idOperationType,\n        c.idHora AS idHours,\n        c.idFecha AS idDate,\n        f.fecha AS  date,\n        CAST(CAST(f.fecha AS nvarchar(10)) + ' ' + h.horaCompleta AS datetime2)AS dateOp,\n        c.codigoTicket As ticketNumber,\n        d.Cantidad AS quantity,\n        d.PrecioUnitario AS unitPrice,\n        d.CosteUnitario AS unitCost,\n        d.importeBruto AS amtTotalEuros,\n        d.importeNeto AS amtNetEuros,\n        d.importeDescuento AS amtTotalEurosDiscount,\n        d.importeNetoDescuento AS amtNetEurosDiscount,\n        CASE WHEN idTarifa IN (2,3) THEN 1\n             ELSE 0 \n        END AS discountSale,\n        CASE WHEN idTarifa = 0 THEN 0\n            ELSE (d.importeBruto - d.importeDescuento) \n        END AS amtTotalDiscount,\n        CASE WHEN idTarifa = 0 THEN 0 \n             ELSE (d.importeNeto - d.importeNetoDescuento) \n        END AS amtNetDiscount,\n        d.fechaCarga AS loadDate,\n        d.fechaDelta AS deltaDate\nFROM [default].[dbo].[desgloseVenta] AS d\nINNER JOIN [default].[dbo].[cabeceraVenta] AS c\n    ON d.idcabecerasVentas = c.idcabeceraVenta\nLEFT JOIN [default].[dbo].[Fechas]  AS f\n    On f.idFechas = c.idFecha\nLEFT JOIN [default].[dbo].[horas]  AS h\n    On h.idHoras = c.idHora",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Fact Silver')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Views Silver"
				},
				"content": {
					"query": "---------------\n-- Fact Sales --  \n---------------\nGO\n--CREATE OR ALTER VIEW [etl].[vw_fact_Sales] AS \nSELECT top 100 idDesgloseVenta AS idSales,\n        d.idArticulo AS idArticles,\n        c.idAlmacen AS idWarehouse,\n        c.idCliente AS idClient,\n        c.idCodigoPostal AS idPostalCodes,\n        c.idDivisa AS idCurrency,\n        c.idTarifa AS idTariff,\n        c.idTipoOperacion AS idOperationType,\n        c.idHora AS idHours,\n        c.idFecha AS idDate,\n        f.fecha AS  date,\n        CAST(CAST(f.fecha AS nvarchar(10)) + ' ' + h.horaCompleta AS datetime2)AS dateOp,\n        COALESCE(c.codigoTicket, 'D') AS ticketNumber,\n        COALESCE(d.Cantidad, 0) AS quantity,\n        COALESCE(d.PrecioUnitario, 0)  AS unitPrice,\n        COALESCE(d.CosteUnitario, 0)  AS unitCost,\n        COALESCE(d.importeBruto, 0)  AS amtTotalEuros,\n        COALESCE(d.importeNeto, 0)  AS amtNetEuros,\n        COALESCE(d.importeDescuento, 0)  AS amtTotalEurosDiscount,\n        COALESCE(d.importeNetoDescuento, 0) AS amtNetEurosDiscount,\n        CASE WHEN idTarifa IN (2,3) THEN 1\n             ELSE 0 \n        END AS discountSale,\n        CASE WHEN idTarifa = 0 THEN 0\n            ELSE (d.importeBruto - d.importeDescuento) \n        END AS amtTotalDiscount,\n        CASE WHEN idTarifa = 0 THEN 0 \n             ELSE (d.importeNeto - d.importeNetoDescuento) \n        END AS amtNetDiscount,\n        d.fechaCarga AS loadDate,\n        d.fechaDelta AS deltaDate\nFROM [default].[dbo].[desgloseVenta] AS d\nINNER JOIN [default].[dbo].[cabeceraVenta] AS c\n    ON d.idcabecerasVentas = c.idcabeceraVenta\nLEFT JOIN [default].[dbo].[Fechas]  AS f\n    On f.idFechas = c.idFecha\nLEFT JOIN [default].[dbo].[horas]  AS h\n    On h.idHoras = c.idHora",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Reporting')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- ==============================================================================\n-- Vistas de la capa Gold para el informe\n-- ==============================================================================\n\n-- Arreglar dateop en noteboobk\n\nCREATE OR ALTER VIEW fact.Sales AS \nSELECT  s.skSales,\n        s.idSales,\n        s.idArticles,\n        skArticles,\n        s.idWarehouse,\n        skWarehouse,\n        s.idClient,\n        skClient, \n        s.idPostalCode,\n        skPostalCode, \n        s.idCurrency,\n        skCurrency,\n        s.idTariff,\n        skTariff, \n        s.idOperationType,\n        skOperationType,\n        s.idHours,\n        skHours,\n        s.idDate,\n        skDate,\n        s.date,\n        s.dateOp,\n        s.ticketNumber,\n        s.quantity,\n        s.unitPrice,\n        s.unitCost,\n        s.amtTotalEuros,\n        s.amtNetEuros,\n        s.amtTotalEurosDiscount,\n        s.amtNetEurosDiscount,\n        s.discountSale,\n        s.amtTotalDiscount,\n        s.amtNetDiscount\nFROM gold.dbo.fact_Sales AS s\n\n\nCREATE OR ALTER VIEW dim.Articles AS\nSELECT  -1 AS skArticles, \n        -1 AS idArticles, \n        'D' AS name, \n        'D' AS description, \n        'D' AS externalCode, \n        'D' AS size, \n        -1  AS numSize, \n        'D' AS colour, \n        'D' AS category, \n        'D' AS codLine, \n        'D' AS line, \n        'D' AS season\n\nUNION \n\nSELECT  skarticles, \n        idarticles, \n        name, \n        description, \n        externalCode, \n        size, \n        numSize, \n        colour, \n        category, \n        codLine, \n        line, \n        season\nFROM gold.dbo.dim_articles;\n\nCREATE OR ALTER VIEW dim.Client AS\nSELECT  -1 AS skClient,\n        -1 AS idClient,\n        'D' AS name,\n        'D' AS lastName1,\n        'D' AS lastName2,\n        'D' AS email,\n        'D' AS phoneNumber,\n        'D' AS birthDay,\n        -1 AS age,\n        'D' AS gender,\n        'D' AS country,\n        'D' AS countryCode,\n        'D' AS region,\n        'D' AS address,\n        'D' AS postalCode,\n        0 AS active\nUNION\nSELECT  skClient,\n        idClient,\n        name,\n        lastName1,\n        lastName2,\n        email,\n        phoneNumber,\n        birthDay,\n        age,\n        gender,\n        country,\n        countryCode,\n        region,\n        address,\n        postalCode,\n        active\nFROM gold.dbo.dim_client\n\nCREATE OR ALTER VIEW dim.Currency AS\nSELECT  -1 AS skCurrency,\n        -1 AS idCurrency,\n        'D' AS name,\n        'D' AS currency\nUNION\nSELECT  skCurrency,\n        idCurrency,\n        name,\n        currency\nFROM gold.dbo.dim_currency\n\nCREATE OR ALTER VIEW dim.Date AS\nSELECT  -1 AS skDate,\n        -1 AS idDate,\n        -1 AS datekey,\n        CAST('1990-01-01' AS datetime2(7)) AS date,\n        'D' AS month,\n        -1 AS monthNumber,\n        -1 AS year,\n        -1 AS weekNumber,\n        'D' AS dayWeek,\n        -1 AS dayWeekNumber,\n        -1 AS yearDay,\n        'D' AS quarter,\n        'D' AS quadrimester,\n        'D' AS semester\nUNION\nSELECT  skDate,\n        idDate,\n        datekey,\n        date,\n        month,\n        monthNumber,\n        year,\n        weekNumber,\n        dayWeek,\n        dayWeekNumber,\n        yearDay,\n        quarter,\n        quadrimester,\n        semester\nFROM gold.dbo.dim_date\n\nCREATE OR ALTER VIEW dim.Hours AS\nSELECT  -1 AS skHours,\n        -1 AS idHours,\n        -1 AS hour,\n        -1 AS minute,\n        'D' AS fullHour\nUNION\nSELECT  skHours,\n        idHours,\n        hour,\n        minute,\n        fullHour\nFROM gold.dbo.dim_hours\n\n\nCREATE OR ALTER VIEW dim.OperationType AS\nSELECT  -1 AS skOperationType,\n        -1 AS idOperationType,\n        'D' AS operation\nUNION \nSELECT  skOperationType,\n        idOperationType,\n        operation\nFROM gold.dbo.dim_operationtype\n\n\nCREATE OR ALTER VIEW dim.PostalCode AS\nSELECT  -1 AS skPostalCode,\n        -1 AS idPostalCode,\n        'D' AS postalCode,\n        'D' AS region,\n        'D' AS countryCode,\n        'D' AS country\nUNION\nSELECT  skPostalCode,\n        idPostalCode,\n        postalCode,\n        region,\n        countryCode,\n        country\nFROM gold.dbo.dim_postalcode\n\n\nCREATE OR ALTER VIEW dim.Tariff AS\nSELECT  -1 AS skTariff,\n        -1 AS idTariff,\n        'D' AS tariff\nUNION\nSELECT  skTariff,\n        idTariff,\n        tariff\nFROM gold.dbo.dim_tariff\n\n\nCREATE OR ALTER VIEW dim.Warehouse AS\nSELECT  -1 AS skWarehouse,\n        -1 AS idWarehouse,\n        'D' AS warehouse,\n        'D' AS externalCode,\n        'D' AS countryCode,\n        'D' AS country,\n        'D' AS city,\n        'D' AS address,\n        'D' AS description\nUNION\nSELECT  skWarehouse,\n        idWarehouse,\n        warehouse,\n        externalCode,\n        countryCode,\n        country,\n        city,\n        address,\n        description\nFROM gold.dbo.dim_warehouse",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ReportingDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/First Load Gold CETAS SCD2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Gold Dim CETAs"
				},
				"content": {
					"query": "---------------\n-- Articles  --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Articles]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles') )\n    DROP EXTERNAL TABLE dim.Articles \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idArticles) idSkArticles,\n\tidArticles,\n\tname,\n\texternalcode,\n\tsize,\n\tnumSize,\n\tcolour,\n\tcategory, \n\tcodLine,\n\tline, \n\tdescription,\n\tseason, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent, \n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idArticles AS NVARCHAR)+ name + description COLLATE DATABASE_DEFAULT + externalcode + size + CAST(numSize AS NVARCHAR) + colour + category + codLine + line + season)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Articles]\n\n\n--------------\n-- Client --\n--------------\n-- drop EXTERNAL TABLE [dim].[Client]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Client \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Client]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkClient, \n\t-1  \t\t AS idClient, \n\t'D' \t\t AS name,\n\t'D' \t\t AS lastName1,\n\t'D' \t\t AS lastName2,\n\t'D' AS email,\n\t'D' AS phoneNumber,\n\tCAST('1900-01-01' AS date) AS birthDay,       \n\t-1 AS age,\n\t'D' AS gender,\n\t'D' AS country,\n\t'D' AS countryCode,\n\t'D'AS region,\n\t'D' AS address,\n\t'D' AS postalCode,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' +'D' + CAST('1900-01-01' AS nvarchar) + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idClient) AS idSkClient,\n\tidClient,\n\tname,\n\tlastName1,\n\tlastName2, \n\temail,\n\tphoneNumber,\n\tbirthDay,       \n\tage,\n\tgender,\n\tcountry,\n\tcountryCode,\n\tregion,\n\taddress,\n\tpostalCode,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idClient AS NVARCHAR) + name COLLATE DATABASE_DEFAULT + lastName1 COLLATE DATABASE_DEFAULT + lastName2 COLLATE DATABASE_DEFAULT\n\t+ email COLLATE DATABASE_DEFAULT + phoneNumber COLLATE DATABASE_DEFAULT + CAST(birthDay AS nvarchar) + CAST(age AS nvarchar) + gender + country + countryCode\n\t+ region + address + postalCode)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Client]\n\n\n--------------\n-- Currency --\n--------------\n-- drop EXTERNAL TABLE [dim].[Currency]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Currency \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idCurrency) AS idSkCurrency,\n\tidCurrency,\n\tname,\n\tcurrency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idCurrency AS NVARCHAR) + name + currency)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Currency]\n\n\n----------\n-- Date --\n----------\n-- drop EXTERNAL TABLE [dim].[Date]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Date') )\n    DROP EXTERNAL TABLE dim.Date \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n    HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n    + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Date]\n\n\n-----------\n-- Hours --\n-----------\n-- drop EXTERNAL TABLE [dim].[Hours]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Hours') )\n    DROP EXTERNAL TABLE dim.Hours \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Hours]\n\n\n-------------------\n-- OperationType --\n-------------------\n-- drop EXTERNAL TABLE [dim].[OperationType]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.OperationType') )\n    DROP EXTERNAL TABLE dim.OperationType \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_OperationType]\n\n\n-----------------\n-- PostalCodes --\n-----------------\n-- drop EXTERNAL TABLE [dim].[PostalCodes]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.PostalCodes') )\n    DROP EXTERNAL TABLE dim.PostalCodes \n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idPostalCodes, \n\t'D' \t\t AS postalCode,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_PostalCodes]\n\n\n------------\n-- Tariff --\n------------\n-- drop EXTERNAL TABLE [dim].[Tariff]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Tariff') )\n    DROP EXTERNAL TABLE dim.Tariff \n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Tariff]\n\n\n---------------\n-- Warehouse --\n---------------\n-- drop EXTERNAL TABLE [dim].[Warehouse]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Warehouse') )\n    DROP EXTERNAL TABLE dim.Warehouse \n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idWarehouse) AS idSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Warehouse]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load SCD2 Silver CETAs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Silver SCD + Fact"
				},
				"content": {
					"query": "---------------\n-- Articles --\n---------------\n-- DROP EXTERNAL TABLE  scd.Articles\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Articles') )\n    DROP EXTERNAL TABLE scd.Articles;\n\n\nCREATE EXTERNAL TABLE  scd.Articles \nWITH\n(\n\tLOCATION = 'silver/SCD/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n    \n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Articles]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idArticles AS NVARCHAR)+ s.name + s.description COLLATE DATABASE_DEFAULT + s.externalcode + s.size + CAST(s.numSize AS NVARCHAR) + s.colour + s.category + s.codLine + s.line + s.season)) AS [$hash]\n    FROM etl.vw_dim_Articles s\n    LEFT JOIN current_data c\n        ON s.idArticles = c.idArticles\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkArticles), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Articles]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idArticles) + max_key AS new_idSkArticles, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idArticles = c.idArticles\n    CROSS JOIN max_surrogate_key\n    WHERE c.idArticles IS NULL  OR (c.[$hash] != chg.[$hash] )\n), --select * from new_or_updated_data order by idarticles\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkarticles AS new_idSkArticles,\n        c.idArticles,\n        c.name,\n        c.externalCode,\n        c.size,\n        c.numSize,\n        c.colour,\n        c.category,\n        c.codLine,\n        c.line,\n        c.description,\n        c.season,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idArticles = chg.idArticles\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkArticles, -- Mantener la clave subrogada original\n    c.idArticles,\n    c.name,\n    c.externalCode,\n    c.size,\n    c.numSize,\n    c.colour,\n    c.category,\n    c.codLine,\n    c.line,\n    c.description,\n    c.season,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idArticles = chg.idArticles\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkArticles;\n\n\n---------------\n-- Currency --\n---------------\n-- DROP EXTERNAL TABLE  scd.Currency\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Currency') )\n    DROP EXTERNAL TABLE scd.Currency;\n\n\nCREATE EXTERNAL TABLE  scd.Currency \nWITH\n(\n\tLOCATION = 'silver/SCD/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Currency]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idCurrency AS NVARCHAR) + s.name + s.currency)) AS [$hash]\n    FROM etl.vw_dim_Currency s\n    LEFT JOIN current_data c\n        ON s.idCurrency = c.idCurrency\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkCurrency), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Currency]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idCurrency) + max_key AS new_idSkCurrency, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idCurrency = c.idCurrency\n    CROSS JOIN max_surrogate_key\n    WHERE c.idCurrency IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkCurrency AS new_idSkCurrency,\n        c.idCurrency,\n        c.name,\n        c.currency,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idCurrency = chg.idCurrency\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkCurrency, -- Mantener la clave subrogada original\n    c.idCurrency,\n    c.name,\n    c.currency,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idCurrency = chg.idCurrency\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkCurrency;\n\n\n------------\n-- Client --\n------------\n--DROP EXTERNAL TABLE  scd.Client\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Client') )\n   DROP EXTERNAL TABLE scd.Client   \n\n\nCREATE EXTERNAL TABLE  scd.Client \nWITH\n(\n\tLOCATION = 'silver/SCD/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Client]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idClient AS NVARCHAR) + s.name COLLATE DATABASE_DEFAULT + s.lastName1 COLLATE DATABASE_DEFAULT + s.lastName2 COLLATE DATABASE_DEFAULT\n\t    + s.email COLLATE DATABASE_DEFAULT + s.phoneNumber COLLATE DATABASE_DEFAULT + CAST(s.birthDay AS nvarchar) + CAST(s.age AS nvarchar) + s.gender + s.country + s.countryCode\n\t    + s.region + s.address + s.postalCode)) AS [$hash]\n    FROM etl.vw_dim_Client s\n    LEFT JOIN current_data c\n        ON s.idClient = c.idClient\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkClient), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Client]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idClient) + max_key AS new_idSkClient, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idClient = c.idClient\n    CROSS JOIN max_surrogate_key\n    WHERE c.idClient IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkClient AS new_idSkClient,\n        c.idClient,\n        c.name,\n        c.lastName1,\n        c.lastName2, \n        c.email,\n        c.phoneNumber,\n        c.birthDay,       \n        c.age,\n        c.gender,\n        c.country,\n        c.countryCode,\n        c.region,\n        c.address,\n        c.postalCode,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idClient = chg.idClient\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkClient, -- Mantener la clave subrogada original\n    c.idClient,\n    c.name,\n    c.lastName1,\n    c.lastName2, \n    c.email,\n    c.phoneNumber,\n    c.birthDay,       \n    c.age,\n    c.gender,\n    c.country,\n    c.countryCode,\n    c.region,\n    c.address,\n    c.postalCode,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idClient = chg.idClient\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkClient;\n\n\n----------\n-- Date --\n----------\n--DROP EXTERNAL TABLE  scd.Date \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Date') )\n   DROP EXTERNAL TABLE scd.Date;  \n\n\nCREATE EXTERNAL TABLE  scd.Date \nWITH\n(\n\tLOCATION = 'silver/SCD/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Date]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idDate AS NVARCHAR) + CAST(s.dateKey AS NVARCHAR) + CAST(s.date AS NVARCHAR) + s.month COLLATE DATABASE_DEFAULT + CAST(s.monthNumber AS NVARCHAR) + CAST(s.year AS NVARCHAR) \n        + CAST(s.weekNumber AS NVARCHAR) + s.dayWeek COLLATE DATABASE_DEFAULT + CAST(s.dayWeekNumber AS NVARCHAR) + CAST(s.yearDay AS NVARCHAR) + s.quarter + s.quadrimester + s.semester )) AS [$hash]\n    FROM etl.vw_dim_Date s\n    LEFT JOIN current_data c\n        ON s.idDate = c.idDate\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkDate), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Date]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idDate) + max_key AS new_idSkDate, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idDate = c.idDate\n    CROSS JOIN max_surrogate_key\n    WHERE c.idDate IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkDate AS new_idSkDate,\n        c.idDate,\n        c.dateKey,\n        c.date,\n        c.month,\n        c.monthNumber,\n        c.year,\n        c.weekNumber,\n        c.dayWeek,\n        c.dayWeekNumber,\n        c.yearDay,\n        c.quarter,\n        c.quadrimester,\n        c.semester,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idDate = chg.idDate\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkDate, -- Mantener la clave subrogada original\n    c.idDate,\n\tc.dateKey,\n\tc.date,\n\tc.month,\n\tc.monthNumber,\n\tc.year,\n\tc.weekNumber,\n\tc.dayWeek,\n\tc.dayWeekNumber,\n\tc.yearDay,\n\tc.quarter,\n\tc.quadrimester,\n\tc.semester,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idDate = chg.idDate\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkDate;\n\n\n---------------\n-- Hours --\n---------------\n-- DROP EXTERNAL TABLE  scd.Hours\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Hours') )\n   DROP EXTERNAL TABLE scd.Hours;   \n\n\nCREATE EXTERNAL TABLE  scd.Hours \nWITH\n(\n\tLOCATION = 'silver/SCD/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Hours]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idHours AS NVARCHAR) + CAST(s.hour AS NVARCHAR) + CAST(s.minute AS NVARCHAR) + s.fullHour)) AS [$hash]\n    FROM etl.vw_dim_Hours s\n    LEFT JOIN current_data c\n        ON s.idHours = c.idHours\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkHours), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Hours]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idHours) + max_key AS new_idSkHours, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idHours = c.idHours\n    CROSS JOIN max_surrogate_key\n    WHERE c.idHours IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkHours AS new_idSkHours,\n        c.idHours,\n        c.hour,\n        c.minute,\n        c.fullHour,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idHours = chg.idHours\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkHours, -- Mantener la clave subrogada original\n\tc.idHours,\n\tc.hour,\n\tc.minute,\n\tc.fullHour,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idHours = chg.idHours\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkHours;\n\n\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE  scd.OperationType \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.OperationType') )\n   DROP EXTERNAL TABLE scd.OperationType;   \n\n\nCREATE EXTERNAL TABLE  scd.OperationType \nWITH\n(\n\tLOCATION = 'silver/SCD/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[OperationType]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idOperationType AS NVARCHAR) + s.operation)) AS [$hash]\n    FROM etl.vw_dim_OperationType s\n    LEFT JOIN current_data c\n        ON s.idOperationType = c.idOperationType\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkOperationType), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[OperationType]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idOperationType) + max_key AS new_idSkOperationType, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idOperationType = c.idOperationType\n    CROSS JOIN max_surrogate_key\n    WHERE c.idOperationType IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkOperationType AS new_idSkOperationType,\n        c.idOperationType,\n        c.operation,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idOperationType = chg.idOperationType\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkOperationType, -- Mantener la clave subrogada original\n    c.idOperationType,\n    c.operation,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idOperationType = chg.idOperationType\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkOperationType,\n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkOperationType, \n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkOperationType;\n\n\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE  scd.PostalCodes \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.PostalCodes') )\n    DROP EXTERNAL TABLE scd.PostalCodes; \n\nCREATE EXTERNAL TABLE  scd.PostalCodes \nWITH\n(\n\tLOCATION = 'silver/SCD/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idPostalCodes AS NVARCHAR) + s.postalCode + s.region + s.countryCode + s.country)) AS [$hash]\n    FROM etl.vw_dim_PostalCodes s\n    LEFT JOIN current_data c\n        ON s.idPostalCodes = c.idPostalCodes\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkPostalCodes), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idPostalCodes) + max_key AS new_idSkPostalCodes, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idPostalCodes = c.idPostalCodes\n    CROSS JOIN max_surrogate_key\n    WHERE c.idPostalCodes IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkPostalCodes AS new_idSkPostalCodes,\n        c.idPostalCodes,\n        c.postalCode,\n        c.region,\n        c.countryCode,\n        c.country,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idPostalCodes = chg.idPostalCodes\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkPostalCodes, -- Mantener la clave subrogada original\n    c.idPostalCodes,\n    c.postalCode,\n    c.region,\n    c.countryCode,\n    c.country,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idPostalCodes = chg.idPostalCodes\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkPostalCodes,\n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkPostalCodes, \n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkPostalCodes;\n\n\n\n------------\n-- Tariff --\n------------\n--DROP EXTERNAL TABLE  scd.Tariff \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Tariff') )\n   DROP EXTERNAL TABLE scd.Tariff;   \n\n\nCREATE EXTERNAL TABLE  scd.Tariff \nWITH\n(\n\tLOCATION = 'silver/SCD/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Tariff]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idTariff AS NVARCHAR) + s.tariff)) AS [$hash]\n    FROM etl.vw_dim_Tariff s\n    LEFT JOIN current_data c\n        ON s.idTariff = c.idTariff\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkTariff), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Tariff]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idTariff) + max_key AS new_idSkTariff, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idTariff = c.idTariff\n    CROSS JOIN max_surrogate_key\n    WHERE c.idTariff IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkTariff AS new_idSkTariff,\n        c.idTariff,\n        c.tariff,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idTariff = chg.idTariff\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkTariff, -- Mantener la clave subrogada original\n    c.idTariff,\n    c.tariff,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idTariff = chg.idTariff\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkTariff;\n\n\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE  scd.Warehouse \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Warehouse') )\n   DROP EXTERNAL TABLE scd.Warehouse;   \n\n\nCREATE EXTERNAL TABLE  scd.Warehouse \nWITH\n(\n\tLOCATION = 'silver/SCD/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.idWarehouse,\n        s.warehouse,\n        s.externalCode,\n        s.countryCode,\n        s.country,\n        s.city,\n        s.address,\n        s.description,\n        s.loadDate,\n        s.deltaDate,\n        HASHBYTES('SHA2_256',(CAST(s.idWarehouse AS NVARCHAR) + s.warehouse + s.externalcode + s.countryCode + s.country + s.city + s.address + s.description)) AS [$hash]\n    FROM etl.vw_dim_Warehouse s\n    LEFT JOIN current_data c\n        ON s.idWarehouse = c.idWarehouse\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkWarehouse), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idWarehouse) + max_key AS new_idSkWarehouse, -- Asignar nueva clave subrogada\n        chg.idWarehouse,\n        chg.warehouse,\n        chg.externalCode,\n        chg.countryCode,\n        chg.country,\n        chg.city,\n        chg.address,\n        chg.description,\n        chg.loadDate,\n        chg.deltaDate,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate,\n        chg.[$hash]\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idWarehouse = c.idWarehouse\n    CROSS JOIN max_surrogate_key\n    WHERE c.idWarehouse IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkWarehouse AS new_idSkWarehouse,\n        c.idWarehouse,\n        c.warehouse,\n        c.externalCode,\n        c.countryCode,\n        c.country,\n        c.city,\n        c.address,\n        c.description,\n        c.loadDate,\n        c.deltaDate,\n        c.isCurrent,\n        c.fromDate,\n        c.toDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idWarehouse = chg.idWarehouse\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\nfinal AS (\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nSELECT\n    c.idSkWarehouse, -- Mantener la clave subrogada original\n    c.idWarehouse,\n    c.warehouse,\n    c.externalCode,\n    c.countryCode,\n    c.country,\n    c.city,\n    c.address,\n    c.description,\n    c.loadDate,\n    c.deltaDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.fromDate,\n    GETDATE() AS toDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idWarehouse = chg.idWarehouse\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM unchanged_data\n\n)\nselect     idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM final\nORDER BY idSkWarehouse;\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load Silver Fact')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Silver SCD + Fact"
				},
				"content": {
					"query": "-- EXEC silver.FactSales\n-- ==============================================================================\n-- Procedimiento almacenado para cargar la tabla de hechos Sales en SilverlessSTG\n-- ==============================================================================\n\n\nCREATE OR ALTER PROCEDURE silver.FactSales\nWITH ENCRYPTION AS \n\n--------------\n-- Sales  --\n---------------\n-- DROP EXTERNAL TABLE [silver].[Sales]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('silver.Sales') )\n    DROP EXTERNAL TABLE silver.Sales \n\nCREATE  EXTERNAL TABLE SilverlessSTG.[silver].[Sales]\nWITH\n(\n\tLOCATION = 'silver/Fact/Sales', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT\n    -1 AS idSkSales,\n    -1 AS idSales,\n    -1 AS idArticles,\n    -1 AS idWarehouse,\n    -1 AS idClient,\n    -1 AS idPostalCodes,\n    -1 AS idCurrency,\n    -1 AS idTariff,\n    -1 AS idOperationType,\n    -1 AS idHours,\n    -1 AS idDate,\n    CAST('1990-01-01' AS datetime2) AS date,\n    CAST('1990-01-01' AS datetime2) dateOp,\n    'D' AS ticketNumber,\n    -1 AS quantity,\n    -1 AS unitPrice,\n    -1 AS unitCost,\n    -1 AS amtTotalEuros,\n    -1 AS amtNetEuros,\n    -1 AS amtTotalEurosDiscount,\n    -1 AS amtNetEurosDiscount,\n    -1 AS discountSale,\n    -1 AS amtTotalDiscount,\n    -1 AS amtNetDiscount,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1' + '-1' +'-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '1990-01-01' + '1990-01-01' + 'D' \n    + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1')) AS [$hash]\nUNION\nSELECT  ROW_NUMBER() OVER (ORDER BY idSales) AS idSkSales,\n        *, \n        HASHBYTES('SHA2_256',( CAST(idSales AS nvarchar) + CAST(idArticles AS nvarchar) + CAST(idWarehouse AS nvarchar) + CAST(idClient AS nvarchar) \n        + CAST(idPostalCodes AS nvarchar) + CAST(idCurrency AS nvarchar) + CAST(idTariff AS nvarchar) + CAST(idOperationType AS nvarchar) \n        + CAST(idHours AS nvarchar) + CAST(idDate AS nvarchar) + CAST(date AS nvarchar) + CAST(dateOp AS nvarchar) + ticketNumber \n        + CAST(quantity AS nvarchar) + CAST(unitPrice AS nvarchar) + CAST(unitCost AS nvarchar) + CAST(amtTotalEuros AS nvarchar) \n        + CAST(amtNetEuros AS nvarchar) + CAST(amtTotalEurosDiscount AS nvarchar) + CAST(amtNetEurosDiscount AS nvarchar) + CAST(discountSale AS nvarchar) \n        + CAST(amtTotalDiscount AS nvarchar) + CAST(amtNetDiscount AS nvarchar) )) AS [$hash]\nFROM [SilverlessSTG].[etl].[vw_fact_Sales]\n) a ORDER BY idSkSales",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Re-Load Fact')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Gold Fact CETAs"
				},
				"content": {
					"query": "-- EXEC gold.FactSales\n-- ==============================================================================\n-- Procedimiento almacenado para cargar la tabla de hechos Sales en GoldenlessSTG\n-- ==============================================================================\n\n\nCREATE OR ALTER PROCEDURE gold.FactSales\nWITH ENCRYPTION AS \n\n--------------\n-- Sales  --\n---------------\n-- DROP EXTERNAL TABLE [fact].[Sales]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('fact.Sales') )\n    DROP EXTERNAL TABLE fact.Sales \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[fact].[Sales]\nWITH\n(\n\tLOCATION = 'gold/Fact/Sales', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT\n    -1 AS idSkSales,\n    -1 AS idSales,\n    -1 AS idArticles,\n    -1 AS idskArticles,\n    -1 AS idWarehouse,\n    -1 AS idskWarehouse,\n    -1 AS idClient,\n    -1 AS idskClient,\n    -1 AS idPostalCodes,\n    -1 AS idskPostalCodes,\n    -1 AS idCurrency,\n    -1 AS idskCurrency,\n    -1 AS idTariff,\n    -1 AS idskTariff,\n    -1 AS idOperationType,\n    -1 AS idskOperationType,\n    -1 AS idHours,\n    -1 AS idskHours,\n    -1 AS idDate,\n    -1 AS idskDate,\n    CAST('1990-01-01' AS datetime2) AS date,\n    CAST('1990-01-01' AS datetime2) dateOp,\n    'D' AS ticketNumber,\n    -1 AS quantity,\n    -1 AS unitPrice,\n    -1 AS unitCost,\n    -1 AS amtTotalEuros,\n    -1 AS amtNetEuros,\n    -1 AS amtTotalEurosDiscount,\n    -1 AS amtNetEurosDiscount,\n    -1 AS discountSale,\n    -1 AS amtTotalDiscount,\n    -1 AS amtNetDiscount,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate\n\t--HASHBYTES('SHA2_256',('-1' + '-1' +'-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '1990-01-01' + '1990-01-01' + 'D' \n    --+ '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1')) AS [$hash]\nUNION\n\nSELECT \n    s.idSkSales,\n    s.idSales,\n    s.idArticles,\n    COALESCE(a.idSkArticles, -1) AS idSkArticles,\n    s.idWarehouse,\n    COALESCE(w.idSkWarehouse, -1) AS idSkWarehouse,\n    s.idClient,\n    COALESCE(cl.idSkClient, -1) AS idSkClient, \n    s.idPostalCodes,\n    COALESCE(p.idSkPostalCodes, -1) AS idSkPostalCodes, \n    s.idCurrency,\n    COALESCE(cu.idSkCurrency, -1) AS idSkCurrency,\n    s.idTariff,\n    COALESCE(t.idSkTariff, -1) AS idSkTariff, \n    s.idOperationType,\n    COALESCE(o.idSkOperationType, -1) AS idSkOperationType,\n    s.idHours,\n    COALESCE(h.idSkHours, -1) AS idSkHours,\n    s.idDate,\n    COALESCE(d.idSkDate, -1) AS idSkDate,\n    s.date,\n    s.dateOp,\n    s.ticketNumber,\n    s.quantity,\n    s.unitPrice,\n    s.unitCost,\n    s.amtTotalEuros,\n    s.amtNetEuros,\n    s.amtTotalEurosDiscount,\n    s.amtNetEurosDiscount,\n    s.discountSale,\n    s.amtTotalDiscount,\n    s.amtNetDiscount,\n    s.loadDate,\n    s.deltaDate\nFROM [SilverlessSTG].[silver].[Sales] AS s\nLEFT JOIN [GoldenlessDWH].[dim].[Articles] AS a\n    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Client] AS cl\n    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Currency] AS cu\n    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Date] AS d\n    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Hours] AS h\n    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[OperationType] AS o\n    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[PostalCodes] AS p\n    ON s.idPostalCodes = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Tariff] AS t\n    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Warehouse] AS w\n    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\n\n\n\n\n\n) a ORDER BY idSkSales",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Re-Load Gold CETAs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Gold Dim CETAs"
				},
				"content": {
					"query": "---------------\n-- Articles  --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Articles]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles') )\n    DROP EXTERNAL TABLE dim.Articles \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash]\nUNION\nSELECT *\nFROM [SilverlessSTG].[scd].[Articles]\n) a ORDER BY idSkArticles\n\n\n--------------\n-- Client--\n--------------\n-- DROP EXTERNAL TABLE [dim].[Client]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Client') )\n    DROP EXTERNAL TABLE dim.Client \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Client]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkClient, \n\t-1  \t\t AS idClient, \n\t'D' \t\t AS name,\n\t'D' \t\t AS lastName1,\n\t'D' \t\t AS lastName2,\n\t'D' AS email,\n\t'D' AS phoneNumber,\n\tCAST('1900-01-01' AS date) AS birthDay,       \n\t-1 AS age,\n\t'D' AS gender,\n\t'D' AS country,\n\t'D' AS countryCode,\n\t'D'AS region,\n\t'D' AS address,\n\t'D' AS postalCode,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' +'D' + CAST('1900-01-01' AS nvarchar) + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Client]\n) C ORDER BY idSkClient\n\n\n--------------\n-- Currency --\n--------------\n-- DROP EXTERNAL TABLE [dim].[Currency]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Currency \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2)  AS toDate,\t\t\n\t1 \t\t\t AS isCurrent, \t\t \t\t\t \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Currency]\n) C ORDER BY idSkCurrency\n\n\n----------\n-- Date --\n----------\n-- DROP EXTERNAL TABLE [dim].[Date]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Date') )\n    DROP EXTERNAL TABLE dim.Date \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n        HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n        + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Date]\n) d ORDER BY idSkDate\n\n-----------\n-- Hours --\n-----------\n-- DROP EXTERNAL TABLE [dim].[Hours]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Hours') )\n    DROP EXTERNAL TABLE dim.Hours \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Hours]\n) h ORDER BY idSkHours\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE [dim].[OperationType]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.OperationType') )\n    DROP EXTERNAL TABLE dim.OperationType \nGO\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_OperationType]\n) o ORDER BY idSkOperationType\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE [dim].[PostalCodes]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.PostalCodes') )\n    DROP EXTERNAL TABLE dim.PostalCodes \nGO\n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idpostalCode, \n\t'D' \t\t AS postalCodes,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_PostalCodes]\n) p ORDER BY idSkPostalCodes\n\n------------\n-- Tariff --\n------------\n-- DROP EXTERNAL TABLE [dim].[Tariff]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Tariff') )\n    DROP EXTERNAL TABLE dim.Tariff \n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Tariff]\n) T ORDER BY idSkTariff\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Warehouse]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Warehouse') )\n    DROP EXTERNAL TABLE dim.Warehouse \nGO\n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM ( \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tidSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\tisCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Warehouse]\n) W ORDER BY idSkWarehouse",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SP Reload Dim Gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- EXEC gold.ReloadDimSCD\n\nCREATE OR ALTER PROCEDURE gold.ReloadDimSCD\nWITH ENCRYPTION AS \n\n---------------\n-- Articles  --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Articles]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles'))\n    DROP EXTERNAL TABLE dim.Articles \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash],\n\txxhash64()\nUNION\nSELECT *\nFROM [SilverlessSTG].[scd].[Articles]\n) a ORDER BY idSkArticles\n\n\n--------------\n-- Client--\n--------------\n-- DROP EXTERNAL TABLE [dim].[Client]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Client') )\n    DROP EXTERNAL TABLE dim.Client \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Client]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkClient, \n\t-1  \t\t AS idClient, \n\t'D' \t\t AS name,\n\t'D' \t\t AS lastName1,\n\t'D' \t\t AS lastName2,\n\t'D' AS email,\n\t'D' AS phoneNumber,\n\tCAST('1900-01-01' AS date) AS birthDay,       \n\t-1 AS age,\n\t'D' AS gender,\n\t'D' AS country,\n\t'D' AS countryCode,\n\t'D'AS region,\n\t'D' AS address,\n\t'D' AS postalCode,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' +'D' + CAST('1900-01-01' AS nvarchar) + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Client]\n) C ORDER BY idSkClient\n\n\n--------------\n-- Currency --\n--------------\n-- DROP EXTERNAL TABLE [dim].[Currency]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Currency \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2)  AS toDate,\t\t\n\t1 \t\t\t AS isCurrent, \t\t \t\t\t \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Currency]\n) C ORDER BY idSkCurrency\n\n\n----------\n-- Date --\n----------\n-- DROP EXTERNAL TABLE [dim].[Date]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Date') )\n    DROP EXTERNAL TABLE dim.Date \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n        HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n        + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Date]\n) d ORDER BY idSkDate\n\n-----------\n-- Hours --\n-----------\n-- DROP EXTERNAL TABLE [dim].[Hours]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Hours') )\n    DROP EXTERNAL TABLE dim.Hours \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Hours]\n) h ORDER BY idSkHours\n\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE [dim].[OperationType]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.OperationType') )\n    DROP EXTERNAL TABLE dim.OperationType \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[OperationType]\n) o ORDER BY idSkOperationType\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE [dim].[PostalCodes]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.PostalCodes') )\n    DROP EXTERNAL TABLE dim.PostalCodes \n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idpostalCode, \n\t'D' \t\t AS postalCodes,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[PostalCodes]\n) p ORDER BY idSkPostalCodes\n\n------------\n-- Tariff --\n------------\n-- DROP EXTERNAL TABLE [dim].[Tariff]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Tariff') )\n    DROP EXTERNAL TABLE dim.Tariff \n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Tariff]\n) T ORDER BY idSkTariff\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Warehouse]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Warehouse') )\n    DROP EXTERNAL TABLE dim.Warehouse \n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM ( \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tidSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\tisCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Warehouse]\n) W ORDER BY idSkWarehouse",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SP Silver Dim SCD2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Silver SCD + Fact"
				},
				"content": {
					"query": "CREATE OR ALTER PROCEDURE silver.DimSCD\nWITH ENCRYPTION AS \n\n---------------\n-- Articles --\n---------------\n-- DROP EXTERNAL TABLE  scd.Articles\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Articles') )\n    DROP EXTERNAL TABLE scd.Articles;\n\n\nCREATE EXTERNAL TABLE  scd.Articles \nWITH\n(\n\tLOCATION = 'silver/SCD/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n    \n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Articles]\n    WHERE isCurrent = 1\n),\n\n-- CTE* creacion\n\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idArticles AS NVARCHAR)+ s.name + s.description COLLATE DATABASE_DEFAULT + s.externalcode + s.size + CAST(s.numSize AS NVARCHAR) + s.colour + s.category + s.codLine + s.line + s.season)) AS [$hash]\n    FROM etl.vw_dim_Articles s  -- METER AQUI DEFINICION LAS VISTAS CTE*\n    LEFT JOIN current_data c\n        ON s.idArticles = c.idArticles\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkArticles), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Articles]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idArticles) + max_key AS new_idSkArticles, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idArticles = c.idArticles\n    CROSS JOIN max_surrogate_key\n    WHERE c.idArticles IS NULL  OR (c.[$hash] != chg.[$hash] )\n), --select * from new_or_updated_data order by idarticles\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkarticles AS new_idSkArticles,\n        c.idArticles,\n        c.name,\n        c.externalCode,\n        c.size,\n        c.numSize,\n        c.colour,\n        c.category,\n        c.codLine,\n        c.line,\n        c.description,\n        c.season,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idArticles = chg.idArticles\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkArticles, -- Mantener la clave subrogada original\n    c.idArticles,\n    c.name,\n    c.externalCode,\n    c.size,\n    c.numSize,\n    c.colour,\n    c.category,\n    c.codLine,\n    c.line,\n    c.description,\n    c.season,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idArticles = chg.idArticles\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkArticles;\n\n\n---------------\n-- Currency --\n---------------\n-- DROP EXTERNAL TABLE  scd.Currency\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Currency') )\n    DROP EXTERNAL TABLE scd.Currency;\n\n\nCREATE EXTERNAL TABLE  scd.Currency \nWITH\n(\n\tLOCATION = 'silver/SCD/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Currency]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idCurrency AS NVARCHAR) + s.name + s.currency)) AS [$hash]\n    FROM etl.vw_dim_Currency s\n    LEFT JOIN current_data c\n        ON s.idCurrency = c.idCurrency\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkCurrency), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Currency]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idCurrency) + max_key AS new_idSkCurrency, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idCurrency = c.idCurrency\n    CROSS JOIN max_surrogate_key\n    WHERE c.idCurrency IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkCurrency AS new_idSkCurrency,\n        c.idCurrency,\n        c.name,\n        c.currency,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idCurrency = chg.idCurrency\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkCurrency, -- Mantener la clave subrogada original\n    c.idCurrency,\n    c.name,\n    c.currency,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idCurrency = chg.idCurrency\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkCurrency;\n\n\n------------\n-- Client --\n------------\n--DROP EXTERNAL TABLE  scd.Client\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Client') )\n   DROP EXTERNAL TABLE scd.Client   \n\n\nCREATE EXTERNAL TABLE  scd.Client \nWITH\n(\n\tLOCATION = 'silver/SCD/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Client]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idClient AS NVARCHAR) + s.name COLLATE DATABASE_DEFAULT + s.lastName1 COLLATE DATABASE_DEFAULT + s.lastName2 COLLATE DATABASE_DEFAULT\n\t    + s.email COLLATE DATABASE_DEFAULT + s.phoneNumber COLLATE DATABASE_DEFAULT + CAST(s.birthDay AS nvarchar) + CAST(s.age AS nvarchar) + s.gender + s.country + s.countryCode\n\t    + s.region + s.address + s.postalCode)) AS [$hash]\n    FROM etl.vw_dim_Client s\n    LEFT JOIN current_data c\n        ON s.idClient = c.idClient\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkClient), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Client]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idClient) + max_key AS new_idSkClient, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idClient = c.idClient\n    CROSS JOIN max_surrogate_key\n    WHERE c.idClient IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkClient AS new_idSkClient,\n        c.idClient,\n        c.name,\n        c.lastName1,\n        c.lastName2, \n        c.email,\n        c.phoneNumber,\n        c.birthDay,       \n        c.age,\n        c.gender,\n        c.country,\n        c.countryCode,\n        c.region,\n        c.address,\n        c.postalCode,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idClient = chg.idClient\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkClient, -- Mantener la clave subrogada original\n    c.idClient,\n    c.name,\n    c.lastName1,\n    c.lastName2, \n    c.email,\n    c.phoneNumber,\n    c.birthDay,       \n    c.age,\n    c.gender,\n    c.country,\n    c.countryCode,\n    c.region,\n    c.address,\n    c.postalCode,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idClient = chg.idClient\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkClient;\n\n\n----------\n-- Date --\n----------\n--DROP EXTERNAL TABLE  scd.Date \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Date') )\n   DROP EXTERNAL TABLE scd.Date;  \n\n\nCREATE EXTERNAL TABLE  scd.Date \nWITH\n(\n\tLOCATION = 'silver/SCD/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Date]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idDate AS NVARCHAR) + CAST(s.dateKey AS NVARCHAR) + CAST(s.date AS NVARCHAR) + s.month COLLATE DATABASE_DEFAULT + CAST(s.monthNumber AS NVARCHAR) + CAST(s.year AS NVARCHAR) \n        + CAST(s.weekNumber AS NVARCHAR) + s.dayWeek COLLATE DATABASE_DEFAULT + CAST(s.dayWeekNumber AS NVARCHAR) + CAST(s.yearDay AS NVARCHAR) + s.quarter + s.quadrimester + s.semester )) AS [$hash]\n    FROM etl.vw_dim_Date s\n    LEFT JOIN current_data c\n        ON s.idDate = c.idDate\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkDate), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Date]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idDate) + max_key AS new_idSkDate, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idDate = c.idDate\n    CROSS JOIN max_surrogate_key\n    WHERE c.idDate IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkDate AS new_idSkDate,\n        c.idDate,\n        c.dateKey,\n        c.date,\n        c.month,\n        c.monthNumber,\n        c.year,\n        c.weekNumber,\n        c.dayWeek,\n        c.dayWeekNumber,\n        c.yearDay,\n        c.quarter,\n        c.quadrimester,\n        c.semester,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idDate = chg.idDate\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkDate, -- Mantener la clave subrogada original\n    c.idDate,\n\tc.dateKey,\n\tc.date,\n\tc.month,\n\tc.monthNumber,\n\tc.year,\n\tc.weekNumber,\n\tc.dayWeek,\n\tc.dayWeekNumber,\n\tc.yearDay,\n\tc.quarter,\n\tc.quadrimester,\n\tc.semester,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idDate = chg.idDate\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkDate;\n\n\n---------------\n-- Hours --\n---------------\n-- DROP EXTERNAL TABLE  scd.Hours\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Hours') )\n   DROP EXTERNAL TABLE scd.Hours;   \n\n\nCREATE EXTERNAL TABLE  scd.Hours \nWITH\n(\n\tLOCATION = 'silver/SCD/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Hours]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idHours AS NVARCHAR) + CAST(s.hour AS NVARCHAR) + CAST(s.minute AS NVARCHAR) + s.fullHour)) AS [$hash]\n    FROM etl.vw_dim_Hours s\n    LEFT JOIN current_data c\n        ON s.idHours = c.idHours\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkHours), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Hours]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idHours) + max_key AS new_idSkHours, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idHours = c.idHours\n    CROSS JOIN max_surrogate_key\n    WHERE c.idHours IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkHours AS new_idSkHours,\n        c.idHours,\n        c.hour,\n        c.minute,\n        c.fullHour,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idHours = chg.idHours\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkHours, -- Mantener la clave subrogada original\n\tc.idHours,\n\tc.hour,\n\tc.minute,\n\tc.fullHour,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idHours = chg.idHours\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkHours;\n\n\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE  scd.OperationType \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.OperationType') )\n   DROP EXTERNAL TABLE scd.OperationType;   \n\n\nCREATE EXTERNAL TABLE  scd.OperationType \nWITH\n(\n\tLOCATION = 'silver/SCD/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[OperationType]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idOperationType AS NVARCHAR) + s.operation)) AS [$hash]\n    FROM etl.vw_dim_OperationType s\n    LEFT JOIN current_data c\n        ON s.idOperationType = c.idOperationType\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkOperationType), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[OperationType]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idOperationType) + max_key AS new_idSkOperationType, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idOperationType = c.idOperationType\n    CROSS JOIN max_surrogate_key\n    WHERE c.idOperationType IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkOperationType AS new_idSkOperationType,\n        c.idOperationType,\n        c.operation,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idOperationType = chg.idOperationType\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkOperationType, -- Mantener la clave subrogada original\n    c.idOperationType,\n    c.operation,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idOperationType = chg.idOperationType\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkOperationType,\n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkOperationType, \n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkOperationType;\n\n\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE  scd.PostalCodes \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.PostalCodes') )\n    DROP EXTERNAL TABLE scd.PostalCodes; \n\nCREATE EXTERNAL TABLE  scd.PostalCodes \nWITH\n(\n\tLOCATION = 'silver/SCD/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idPostalCodes AS NVARCHAR) + s.postalCode + s.region + s.countryCode + s.country)) AS [$hash]\n    FROM etl.vw_dim_PostalCodes s\n    LEFT JOIN current_data c\n        ON s.idPostalCodes = c.idPostalCodes\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkPostalCodes), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idPostalCodes) + max_key AS new_idSkPostalCodes, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idPostalCodes = c.idPostalCodes\n    CROSS JOIN max_surrogate_key\n    WHERE c.idPostalCodes IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkPostalCodes AS new_idSkPostalCodes,\n        c.idPostalCodes,\n        c.postalCode,\n        c.region,\n        c.countryCode,\n        c.country,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idPostalCodes = chg.idPostalCodes\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkPostalCodes, -- Mantener la clave subrogada original\n    c.idPostalCodes,\n    c.postalCode,\n    c.region,\n    c.countryCode,\n    c.country,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idPostalCodes = chg.idPostalCodes\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkPostalCodes,\n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkPostalCodes, \n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkPostalCodes;\n\n\n\n------------\n-- Tariff --\n------------\n--DROP EXTERNAL TABLE  scd.Tariff \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Tariff') )\n   DROP EXTERNAL TABLE scd.Tariff;   \n\n\nCREATE EXTERNAL TABLE  scd.Tariff \nWITH\n(\n\tLOCATION = 'silver/SCD/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Tariff]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idTariff AS NVARCHAR) + s.tariff)) AS [$hash]\n    FROM etl.vw_dim_Tariff s\n    LEFT JOIN current_data c\n        ON s.idTariff = c.idTariff\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkTariff), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Tariff]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idTariff) + max_key AS new_idSkTariff, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idTariff = c.idTariff\n    CROSS JOIN max_surrogate_key\n    WHERE c.idTariff IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkTariff AS new_idSkTariff,\n        c.idTariff,\n        c.tariff,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idTariff = chg.idTariff\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkTariff, -- Mantener la clave subrogada original\n    c.idTariff,\n    c.tariff,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idTariff = chg.idTariff\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkTariff;\n\n\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE  scd.Warehouse \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Warehouse') )\n   DROP EXTERNAL TABLE scd.Warehouse;   \n\n\nCREATE EXTERNAL TABLE  scd.Warehouse \nWITH\n(\n\tLOCATION = 'silver/SCD/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.idWarehouse,\n        s.warehouse,\n        s.externalCode,\n        s.countryCode,\n        s.country,\n        s.city,\n        s.address,\n        s.description,\n        s.loadDate,\n        s.deltaDate,\n        HASHBYTES('SHA2_256',(CAST(s.idWarehouse AS NVARCHAR) + s.warehouse + s.externalcode + s.countryCode + s.country + s.city + s.address + s.description)) AS [$hash]\n    FROM etl.vw_dim_Warehouse s\n    LEFT JOIN current_data c\n        ON s.idWarehouse = c.idWarehouse\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkWarehouse), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idWarehouse) + max_key AS new_idSkWarehouse, -- Asignar nueva clave subrogada\n        chg.idWarehouse,\n        chg.warehouse,\n        chg.externalCode,\n        chg.countryCode,\n        chg.country,\n        chg.city,\n        chg.address,\n        chg.description,\n        chg.loadDate,\n        chg.deltaDate,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate,\n        chg.[$hash]\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idWarehouse = c.idWarehouse\n    CROSS JOIN max_surrogate_key\n    WHERE c.idWarehouse IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkWarehouse AS new_idSkWarehouse,\n        c.idWarehouse,\n        c.warehouse,\n        c.externalCode,\n        c.countryCode,\n        c.country,\n        c.city,\n        c.address,\n        c.description,\n        c.loadDate,\n        c.deltaDate,\n        c.isCurrent,\n        c.fromDate,\n        c.toDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idWarehouse = chg.idWarehouse\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\nfinal AS (\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nSELECT\n    c.idSkWarehouse, -- Mantener la clave subrogada original\n    c.idWarehouse,\n    c.warehouse,\n    c.externalCode,\n    c.countryCode,\n    c.country,\n    c.city,\n    c.address,\n    c.description,\n    c.loadDate,\n    c.deltaDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.fromDate,\n    GETDATE() AS toDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idWarehouse = chg.idWarehouse\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM unchanged_data\n\n)\nselect     idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM final\nORDER BY idSkWarehouse;\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [skWarehouse]\n,[idWarehouse]\n,[warehouse]\n,[externalCode]\n,[countryCode]\n,[country]\n,[city]\n,[address]\n,[description]\n FROM [dim].[Warehouse]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ReportingDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoadSCD2AndFact')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ed0ee182-1310-49ca-8217-332cd9c54c8b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Import modules**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format,concat_ws,xxhash64,current_timestamp\r\n",
							"from pyspark.sql import SparkSession,Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"import json\r\n",
							"import re\r\n",
							"from datetime import datetime\r\n",
							"from azure.storage.blob import BlobServiceClient\r\n",
							"import io\r\n",
							"import pandas as pd\r\n",
							"import csv"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#%%sql\r\n",
							"# USE default;\r\n",
							"# DROP DATABASE  gold CASCADE ;\r\n",
							"# USE default;\r\n",
							"# CREATE DATABASE IF NOT EXISTS gold;"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"USE gold; \r\n",
							"DROP TABLE IF EXISTS fact_Sales;"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"factList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Sales\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idDesgloseVenta AS idSales,\r\n",
							"                        d.idArticulo AS idArticles,\r\n",
							"                        c.idAlmacen AS idWarehouse,\r\n",
							"                        c.idCliente AS idClient,\r\n",
							"                        c.idCodigoPostal AS idPostalCode,\r\n",
							"                        c.idDivisa AS idCurrency,\r\n",
							"                        c.idTarifa AS idTariff,\r\n",
							"                        c.idTipoOperacion AS idOperationType,\r\n",
							"                        c.idHora AS idHours,\r\n",
							"                        c.idFecha AS idDate,\r\n",
							"                        f.fecha AS  date,\r\n",
							"                        CAST(CONCAT(DATE_FORMAT(f.fecha, 'yyyy-MM-dd'), ' ', h.horaCompleta) AS TIMESTAMP) AS dateOp,\r\n",
							"                        COALESCE(c.codigoTicket, 'D') AS ticketNumber,\r\n",
							"                        COALESCE(d.Cantidad, 0) AS quantity,\r\n",
							"                        COALESCE(d.PrecioUnitario, 0)  AS unitPrice,\r\n",
							"                        COALESCE(d.CosteUnitario, 0)  AS unitCost,\r\n",
							"                        COALESCE(d.importeBruto, 0)  AS amtTotalEuros,\r\n",
							"                        COALESCE(d.importeNeto, 0)  AS amtNetEuros,\r\n",
							"                        COALESCE(d.importeDescuento, 0)  AS amtTotalEurosDiscount,\r\n",
							"                        COALESCE(d.importeNetoDescuento, 0) AS amtNetEurosDiscount,\r\n",
							"                        CASE WHEN idTarifa IN (2,3) THEN 1\r\n",
							"                            ELSE 0 \r\n",
							"                        END AS discountSale,\r\n",
							"                        CASE WHEN idTarifa = 0 THEN 0\r\n",
							"                            ELSE (d.importeBruto - d.importeDescuento) \r\n",
							"                        END AS amtTotalDiscount,\r\n",
							"                        CASE WHEN idTarifa = 0 THEN 0 \r\n",
							"                            ELSE (d.importeNeto - d.importeNetoDescuento) \r\n",
							"                        END AS amtNetDiscount,\r\n",
							"                        GREATEST(d.fechaCarga, c.fechaCarga) AS loadDate,\r\n",
							"                        GREATEST(d.fechaDelta, c.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.desgloseVenta AS d\r\n",
							"                    INNER JOIN silver.cabeceraVenta AS c\r\n",
							"                        ON d.idcabecerasVentas = c.idcabeceraVenta\r\n",
							"                    LEFT JOIN silver.Fechas  AS f\r\n",
							"                        On f.idFechas = c.idFecha\r\n",
							"                    LEFT JOIN silver.horas  AS h\r\n",
							"                        On h.idHoras = c.idHora\r\n",
							"                    WHERE GREATEST(d.fechaDelta, c.fechaDelta) >=\r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"lookupFact =  [ \r\n",
							"        {\r\n",
							"            \"name\": \"Sales\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT    \r\n",
							"                        s.*,\r\n",
							"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
							"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
							"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
							"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
							"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
							"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
							"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
							"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
							"                        COALESCE(d.skDate, -1) AS skDate\r\n",
							"                FROM temp_sales_view as s\r\n",
							"                LEFT JOIN gold.dim_Articles AS a\r\n",
							"                    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
							"                LEFT JOIN gold.dim_Client AS cl\r\n",
							"                    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
							"                LEFT JOIN gold.dim_Currency AS cu\r\n",
							"                    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
							"                LEFT JOIN gold.dim_Date AS d\r\n",
							"                    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
							"                LEFT JOIN gold.dim_Hours AS h\r\n",
							"                    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
							"                LEFT JOIN gold.dim_OperationType AS o\r\n",
							"                    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
							"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
							"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
							"                LEFT JOIN gold.dim_Tariff AS t\r\n",
							"                    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
							"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
							"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dimensionsList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Articles\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idArticulos                                                                                             AS idArticles,\r\n",
							"                            COALESCE(nombre, 'D')                                                                                   AS name,\r\n",
							"                            COALESCE(descripcion, 'D')                                                                              AS description,\r\n",
							"                            COALESCE(codigoReferencia, 'D')                                                                         AS externalCode,\r\n",
							"                            COALESCE(t.talla, 'D')                                                                                  AS size,\r\n",
							"                            COALESCE( t.numeroTalla, -1)                                                                            AS numSize,\r\n",
							"                            COALESCE(co.color, 'D')                                                                                 AS colour,\r\n",
							"                            COALESCE(ca.categoria, 'D')                                                                             AS category,\r\n",
							"                            COALESCE(l.codigoLinea, 'D')                                                                            AS codLine,\r\n",
							"                            COALESCE(l.Linea, 'D')                                                                                  AS line, \r\n",
							"                            CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                                 WHEN a.idCategoria IN (2,4,6,7,9) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                                 ELSE CAST(CONCAT('PV',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                            END                                                                                                     AS season,              \r\n",
							"                            GREATEST(a.fechaCarga, t.fechaCarga, co.fechaCarga, ca.fechaCarga, l.fechaCarga)                        AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta)                        AS deltaDate\r\n",
							"                    FROM silver.articulos AS a\r\n",
							"                        LEFT JOIN silver.talla AS t\r\n",
							"                            ON t.idTalla = a.idTalla\r\n",
							"                        LEFT JOIN silver.color AS co\r\n",
							"                            ON co.idColor = a.idColor\r\n",
							"                        LEFT JOIN silver.categoria AS ca -- select * from silver.categoria\r\n",
							"                            ON ca.idCategoria = a.idCategoria\r\n",
							"                        LEFT JOIN silver.Linea AS l\r\n",
							"                            ON l.idLinea = a.idLinea\r\n",
							"                    WHERE GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Client\",\r\n",
							"            \"query\": \"\"\"              \r\n",
							"                    SELECT  idCliente                                           AS idClient,\r\n",
							"                            COALESCE(c.nombre, 'D')                             AS name,\r\n",
							"                            COALESCE(apellido1, 'D')                            AS lastName1,\r\n",
							"                            COALESCE(apellido2, 'D')                            AS lastName2, \r\n",
							"                            COALESCE(email, 'D')                                AS email,\r\n",
							"                            COALESCE(telefono, 'D')                             AS phoneNumber,\r\n",
							"                            COALESCE(CAST(cumpleanos AS STRING), '1900-01-01')  AS birthDay,       \r\n",
							"                            YEAR(current_date()) - YEAR(CAST(cumpleanos AS DATE)) \r\n",
							"                                - CASE \r\n",
							"                                    WHEN MONTH(current_date()) < MONTH(CAST(cumpleanos AS DATE)) \r\n",
							"                                        OR (MONTH(current_date()) = MONTH(CAST(cumpleanos AS DATE)) AND DAY(current_date()) < DAY(CAST(cumpleanos AS DATE)))\r\n",
							"                                    THEN 1\r\n",
							"                                    ELSE 0\r\n",
							"                            END                                                 AS age,\r\n",
							"                            CASE WHEN hombre = 1 THEN 'Hombre' \r\n",
							"                                 ELSE 'Mujer' \r\n",
							"                            END                                                 AS gender,\r\n",
							"                            COALESCE(p.nombre, 'D')                             AS country,\r\n",
							"                            COALESCE(p.codigoPais, 'D')                         AS countryCode,\r\n",
							"                            COALESCE(cp.region, 'D')                            AS region,\r\n",
							"                            COALESCE(c.Direcion, 'D')                           AS address,  \r\n",
							"                            COALESCE(codigoPostal, 'D')                         AS postalCode,\r\n",
							"                            COALESCE(activo, false)                             AS active,  -- Cambiado 0 a false aquÃ­\r\n",
							"                            GREATEST(c.fechaCarga, p.fechaCarga, cp.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) AS deltaDate\r\n",
							"                        FROM silver.cliente AS c\r\n",
							"                            LEFT JOIN silver.pais AS p \r\n",
							"                                ON c.idPais = p.idPais\r\n",
							"                            INNER JOIN silver.codigoPostal AS cp \r\n",
							"                                ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\r\n",
							"                        WHERE GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Currency\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idDivisa              AS idCurrency,\r\n",
							"                            COALESCE(nombre, 'D') AS name,\r\n",
							"                            COALESCE(Divisa, 'D') AS currency,\r\n",
							"                            fechaCarga            AS loadDate,\r\n",
							"                            fechaDelta            AS deltaDate\r\n",
							"                    FROM silver.divisa\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Date\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idFechas                                  AS idDate,\r\n",
							"                            ClaveFecha                                AS dateKey,\r\n",
							"                            Fecha                                     AS date,\r\n",
							"                            COALESCE(Mes, 'D')                        AS month,\r\n",
							"                            COALESCE(NumeroMes, -1)                   AS monthNumber,\r\n",
							"                            COALESCE(Ano, -1)                         AS year,\r\n",
							"                            COALESCE(NumeroSemana, -1)                AS weekNumber,\r\n",
							"                            COALESCE(DiaSemana, 'D')                  AS dayWeek,\r\n",
							"                            COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\r\n",
							"                            COALESCE(DiaAno, -1)                      AS yearDay,\r\n",
							"                            CASE WHEN Trimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Trimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Trimestre = 3 THEN 'Q3'\r\n",
							"                                ELSE '-1' END                         AS quarter,\r\n",
							"                            CASE WHEN Cuatrimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Cuatrimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Cuatrimestre = 3 THEN 'Q3'\r\n",
							"                                WHEN Cuatrimestre = 4 THEN 'Q4'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS quadrimester,\r\n",
							"                            CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\r\n",
							"                                WHEN Cuatrimestre IN (2,4) THEN 'S2'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS semester,\r\n",
							"                            fechaCarga                                AS loadDate,\r\n",
							"                            fechaDelta                                AS deltaDate\r\n",
							"                    FROM silver.fechas\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Hours\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idHoras                     AS idHours,\r\n",
							"                            COALESCE(Hora, -1)          AS hour,\r\n",
							"                            COALESCE(Minuto, -1)        AS minute,\r\n",
							"                            COALESCE(HoraCompleta, 'D') AS fullHour,\r\n",
							"                            fechaCarga                  AS loadDate,\r\n",
							"                            fechaDelta                  AS deltaDate\r\n",
							"                    FROM silver.horas\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"OperationType\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTipoOperacion          AS idOperationType,\r\n",
							"                            COALESCE(Operacion, 'D') AS operation,\r\n",
							"                            fechaCarga               AS loadDate,\r\n",
							"                            fechaDelta               AS deltaDate\r\n",
							"                    FROM silver.tipoOperacion\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"PostalCode\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idCodigoPostal                       AS idPostalCode,\r\n",
							"                            COALESCE(codigoPostal, 'D')          AS postalCode,\r\n",
							"                            COALESCE(region, 'D')                AS region,\r\n",
							"                            COALESCE(c.codigoPais, 'D')          AS countryCode,\r\n",
							"                            COALESCE(nombre, 'D')                AS country,\r\n",
							"                            GREATEST(c.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(c.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.codigoPostal AS c\r\n",
							"                    LEFT JOIN silver.pais AS p\r\n",
							"                        ON p.codigoPais = c.codigoPais\r\n",
							"                    WHERE GREATEST(c.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Tariff\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTarifa              AS idTariff,\r\n",
							"                            COALESCE(Tarifa, 'D') AS tariff,\r\n",
							"                            fechaCarga            AS loadDate,\r\n",
							"                            fechaDelta            AS deltaDate       \r\n",
							"                    FROM silver.tarifa\r\n",
							"                    WHERE fechaDelta >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Warehouse\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idAlmacenes                          AS idWarehouse,\r\n",
							"                            COALESCE(a.Nombre, 'D')              AS warehouse,\r\n",
							"                            COALESCE(a.codigoAlmacen, 'D')       AS externalCode,\r\n",
							"                            COALESCE(p.codigoPais, 'D')          AS countryCode, \r\n",
							"                            COALESCE(p.nombre, 'D')              AS country,\r\n",
							"                            COALESCE(a.ciudad, 'D')              AS city,\r\n",
							"                            COALESCE(a.Direccion, 'D')           AS address,\r\n",
							"                            COALESCE(a.descripcion, 'D')         AS description,\r\n",
							"                            GREATEST(a.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.almacenes AS a\r\n",
							"                    LEFT JOIN silver.pais AS p\r\n",
							"                        ON p.idpais = a.idPais\r\n",
							"                    WHERE GREATEST(a.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Configuration**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# parameter string to array\r\n",
							"tables_to_load = json.loads(tables_to_load)\r\n",
							"\r\n",
							"today_datetime = datetime.now()\r\n",
							"\r\n",
							"# delta_date_filter = (today_datetime.strftime('%Y%m%d')) # 'yyyyMMdd'\r\n",
							"# delta_date_filter = '19900101' # Debud\r\n",
							"\r\n",
							"update_date_string = today_datetime.strftime('%Y-%m-%d') # 'yyyy-mm-dd'\r\n",
							"#update_date_string = '1990-01-01'\r\n",
							"\r\n",
							"# Convertir cada valor del CSV en un diccionario estructurado\r\n",
							"dict_tables_to_load = []\r\n",
							"\r\n",
							"for row in tables_to_load:\r\n",
							"    dict_tables_to_load.append({\r\n",
							"    \"SchemaName\": row[\"SchemaName\"],\r\n",
							"    \"TableName\": row[\"TableName\"],\r\n",
							"    \"UpdateDate\": row[\"UpdateDate\"],\r\n",
							"    \"Load\": row[\"Load\"]\r\n",
							"})\r\n",
							"\r\n",
							"# Variables para almacenar los datos de 'dim' y 'fact'\r\n",
							"dim_tables = []\r\n",
							"fact_tables = []\r\n",
							"fact_lockup = []\r\n",
							"\r\n",
							"# Clasificar los datos\r\n",
							"for row in dict_tables_to_load:\r\n",
							"    if row['SchemaName'] == 'dim':\r\n",
							"        dim_tables.append(row)\r\n",
							"    elif row['SchemaName'] == 'fact':\r\n",
							"        fact_tables.append(row)\r\n",
							"\r\n",
							"# Filtrar las listas de antes para obtener solo las tablas con Load='Y' y devolver una lista de diccionarios\r\n",
							"load_y_tables = {table['TableName'] for table in dim_tables if table['Load'] == 'Y'}\r\n",
							"filtered_dimensionsList = [\r\n",
							"    dim for dim in dimensionsList if dim[\"name\"] in load_y_tables\r\n",
							"]\r\n",
							"\r\n",
							"load_y_tables2 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
							"filtered_factList = [\r\n",
							"    dim for dim in factList if dim[\"name\"] in load_y_tables2\r\n",
							"]\r\n",
							"\r\n",
							"# Implementar\r\n",
							"load_y_tables3 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
							"filtered_factList_Lookup = [\r\n",
							"    dim for dim in lookupFact if dim[\"name\"] in load_y_tables3\r\n",
							"]\r\n",
							"\r\n",
							"def update_query_with_update_date(table_info_list, query_list):\r\n",
							"    for table_info in table_info_list:\r\n",
							"        table_name = table_info['TableName']\r\n",
							"        update_date = table_info['UpdateDate']\r\n",
							"        \r\n",
							"        for query_info in query_list:\r\n",
							"            if query_info['name'] == table_name:\r\n",
							"                # AÃ±adir UpdateDate al final de la query en el WHERE\r\n",
							"                query_info['query'] += f\" '{update_date}'\\n\"\r\n",
							"    return query_list\r\n",
							"\r\n",
							"# Ejecutar la funciÃ³n\r\n",
							"updated_list_Dim = update_query_with_update_date(dim_tables, filtered_dimensionsList)\r\n",
							"updated_list2_Fact = update_query_with_update_date(fact_tables, filtered_factList)\r\n",
							"\r\n",
							"# # Mostrar el resultado\r\n",
							"# for query in updated_list2:\r\n",
							"#     print(\"Tabla:\", query['name'])\r\n",
							"#     print(\"Query actualizada:\", query['query'])\r\n",
							""
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Load SCD2 Dimensions**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Carga o Merge Dimensiones\r\n",
							"results = []\r\n",
							"exception = []\r\n",
							"\r\n",
							"for file in updated_list_Dim:  #  updated_list_Dim  filtered_dimensionsList\r\n",
							"    file_result = {\r\n",
							"        'table': f\"dim_{file['name']}\",\r\n",
							"        'status': 'Incorrect',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now()) \r\n",
							"    }\r\n",
							"\r\n",
							"    try:\r\n",
							"\r\n",
							"        table_name = file[\"name\"].split('_')[0]\r\n",
							"        #source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
							"        key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
							"\r\n",
							"        key_columns = key_columns_str.split(',')\r\n",
							"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"        #delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Dimensions/{table_name}\"\r\n",
							"        delta_table_path_dimensions = f\"{data_lake_container}/{golden_folder_dimensions}/{table_name}\"\r\n",
							"\r\n",
							"\r\n",
							"        #df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
							"        df = spark.sql(file[\"query\"].strip()) \r\n",
							"        sdf = df.cache()\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"There is no new data\"\r\n",
							"            results.append(file_result)\r\n",
							"            continue\r\n",
							"\r\n",
							"        \r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path_dimensions):\r\n",
							"            # AÃ±adir las columnas necesarias\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            todas_las_columnas = sdf.columns\r\n",
							"            columnas_al_principio = [f\"id{table_name}\"]\r\n",
							"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
							"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
							"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
							"            sdf = sdf.select(*nuevo_orden_columnas)\r\n",
							"\r\n",
							"            next_surrogate_key = spark.sql(f\"SELECT COALESCE(MAX(sk{table_name}), 0) + 1 AS next_key FROM gold.dim_{table_name}\").collect()[0][\"next_key\"]\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_view\")\r\n",
							"\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                MERGE INTO gold.dim_{table_name}  \r\n",
							"                AS existing\r\n",
							"                USING temp_view AS updates\r\n",
							"                ON {\" AND \".join([f\"existing.{key}=updates.{key}\" for key in key_columns])}\r\n",
							"                WHEN MATCHED AND existing.isCurrent = 1 AND existing.hash != updates.hash THEN\r\n",
							"                UPDATE SET\r\n",
							"                    existing.toDate = current_date(),\r\n",
							"                    existing.isCurrent = 0\r\n",
							"            \"\"\")\r\n",
							"\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                INSERT INTO gold.dim_{table_name}    \r\n",
							"                SELECT \r\n",
							"                    {next_surrogate_key} + row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS sk{table_name},  \r\n",
							"                    updates.*\r\n",
							"                FROM temp_view AS updates\r\n",
							"                LEFT JOIN gold.dim_{table_name} AS existing\r\n",
							"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
							"                WHERE \r\n",
							"                    existing.id{table_name} IS NULL OR \r\n",
							"                    (existing.isCurrent = 0 AND existing.hash != updates.hash) OR \r\n",
							"                    (existing.isCurrent = 1 AND existing.hash != updates.hash)\r\n",
							"            \"\"\")\r\n",
							"\r\n",
							"            # Almacenar conteos de filas\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.dim_{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            if last_result:\r\n",
							"                file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
							"                file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
							"            else:\r\n",
							"                print(f\"No hay registros de operaciÃ³n MERGE para la tabla {file['name']}\")\r\n",
							"                file_result['inserted_rows'] = 0\r\n",
							"                file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"        else: # Crear nueva tabla Delta\r\n",
							"      \r\n",
							"            last_surrogate_key = 0  # Iniciar con 0 si la tabla no existe\r\n",
							"            #sdf = sdf.withColumn(f\"sk{table_name}\", F.monotonically_increasing_id() + last_surrogate_key + 1)  # Asigna un valor Ãºnico a cada fila.\r\n",
							"            sdf = sdf.withColumn(f\"sk{table_name}\", F.col(f\"id{table_name}\")) # Primera carga sk = id\r\n",
							"            # AÃ±adir las columnas necesarias\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            # Hash\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\" ,'fechaCarga', 'fechaDelta','fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # Reorganizar la estructura de la tabla\r\n",
							"            todas_las_columnas = sdf.columns\r\n",
							"            columnas_al_principio = [f\"sk{table_name}\", f\"id{table_name}\"]\r\n",
							"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
							"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
							"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
							"            sdf_reordenado = sdf.select(*nuevo_orden_columnas)\r\n",
							"            \r\n",
							"            # Crear la tabla Delta\r\n",
							"            sdf_reordenado.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path_dimensions)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.dim_{table_name} USING DELTA LOCATION \\'{delta_table_path_dimensions}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"            # Almacenar archivo procesado correctamente\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"            file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Load Fact Table**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for file, file2 in zip(updated_list2_Fact, filtered_factList_Lookup):\r\n",
							"    file_result = {\r\n",
							"        'table': f\"fact_{file['name']}\",\r\n",
							"        'status': 'Incorrect',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now())  \r\n",
							"    }\r\n",
							"    table_name = file[\"name\"].split('_')[0]\r\n",
							"    key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
							"\r\n",
							"    key_columns = key_columns_str.split(',')\r\n",
							"    conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"    # Definir la ruta de la tabla Delta\r\n",
							"    delta_table_path_fact = f\"{data_lake_container}/{golden_folder_fact}/{table_name}\"\r\n",
							"    \r\n",
							"    #df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
							"    df = spark.sql(file[\"query\"].strip())       # Hacer lo mismo mÃ¡s adelante con los lookups \r\n",
							"    sdf = df.cache()\r\n",
							"\r\n",
							"    try:\r\n",
							"\r\n",
							"        #source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
							"\r\n",
							"\r\n",
							"        # Verificar si el DataFrame estÃ¡ vacÃ­o\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ la tabla {file['name']} porque no contiene nuevos datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"There is no new data\"\r\n",
							"            results.append(file_result)\r\n",
							"            continue\r\n",
							"\r\n",
							"        # Truncar la tabla Delta si existe\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path_fact):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path_fact)\r\n",
							"            #delta_table.delete()  # Truncar la tabla Delta\r\n",
							"\r\n",
							"            #last_row number para incremental\r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
							"            # spark.sql(f\"\"\"\r\n",
							"            #             INSERT INTO gold.fact_{table_name} \r\n",
							"            #             {file2[\"query\"].strip()}\r\n",
							"            #            \"\"\")  # df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
							"\r\n",
							"            sdf =spark.sql(f\"\"\"\r\n",
							"                        {file2[\"query\"].strip()} \r\n",
							"                        \"\"\")\r\n",
							"\r\n",
							"            #max_skSales = 0\r\n",
							"            max_skSales = spark.sql(f\"SELECT COALESCE(MAX(sk{table_name}), 0) AS next_key FROM gold.fact_{table_name}\").collect()[0][\"next_key\"]  # +1???           \r\n",
							"            sdf = sdf.withColumn(\"skSales\", F.expr(f\"{max_skSales} + row_number() OVER (ORDER BY id{table_name})\"))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('loadDate', 'deltaDate', 'skSales')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"            \r\n",
							"            #next_surrogate_key = spark.sql(f\"SELECT COALESCE(MAX(sk{table_name}), 0) + 1 AS next_key FROM gold.dim_{table_name}\").collect()[0][\"next_key\"]\r\n",
							"          \r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view_merge\")\r\n",
							"\r\n",
							"            table_schema = spark.table(f\"gold.fact_{table_name}\").schema\r\n",
							"\r\n",
							"            # Listas de columnas para construir dinÃ¡micamente la consulta\r\n",
							"            columns = [col for col in sdf.columns if col not in ('skSales', 'hash')]\r\n",
							"\r\n",
							"            # 2. Construir las clÃ¡usulas para el INSERT\r\n",
							"            update_set = \", \".join([f\"existing.{col} = updates.{col}\" for col in columns] + [\"existing.hash = updates.hash\"])\r\n",
							"            insert_columns = \", \".join(columns + ['skSales', 'hash'])  # Agregar 'skSales' y 'hash' al final\r\n",
							"            insert_values = \", \".join([f\"updates.{col}\" for col in columns] + ['updates.skSales', 'updates.hash'])  # Asegurarte de que los valores estÃ©n en el mismo orden\r\n",
							"\r\n",
							"            # 3. Ejecutar el MERGE o INSERT\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                MERGE INTO gold.fact_{table_name} AS existing\r\n",
							"                USING temp_sales_view_merge AS updates\r\n",
							"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
							"\r\n",
							"                WHEN MATCHED AND existing.hash != updates.hash THEN\r\n",
							"                UPDATE SET {update_set}  -- AquÃ­ se actualizarÃ¡n las columnas existentes\r\n",
							"\r\n",
							"                WHEN NOT MATCHED THEN\r\n",
							"                INSERT ({insert_columns}) \r\n",
							"                VALUES ({insert_values})\r\n",
							"            \"\"\")\r\n",
							"            \r\n",
							"            # Almacenar conteos de filas\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.fact_{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            if last_result:\r\n",
							"                file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
							"                file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
							"            else:\r\n",
							"                #print(f\"No hay registros de operaciÃ³n MERGE para la tabla {file['name']}\")\r\n",
							"                file_result['inserted_rows'] = 0\r\n",
							"                file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"        else:\r\n",
							"            \r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
							"            sdf =spark.sql(f\"\"\"\r\n",
							"                        {file2[\"query\"].strip()} \r\n",
							"                        \"\"\")\r\n",
							"\r\n",
							"            max_skSales = 0\r\n",
							"            sdf = sdf.withColumn(\"skSales\", F.expr(f\"{max_skSales} + row_number() OVER (ORDER BY id{table_name})\"))\r\n",
							"\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('loadDate', 'deltaDate', 'skSales')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"            \r\n",
							"            sdf.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_fact)\r\n",
							"\r\n",
							"            # Crear la tabla Delta en caso de que no exista\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.fact_{table_name} USING DELTA LOCATION \\'{delta_table_path_fact}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"            \r\n",
							"        # Almacenar conteos de filas\r\n",
							"        file_result['status'] = 'Correct'\r\n",
							"        file_result['inserted_rows'] = sdf.count()\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        continue\r\n",
							"\r\n",
							"    #results.append(file_result)"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Guardar ficheros Logs**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Obtener la fecha actual\r\n",
							"# fecha_actual = datetime.now()\r\n",
							"# year = fecha_actual.strftime('%Y') \r\n",
							"# month = fecha_actual.strftime('%m') \r\n",
							"# day = fecha_actual.strftime('%d')  \r\n",
							"# hour = fecha_actual.strftime('%H%M%S') \r\n",
							"\r\n",
							"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
							"#archivo_nombre = f\"Log_{day}_{hour}.json\"\r\n",
							"log_file_name = f\"Log_{executionID}.json\"\r\n",
							"\r\n",
							"# Reemplaza con tu cadena de conexiÃ³n a Azure\r\n",
							"\r\n",
							"# Cliente de servicio de blobs\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# Convetimos el diccionario a formato json\r\n",
							"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
							"\r\n",
							"# Crear la ruta de destino con la jerarquÃ­a AÃ±o/Mes y el nombre del archivo\r\n",
							"destination_blob_name = f\"{gold_folder_logs}/{year}/{month}/{log_file_name}\"\r\n",
							"\r\n",
							"# Crear un cliente del blob de destino\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=container_name, blob=destination_blob_name)\r\n",
							"\r\n",
							"destination_blob.upload_blob(json_data, overwrite=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Actualizar fichero metadata**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Configurar la conexiÃ³n\r\n",
							"\r\n",
							"#update_date_string = '1900-01-01'\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"csv_blob_name = \"ConfiguracionReporting.csv\"  # nombre de tu archivo/ se puede poner como parametro\r\n",
							"\r\n",
							"# Leer el archivo CSV desde el Data Lake\r\n",
							"blob_client = blob_service_client.get_blob_client(container=metadata_folder, blob=csv_blob_name)\r\n",
							"\r\n",
							"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
							"df = pd.read_csv(stream, sep=',')  # Especifica el separador aquÃ­\r\n",
							"\r\n",
							"# Limpiar nombres de columnas para eliminar espacios en blanco\r\n",
							"df.columns = df.columns.str.strip()\r\n",
							"\r\n",
							"names_dimensionsList = [item['name'] for item in filtered_dimensionsList]\r\n",
							"for i in names_dimensionsList:\r\n",
							"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{update_date_string}'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"\r\n",
							"names_factList = [item['name'] for item in filtered_factList]\r\n",
							"for i in names_factList:\r\n",
							"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{update_date_string}'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"\r\n",
							"# Guardar el DataFrame modificado en un nuevo flujo de bytes\r\n",
							"output_stream = io.BytesIO()\r\n",
							"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')  # Usar line_terminator para evitar lÃ­neas en blanco\r\n",
							"output_stream.seek(0)  # Regresar al inicio del flujo\r\n",
							"\r\n",
							"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
						],
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoadSCD2AndFact_1d83bd0a-dd01-4d44-adb2-c1fdc98194df')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "52a34a39-253f-486a-ac68-b333ab6238a7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters_overwritten"
							]
						},
						"source": [
							"# This cell is generated from runtime parameters. Learn more: https://go.microsoft.com/fwlink/?linkid=2161015\n",
							"data_lake_container = \"abfss://mdw@datalake1pgc.dfs.core.windows.net\"\n",
							"gold_folder_logs = \"gold/Logs\"\n",
							"container_name = \"mdw\"\n",
							"metadata_folder = \"mdw/bronze/Configuration\"\n",
							"tables_to_load = \"[{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Articles\\\",\\\"UpdateDate\\\":\\\"2024-10-25\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Client\\\",\\\"UpdateDate\\\":\\\"2024-10-25\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Currency\\\",\\\"UpdateDate\\\":\\\"2024-10-25\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Date\\\",\\\"UpdateDate\\\":\\\"2024-10-25\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Hours\\\",\\\"UpdateDate\\\":\\\"2024-10-25\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"OperationType\\\",\\\"UpdateDate\\\":\\\"2024-10-25\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"PostalCode\\\",\\\"UpdateDate\\\":\\\"2024-10-25\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Tariff\\\",\\\"UpdateDate\\\":\\\"2024-10-25\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Warehouse\\\",\\\"UpdateDate\\\":\\\"2024-10-25\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"fact\\\",\\\"TableName\\\":\\\"Sales\\\",\\\"UpdateDate\\\":\\\"2024-10-25\\\",\\\"Load\\\":\\\"Y\\\"}]\"\n",
							"golden_folder_dimensions = \"gold/Dimensions\"\n",
							"golden_folder_fact = \"gold/Fact\"\n",
							""
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Import modules**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format,concat_ws,xxhash64,current_timestamp\r\n",
							"from pyspark.sql import SparkSession,Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"import json\r\n",
							"import re\r\n",
							"from datetime import datetime\r\n",
							"from azure.storage.blob import BlobServiceClient\r\n",
							"import io\r\n",
							"import pandas as pd\r\n",
							"import csv"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# %%sql\r\n",
							"# USE default;\r\n",
							"# DROP DATABASE  gold CASCADE ;\r\n",
							"# USE default;\r\n",
							"# CREATE DATABASE IF NOT EXISTS gold;\r\n",
							"# USE gold;"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"USE gold;\r\n",
							"DROP TABLE IF EXISTS fact_Sales;"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"factList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Sales\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idDesgloseVenta AS idSales,\r\n",
							"                        d.idArticulo AS idArticles,\r\n",
							"                        c.idAlmacen AS idWarehouse,\r\n",
							"                        c.idCliente AS idClient,\r\n",
							"                        c.idCodigoPostal AS idPostalCode,\r\n",
							"                        c.idDivisa AS idCurrency,\r\n",
							"                        c.idTarifa AS idTariff,\r\n",
							"                        c.idTipoOperacion AS idOperationType,\r\n",
							"                        c.idHora AS idHours,\r\n",
							"                        c.idFecha AS idDate,\r\n",
							"                        f.fecha AS  date,\r\n",
							"                        CAST(CONCAT(DATE_FORMAT(f.fecha, 'yyyy-MM-dd'), ' ', h.horaCompleta) AS TIMESTAMP) AS dateOp,\r\n",
							"                        COALESCE(c.codigoTicket, 'D') AS ticketNumber,\r\n",
							"                        COALESCE(d.Cantidad, 0) AS quantity,\r\n",
							"                        COALESCE(d.PrecioUnitario, 0)  AS unitPrice,\r\n",
							"                        COALESCE(d.CosteUnitario, 0)  AS unitCost,\r\n",
							"                        COALESCE(d.importeBruto, 0)  AS amtTotalEuros,\r\n",
							"                        COALESCE(d.importeNeto, 0)  AS amtNetEuros,\r\n",
							"                        COALESCE(d.importeDescuento, 0)  AS amtTotalEurosDiscount,\r\n",
							"                        COALESCE(d.importeNetoDescuento, 0) AS amtNetEurosDiscount,\r\n",
							"                        CASE WHEN idTarifa IN (2,3) THEN 1\r\n",
							"                            ELSE 0 \r\n",
							"                        END AS discountSale,\r\n",
							"                        CASE WHEN idTarifa = 0 THEN 0\r\n",
							"                            ELSE (d.importeBruto - d.importeDescuento) \r\n",
							"                        END AS amtTotalDiscount,\r\n",
							"                        CASE WHEN idTarifa = 0 THEN 0 \r\n",
							"                            ELSE (d.importeNeto - d.importeNetoDescuento) \r\n",
							"                        END AS amtNetDiscount,\r\n",
							"                        GREATEST(d.fechaCarga, c.fechaCarga) AS loadDate,\r\n",
							"                        GREATEST(d.fechaDelta, c.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.desgloseVenta AS d\r\n",
							"                    INNER JOIN silver.cabeceraVenta AS c\r\n",
							"                        ON d.idcabecerasVentas = c.idcabeceraVenta\r\n",
							"                    LEFT JOIN silver.Fechas  AS f\r\n",
							"                        On f.idFechas = c.idFecha\r\n",
							"                    LEFT JOIN silver.horas  AS h\r\n",
							"                        On h.idHoras = c.idHora\r\n",
							"                    WHERE GREATEST(d.fechaDelta, c.fechaDelta) >=\r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dimensionsList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Articles\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idArticulos                                                                                             AS idArticles,\r\n",
							"                            COALESCE(nombre, 'D')                                                                                   AS name,\r\n",
							"                            COALESCE(descripcion, 'D')                                                                              AS description,\r\n",
							"                            COALESCE(codigoReferencia, 'D')                                                                         AS externalCode,\r\n",
							"                            COALESCE(t.talla, 'D')                                                                                  AS size,\r\n",
							"                            COALESCE( t.numeroTalla, -1)                                                                            AS numSize,\r\n",
							"                            COALESCE(co.color, 'D')                                                                                 AS colour,\r\n",
							"                            COALESCE(ca.categoria, 'D')                                                                             AS category,\r\n",
							"                            COALESCE(l.codigoLinea, 'D')                                                                            AS codLine,\r\n",
							"                            COALESCE(l.Linea, 'D')                                                                                  AS line, \r\n",
							"                            CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                                 WHEN a.idCategoria IN (2,4,6,7,9) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                                 ELSE CAST(CONCAT('PV',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                            END                                                                                                     AS season,              \r\n",
							"                            GREATEST(a.fechaCarga, t.fechaCarga, co.fechaCarga, ca.fechaCarga, l.fechaCarga)                        AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta)                        AS deltaDate\r\n",
							"                    FROM silver.articulos AS a\r\n",
							"                        LEFT JOIN silver.talla AS t\r\n",
							"                            ON t.idTalla = a.idTalla\r\n",
							"                        LEFT JOIN silver.color AS co\r\n",
							"                            ON co.idColor = a.idColor\r\n",
							"                        LEFT JOIN silver.categoria AS ca -- select * from silver.categoria\r\n",
							"                            ON ca.idCategoria = a.idCategoria\r\n",
							"                        LEFT JOIN silver.Linea AS l\r\n",
							"                            ON l.idLinea = a.idLinea\r\n",
							"                    WHERE GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Client\",\r\n",
							"            \"query\": \"\"\"              \r\n",
							"                    SELECT  idCliente                                           AS idClient,\r\n",
							"                            COALESCE(c.nombre, 'D')                             AS name,\r\n",
							"                            COALESCE(apellido1, 'D')                            AS lastName1,\r\n",
							"                            COALESCE(apellido2, 'D')                            AS lastName2, \r\n",
							"                            COALESCE(email, 'D')                                AS email,\r\n",
							"                            COALESCE(telefono, 'D')                             AS phoneNumber,\r\n",
							"                            COALESCE(CAST(cumpleanos AS STRING), '1900-01-01')  AS birthDay,       \r\n",
							"                            YEAR(current_date()) - YEAR(CAST(cumpleanos AS DATE)) \r\n",
							"                                - CASE \r\n",
							"                                    WHEN MONTH(current_date()) < MONTH(CAST(cumpleanos AS DATE)) \r\n",
							"                                        OR (MONTH(current_date()) = MONTH(CAST(cumpleanos AS DATE)) AND DAY(current_date()) < DAY(CAST(cumpleanos AS DATE)))\r\n",
							"                                    THEN 1\r\n",
							"                                    ELSE 0\r\n",
							"                            END                                                 AS age,\r\n",
							"                            CASE WHEN hombre = 1 THEN 'Hombre' \r\n",
							"                                 ELSE 'Mujer' \r\n",
							"                            END                                                 AS gender,\r\n",
							"                            COALESCE(p.nombre, 'D')                             AS country,\r\n",
							"                            COALESCE(p.codigoPais, 'D')                         AS countryCode,\r\n",
							"                            COALESCE(cp.region, 'D')                            AS region,\r\n",
							"                            COALESCE(c.Direcion, 'D')                           AS address,  \r\n",
							"                            COALESCE(codigoPostal, 'D')                         AS postalCode,\r\n",
							"                            COALESCE(activo, false)                             AS active,  -- Cambiado 0 a false aquÃ­\r\n",
							"                            GREATEST(c.fechaCarga, p.fechaCarga, cp.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) AS deltaDate\r\n",
							"                        FROM silver.cliente AS c\r\n",
							"                            LEFT JOIN silver.pais AS p \r\n",
							"                                ON c.idPais = p.idPais\r\n",
							"                            INNER JOIN silver.codigoPostal AS cp \r\n",
							"                                ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\r\n",
							"                        WHERE GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Currency\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idDivisa              AS idCurrency,\r\n",
							"                            COALESCE(nombre, 'D') AS name,\r\n",
							"                            COALESCE(Divisa, 'D') AS currency,\r\n",
							"                            fechaCarga            AS loadDate,\r\n",
							"                            fechaDelta            AS deltaDate\r\n",
							"                    FROM silver.divisa\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Date\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idFechas                                  AS idDate,\r\n",
							"                            ClaveFecha                                AS dateKey,\r\n",
							"                            Fecha                                     AS date,\r\n",
							"                            COALESCE(Mes, 'D')                        AS month,\r\n",
							"                            COALESCE(NumeroMes, -1)                   AS monthNumber,\r\n",
							"                            COALESCE(Ano, -1)                         AS year,\r\n",
							"                            COALESCE(NumeroSemana, -1)                AS weekNumber,\r\n",
							"                            COALESCE(DiaSemana, 'D')                  AS dayWeek,\r\n",
							"                            COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\r\n",
							"                            COALESCE(DiaAno, -1)                      AS yearDay,\r\n",
							"                            CASE WHEN Trimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Trimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Trimestre = 3 THEN 'Q3'\r\n",
							"                                ELSE '-1' END                         AS quarter,\r\n",
							"                            CASE WHEN Cuatrimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Cuatrimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Cuatrimestre = 3 THEN 'Q3'\r\n",
							"                                WHEN Cuatrimestre = 4 THEN 'Q4'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS quadrimester,\r\n",
							"                            CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\r\n",
							"                                WHEN Cuatrimestre IN (2,4) THEN 'S2'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS semester,\r\n",
							"                            fechaCarga                                AS loadDate,\r\n",
							"                            fechaDelta                                AS deltaDate\r\n",
							"                    FROM silver.fechas\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Hours\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idHoras                     AS idHours,\r\n",
							"                            COALESCE(Hora, -1)          AS hour,\r\n",
							"                            COALESCE(Minuto, -1)        AS minute,\r\n",
							"                            COALESCE(HoraCompleta, 'D') AS fullHour,\r\n",
							"                            fechaCarga                  AS loadDate,\r\n",
							"                            fechaDelta                  AS deltaDate\r\n",
							"                    FROM silver.horas\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"OperationType\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTipoOperacion          AS idOperationType,\r\n",
							"                            COALESCE(Operacion, 'D') AS operation,\r\n",
							"                            fechaCarga               AS loadDate,\r\n",
							"                            fechaDelta               AS deltaDate\r\n",
							"                    FROM silver.tipoOperacion\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"PostalCode\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idCodigoPostal                       AS idPostalCode,\r\n",
							"                            COALESCE(codigoPostal, 'D')          AS postalCode,\r\n",
							"                            COALESCE(region, 'D')                AS region,\r\n",
							"                            COALESCE(c.codigoPais, 'D')          AS countryCode,\r\n",
							"                            COALESCE(nombre, 'D')                AS country,\r\n",
							"                            GREATEST(c.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(c.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.codigoPostal AS c\r\n",
							"                    LEFT JOIN silver.pais AS p\r\n",
							"                        ON p.codigoPais = c.codigoPais\r\n",
							"                    WHERE GREATEST(c.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Tariff\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTarifa              AS idTariff,\r\n",
							"                            COALESCE(Tarifa, 'D') AS tariff,\r\n",
							"                            fechaCarga            AS loadDate,\r\n",
							"                            fechaDelta            AS deltaDate       \r\n",
							"                    FROM silver.tarifa\r\n",
							"                    WHERE fechaDelta >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Warehouse\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idAlmacenes                          AS idWarehouse,\r\n",
							"                            COALESCE(a.Nombre, 'D')              AS warehouse,\r\n",
							"                            COALESCE(a.codigoAlmacen, 'D')       AS externalCode,\r\n",
							"                            COALESCE(p.codigoPais, 'D')          AS countryCode, \r\n",
							"                            COALESCE(p.nombre, 'D')              AS country,\r\n",
							"                            COALESCE(a.ciudad, 'D')              AS city,\r\n",
							"                            COALESCE(a.Direccion, 'D')           AS address,\r\n",
							"                            COALESCE(a.descripcion, 'D')         AS description,\r\n",
							"                            GREATEST(a.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.almacenes AS a\r\n",
							"                    LEFT JOIN silver.pais AS p\r\n",
							"                        ON p.idpais = a.idPais\r\n",
							"                    --WHERE a.fechaDelta <> 20241011\r\n",
							"                    --UNION \r\n",
							"                    --select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\r\n",
							"                    --UNION \r\n",
							"                    --select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\r\n",
							"                    WHERE GREATEST(a.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Configuration**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# parameter string to array\r\n",
							"tables_to_load = json.loads(tables_to_load)\r\n",
							"\r\n",
							"today_datetime = datetime.now()\r\n",
							"\r\n",
							"delta_date_filter = (today_datetime.strftime('%Y%m%d')) # 'yyyyMMdd'\r\n",
							"#delta_date_filter = '20241008'\r\n",
							"\r\n",
							"delta_date_string = today_datetime.strftime('%Y-%m-%d') # 'yyyy-mm-dd'\r\n",
							"#delta_date_string = '2024-10-08'\r\n",
							"\r\n",
							"# Convertir cada valor del CSV en un diccionario estructurado\r\n",
							"dict_tables_to_load = []\r\n",
							"\r\n",
							"for row in tables_to_load:\r\n",
							"    #csv_data = row[\"SchemaName,TableName,UpdateDate,Load\"]\r\n",
							"    \r\n",
							"    # Usar el lector de csv para separar los campos\r\n",
							"    #reader = csv.reader(io.StringIO(csv_data))\r\n",
							"    \r\n",
							"    # for schema, table, update_date, load in reader:\r\n",
							"    #     dict_tables_to_load.append({\r\n",
							"    #         \"SchemaName\": schema,\r\n",
							"    #         \"TableName\": table,\r\n",
							"    #         \"UpdateDate\": update_date,\r\n",
							"    #         \"Load\": load\r\n",
							"    #     })\r\n",
							"    dict_tables_to_load.append({\r\n",
							"    \"SchemaName\": row[\"SchemaName\"],\r\n",
							"    \"TableName\": row[\"TableName\"],\r\n",
							"    \"UpdateDate\": row[\"UpdateDate\"],\r\n",
							"    \"Load\": row[\"Load\"]\r\n",
							"})\r\n",
							"\r\n",
							"# Variables para almacenar los datos de 'dim' y 'fact'\r\n",
							"dim_tables = []\r\n",
							"fact_tables = []\r\n",
							"\r\n",
							"# Clasificar los datos\r\n",
							"for row in dict_tables_to_load:\r\n",
							"    if row['SchemaName'] == 'dim':\r\n",
							"        dim_tables.append(row)\r\n",
							"    elif row['SchemaName'] == 'fact':\r\n",
							"        fact_tables.append(row)\r\n",
							"\r\n",
							"\r\n",
							"load_y_tables = {table['TableName'] for table in dim_tables if table['Load'] == 'Y'}\r\n",
							"# Filtrar el dimensionsList para obtener solo las tablas con Load='Y' y devolver una lista de diccionarios\r\n",
							"filtered_dimensionsList = [\r\n",
							"    dim for dim in dimensionsList if dim[\"name\"] in load_y_tables\r\n",
							"]\r\n",
							"\r\n",
							"load_y_tables2 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
							"#print(load_y_tables2)\r\n",
							"# Filtrar el dimensionsList para obtener solo las tablas con Load='Y' y devolver una lista de diccionarios\r\n",
							"filtered_factList = [\r\n",
							"    dim for dim in factList if dim[\"name\"] in load_y_tables2\r\n",
							"]"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"results = []\r\n",
							"exception = []"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Load SCD2 Dimensions**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Carga o Merge Dimensiones\r\n",
							"results = []\r\n",
							"exception = []\r\n",
							"\r\n",
							"for file in filtered_dimensionsList:\r\n",
							"    file_result = {\r\n",
							"        'table': f\"dim_{file['name']}\",\r\n",
							"        'status': 'Incorrect',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now()) \r\n",
							"    }\r\n",
							"\r\n",
							"    try:\r\n",
							"\r\n",
							"        table_name = file[\"name\"].split('_')[0]\r\n",
							"        #source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
							"        key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
							"\r\n",
							"        key_columns = key_columns_str.split(',')\r\n",
							"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"        #delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Dimensions/{table_name}\"\r\n",
							"        delta_table_path_dimensions = f\"{data_lake_container}/{golden_folder_dimensions}/{table_name}\"\r\n",
							"        # Leer archivo(s) en DataFrame de Spark\r\n",
							"        # Meter filtro deltaDate\r\n",
							"\r\n",
							"        df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
							"        sdf = df.cache()\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"There is no new data\"\r\n",
							"            results.append(file_result)\r\n",
							"            continue\r\n",
							"\r\n",
							"        \r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path_dimensions):\r\n",
							"            # AÃ±adir las columnas necesarias\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            todas_las_columnas = sdf.columns\r\n",
							"            columnas_al_principio = [f\"id{table_name}\"]\r\n",
							"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
							"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
							"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
							"            sdf = sdf.select(*nuevo_orden_columnas)\r\n",
							"\r\n",
							"            next_surrogate_key = spark.sql(f\"SELECT COALESCE(MAX(sk{table_name}), 0) + 1 AS next_key FROM gold.dim_{table_name}\").collect()[0][\"next_key\"]\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_view\")\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                MERGE INTO gold.dim_{table_name}  \r\n",
							"                AS existing\r\n",
							"                USING temp_view AS updates\r\n",
							"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
							"                WHEN MATCHED AND existing.isCurrent = 1 AND existing.hash != updates.hash THEN\r\n",
							"                UPDATE SET\r\n",
							"                    existing.toDate = current_date(),\r\n",
							"                    existing.isCurrent = 0\r\n",
							"            \"\"\")\r\n",
							"\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                INSERT INTO gold.dim_{table_name}    \r\n",
							"                SELECT \r\n",
							"                    {next_surrogate_key} + row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS sk{table_name}, \r\n",
							"                    updates.*\r\n",
							"                FROM temp_view AS updates\r\n",
							"                LEFT JOIN gold.dim_{table_name}  \r\n",
							"                AS existing\r\n",
							"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
							"                WHERE existing.{key_columns_str} IS NULL OR existing.isCurrent = 0 OR existing.hash != updates.hash\r\n",
							"            \"\"\")\r\n",
							"\r\n",
							"            # Almacenar conteos de filas\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.dim_{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            if last_result:\r\n",
							"                file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
							"                file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
							"            else:\r\n",
							"                print(f\"No hay registros de operaciÃ³n MERGE para la tabla {file['name']}\")\r\n",
							"                file_result['inserted_rows'] = 0\r\n",
							"                file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"        else: # Crear nueva tabla Delta\r\n",
							"      \r\n",
							"            last_surrogate_key = 0  # Iniciar con 0 si la tabla no existe\r\n",
							"            #sdf = sdf.withColumn(f\"sk{table_name}\", F.monotonically_increasing_id() + last_surrogate_key + 1)  # Asigna un valor Ãºnico a cada fila.\r\n",
							"            sdf = sdf.withColumn(f\"sk{table_name}\", F.col(f\"id{table_name}\"))\r\n",
							"            # AÃ±adir las columnas necesarias\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            # Hash\r\n",
							"            #columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\" ,'fechaCarga', 'fechaDelta','fromDate', 'toDate', 'isCurrent')]\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\" ,'loadDate', 'deltaDate','fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # Reorganizar la estructura de la tabla\r\n",
							"            todas_las_columnas = sdf.columns\r\n",
							"            columnas_al_principio = [f\"sk{table_name}\", f\"id{table_name}\"]\r\n",
							"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
							"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
							"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
							"            sdf_reordenado = sdf.select(*nuevo_orden_columnas)\r\n",
							"            \r\n",
							"            # Crear la tabla Delta\r\n",
							"            sdf_reordenado.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path_dimensions)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.dim_{table_name} USING DELTA LOCATION \\'{delta_table_path_dimensions}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            #spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"            #spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')\")\r\n",
							"\r\n",
							"            # Almacenar archivo procesado correctamente\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"            file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Load Fact Table**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for file in filtered_factList:\r\n",
							"    file_result = {\r\n",
							"        'table': f\"fact_{file['name']}\",\r\n",
							"        'status': 'Incorrect',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now())  \r\n",
							"    }\r\n",
							"    table_name = file[\"name\"].split('_')[0]\r\n",
							"    key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
							"\r\n",
							"    key_columns = key_columns_str.split(',')\r\n",
							"    conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"    # Definir la ruta de la tabla Delta\r\n",
							"    delta_table_path_fact = f\"{data_lake_container}/{golden_folder_fact}/{table_name}\"\r\n",
							"    #delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Fact/{table_name}\"\r\n",
							"    #file[\"query\"] = file[\"query\"].strip() + f\" '{delta_date_filter}'\"\r\n",
							"    #df = spark.sql(file[\"query\"]) \r\n",
							"    df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
							"    sdf = df.cache()\r\n",
							"\r\n",
							"    try:\r\n",
							"\r\n",
							"        # Check empty query\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"There is no new data\"\r\n",
							"            results.append(file_result)\r\n",
							"            continue\r\n",
							"\r\n",
							"        # Truncar la tabla Delta si existe\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path_fact):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path_fact)\r\n",
							"            existing_columns = delta_table.toDF().columns\r\n",
							"            #delta_table.delete()  # Truncar la tabla Delta\r\n",
							"\r\n",
							"            #last_row number para incremental\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
							"            spark.sql(f\"\"\" \r\n",
							"                SELECT  s.*, \r\n",
							"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
							"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
							"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
							"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
							"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
							"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
							"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
							"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
							"                        COALESCE(d.skDate, -1) AS skDate\r\n",
							"                FROM temp_sales_view as s\r\n",
							"                LEFT JOIN gold.dim_Articles AS a\r\n",
							"                    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
							"                LEFT JOIN gold.dim_Client AS cl\r\n",
							"                    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
							"                LEFT JOIN gold.dim_Currency AS cu\r\n",
							"                    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
							"                LEFT JOIN gold.dim_Date AS d\r\n",
							"                    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
							"                LEFT JOIN gold.dim_Hours AS h\r\n",
							"                    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
							"                LEFT JOIN gold.dim_OperationType AS o\r\n",
							"                    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
							"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
							"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
							"                LEFT JOIN gold.dim_Tariff AS t\r\n",
							"                    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
							"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
							"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
							"                \"\"\").createOrReplaceTempView(\"temp_sales_extended_view\")\r\n",
							"\r\n",
							"                #columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\", 'fechaCarga', 'fechaDelta', 'fromDate', 'toDate', 'isCurrent')]\r\n",
							"\r\n",
							"            # 2. Usar la vista temporal existente y crear un nuevo DataFrame con el hash\r\n",
							"            temp_sales_df = spark.sql(\"SELECT * FROM temp_sales_extended_view\")\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in temp_sales_df.columns if col not in (f\"sk{table_name}\", 'loadDate', 'deltaDate')]\r\n",
							"            # 3. AÃ±adir la columna de hash al DataFrame\r\n",
							"            temp_sales_df_with_hash = temp_sales_df.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # 4. Crear una nueva vista temporal que incluya el hash\r\n",
							"            temp_sales_df_with_hash.createOrReplaceTempView(\"temp_sales_extended_view_with_hash\")\r\n",
							"\r\n",
							"\r\n",
							"            #next_surrogate_key = spark.sql(f\"SELECT COALESCE(MAX(sk{table_name}), 0) + 1 AS next_key FROM gold.fact_{table_name}\").collect()[0][\"next_key\"]\r\n",
							"            # Comprobar si la tabla Delta existe\r\n",
							" \r\n",
							"                \r\n",
							"            # Obtener la Ãºltima surrogate key para crear nuevas\r\n",
							"            last_surrogate_key = delta_table.toDF().agg(F.max(f\"sk{table_name}\")).collect()[0][0] or 0\r\n",
							"            next_surrogate_key = last_surrogate_key + 1\r\n",
							"            \r\n",
							"            # Merge new data into existing table con lÃ³gica SCD Tipo 2\r\n",
							"            delta_table.alias(\"existing\").merge(\r\n",
							"                source=temp_sales_df_with_hash.alias(\"updates\"),\r\n",
							"                condition=\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])\r\n",
							"            ).whenMatchedUpdate(\r\n",
							"                condition=\"existing.hash != updates.hash\",\r\n",
							"                set={\r\n",
							"                    **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('loadDate', 'skSales')}  # Actualiza todos los campos menos `fechaCarga`\r\n",
							"\r\n",
							"                }\r\n",
							"            ).whenNotMatchedInsert(\r\n",
							"                values={\r\n",
							"                    f\"sk{table_name}\": F.expr(f\"{next_surrogate_key} + row_number() over (order by (select null)) - 1\"),  # Aumenta la surrogate key en base a la fila\r\n",
							"                    **{f\"updates.{col}\": f\"updates.{col}\" for col in existing_columns}\r\n",
							"                }\r\n",
							"            ).execute()\r\n",
							"\r\n",
							"\r\n",
							"            # Almacenar conteos de filas\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.fact_{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            #file_result['inserted_rows'] = last_result[0].operationMetrics[\"numTargetRowsInserted\"]\r\n",
							"            #file_result['updated_rows'] = last_result[0].operationMetrics[\"numTargetRowsUpdated\"]\r\n",
							"            if last_result:\r\n",
							"                file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
							"                file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
							"            else:\r\n",
							"                #print(f\"No hay registros de operaciÃ³n MERGE para la tabla {file['name']}\")\r\n",
							"                file_result['inserted_rows'] = 0\r\n",
							"                file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"        else:\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
							"            sdf = spark.sql(f\"\"\" \r\n",
							"                SELECT ROW_NUMBER() OVER(ORDER BY s.dateOP) AS skSales, s.*, \r\n",
							"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
							"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
							"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
							"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
							"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
							"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
							"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
							"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
							"                        COALESCE(d.skDate, -1) AS skDate\r\n",
							"                FROM temp_sales_view AS s\r\n",
							"                LEFT JOIN gold.dim_Articles AS a\r\n",
							"                    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
							"                LEFT JOIN gold.dim_Client AS cl\r\n",
							"                    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
							"                LEFT JOIN gold.dim_Currency AS cu\r\n",
							"                    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
							"                LEFT JOIN gold.dim_Date AS d\r\n",
							"                    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
							"                LEFT JOIN gold.dim_Hours AS h\r\n",
							"                    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
							"                LEFT JOIN gold.dim_OperationType AS o\r\n",
							"                    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
							"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
							"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
							"                LEFT JOIN gold.dim_Tariff AS t\r\n",
							"                    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
							"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
							"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
							"                \"\"\")\r\n",
							"\r\n",
							"            #temp_sales_df = spark.sql(\"SELECT * FROM temp_sales_extended_view\")\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\", 'fechaCarga', 'fechaDelta')]\r\n",
							"            # 3. AÃ±adir la columna de hash al DataFrame\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # 4. Crear una nueva vista temporal que incluya el hash\r\n",
							"            #temp_sales_df_with_hash.createOrReplaceTempView(\"temp_sales_extended_view_with_hash\")\r\n",
							"\r\n",
							"\r\n",
							"        # Cargar datos en la tabla Delta\r\n",
							"            sdf.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_fact)\r\n",
							"            #sdf.write.format(\"delta\").mode(\"append\").save(delta_table_path)  # valorar carga incremental mÃ¡s adelante\r\n",
							"\r\n",
							"            # Crear la tabla Delta en caso de que no exista\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.fact_{table_name} USING DELTA LOCATION \\'{delta_table_path_fact}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"            \r\n",
							"        # Almacenar conteos de filas\r\n",
							"        file_result['status'] = 'Correct'\r\n",
							"        file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Guardar ficheros Logs**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Obtener la fecha actual\r\n",
							"fecha_actual = datetime.now()\r\n",
							"year = fecha_actual.strftime('%Y')  # AÃ±o actual (por ejemplo, '2024')\r\n",
							"month = fecha_actual.strftime('%m')  # Mes actual (por ejemplo, '10')\r\n",
							"day = fecha_actual.strftime('%d')  # DÃ­a actual (por ejemplo, '24')\r\n",
							"hora = fecha_actual.strftime('%H%M%S')  # Hora actual en formato HHMMSS (por ejemplo, '235959')\r\n",
							"\r\n",
							"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
							"archivo_nombre = f\"Log_{day}_{hora}.json\"\r\n",
							"\r\n",
							"# Reemplaza con tu cadena de conexiÃ³n a Azure\r\n",
							"\r\n",
							"# Cliente de servicio de blobs\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# Convetimos el diccionario a formato json\r\n",
							"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
							"\r\n",
							"# Crear la ruta de destino con la jerarquÃ­a AÃ±o/Mes y el nombre del archivo\r\n",
							"destination_blob_name = f\"{gold_folder_logs}/{year}/{month}/{archivo_nombre}\"\r\n",
							"\r\n",
							"# Crear un cliente del blob de destino\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=container_name, blob=destination_blob_name)\r\n",
							"\r\n",
							"destination_blob.upload_blob(json_data, overwrite=True)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Actualizar fichero metadata**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Configurar la conexiÃ³n\r\n",
							"\r\n",
							"#delta_date_string = '1900-01-01'\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# Definir los nombres de los contenedores y blobs\r\n",
							"#container_name = \"etl/bronze/Configuration\"\r\n",
							"#metadata_folder = \"mdw/bronze/Configuration\"\r\n",
							"#container_name = metadata_folder\r\n",
							"csv_blob_name = \"ConfiguracionReporting.csv\"  # Reemplaza con el nombre de tu archivo\r\n",
							"\r\n",
							"# Leer el archivo CSV desde el Data Lake\r\n",
							"blob_client = blob_service_client.get_blob_client(container=metadata_folder, blob=csv_blob_name)\r\n",
							"\r\n",
							"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
							"df = pd.read_csv(stream, sep=',')  # Especifica el separador aquÃ­\r\n",
							"\r\n",
							"# Limpiar nombres de columnas para eliminar espacios en blanco\r\n",
							"df.columns = df.columns.str.strip()\r\n",
							"\r\n",
							"names_dimensionsList = [item['name'] for item in filtered_dimensionsList]\r\n",
							"for i in names_dimensionsList:\r\n",
							"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"\r\n",
							"names_factList = [item['name'] for item in filtered_factList]\r\n",
							"for i in names_factList:\r\n",
							"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"\r\n",
							"\r\n",
							"# Guardar el DataFrame modificado en un nuevo flujo de bytes\r\n",
							"output_stream = io.BytesIO()\r\n",
							"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')  # Usar line_terminator para evitar lÃ­neas en blanco\r\n",
							"output_stream.seek(0)  # Regresar al inicio del flujo\r\n",
							"\r\n",
							"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"        # # Comprobar si la tabla Delta existe\r\n",
							"        # if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"        #     delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"        #     existing_columns = delta_table.toDF().columns\r\n",
							"            \r\n",
							"        #     # Obtener la Ãºltima surrogate key para crear nuevas\r\n",
							"        #     last_surrogate_key = delta_table.toDF().agg(F.max(f\"sk{table_name}\")).collect()[0][0] or 0\r\n",
							"        #     next_surrogate_key = last_surrogate_key + 1\r\n",
							"            \r\n",
							"        #     # Merge new data into existing table con lÃ³gica SCD Tipo 2\r\n",
							"        #     delta_table.alias(\"existing\").merge(\r\n",
							"        #         source=sdf.alias(\"updates\"),\r\n",
							"        #         condition=\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])\r\n",
							"        #     ).whenMatchedUpdate(\r\n",
							"        #         condition=\"existing.isCurrent = 1 AND existing.hash != updates.hash\",\r\n",
							"        #         set={\r\n",
							"        #             \"toDate\": F.current_date(),\r\n",
							"        #             \"isCurrent\": F.lit(0)\r\n",
							"        #         }\r\n",
							"        #     ).whenNotMatchedInsert(\r\n",
							"        #         values={\r\n",
							"        #             f\"sk{table_name}\": F.expr(f\"{next_surrogate_key} + row_number() over (order by (select null)) - 1\"),  # Aumenta la surrogate key en base a la fila\r\n",
							"        #             **{f\"updates.{col}\": f\"updates.{col}\" for col in existing_columns}\r\n",
							"        #         }\r\n",
							"        #     ).execute()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MergeLandingFilesToSilver')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cecc213f-355d-4412-90a1-36f38cfcf5fd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Import modules**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format, concat_ws, xxhash64\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
							"import re\r\n",
							"import json\r\n",
							"from datetime import datetime\r\n",
							"import pandas as pd\r\n",
							"import io\r\n",
							"from pyspark.sql import functions as F"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#%%sql\r\n",
							"# USE default;\r\n",
							"# DROP DATABASE  silver CASCADE ;\r\n",
							"# USE default;\r\n",
							"# CREATE DATABASE IF NOT EXISTS silver;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Configuration**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Connection to our ADLG2\r\n",
							"\r\n",
							"today_date = datetime.now()\r\n",
							"\r\n",
							"delta_date_filter = (today_date.strftime('%Y%m%d')) # Format 'yyyymmdd'\r\n",
							"#delta_date_filter = '20241010'# test date\r\n",
							"\r\n",
							"delta_date_string = today_date.strftime('%Y-%m-%d') # Format 'yyyy-mm-dd'\r\n",
							"#delta_date_string = '2024-10-10' # test date\r\n",
							"\r\n",
							"landing_files = json.loads(landing_files)  # String parameter to array again\r\n",
							"\r\n",
							"# List to save results\r\n",
							"results = []\r\n",
							"correct_files = []\r\n",
							"incorrect_files = []"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Load/Merge parquet Landing files to silver**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for file in landing_files:\r\n",
							"    file_result = {\r\n",
							"        'table': file[\"name\"][:file[\"name\"].index('_')],  \r\n",
							"        'file_name': file['name'],\r\n",
							"        'status': 'Incorrecto',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now())  \r\n",
							"    }\r\n",
							"\r\n",
							"\r\n",
							"    try:\r\n",
							"        table_name = file[\"name\"][:file[\"name\"].index('_')]  # Name of the table\r\n",
							"        source_wildcard = f\"{table_name}*.parquet\"   # Wildcard for files\r\n",
							"        key_columns_str = f\"id{table_name}\" \r\n",
							"\r\n",
							"        source_path = data_lake_container + '/' + bronze_landing + '/' + source_wildcard\r\n",
							"        delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"        sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"        sdf.cache()\r\n",
							"\r\n",
							"        # fecha_carga_actualizada = 20241007\r\n",
							"\r\n",
							"        # if table_name == 'fechas':\r\n",
							"        #     sdf = sdf.withColumn(\"fechaCarga\", F.lit(fecha_carga_actualizada))\r\n",
							"\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"File with no data\"\r\n",
							"            results.append(file_result)\r\n",
							"            correct_files.append(file['name'])\r\n",
							"            continue\r\n",
							"\r\n",
							"        key_columns = key_columns_str.split(',')\r\n",
							"        #key_columns = key_columns_str\r\n",
							"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"        sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")  # DROP column we don't need\r\n",
							"\r\n",
							"        # Check Delta Table exists\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"            existing_columns = delta_table.toDF().columns\r\n",
							"            \r\n",
							"            # ADD fechaDelta\r\n",
							"            sdf = sdf.withColumn(\"fechaDelta\", lit(delta_date_filter))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # Create update set excluding fechaCarga\r\n",
							"            update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"            # Merge new data into existing table y captura el conteo\r\n",
							"            merge_result = delta_table.alias(\"existing\").merge(\r\n",
							"                source=sdf.alias(\"updates\"),\r\n",
							"                condition=\" AND \".join(conditions_list)\r\n",
							"            ).whenMatchedUpdate(\r\n",
							"                condition=\" AND \".join(conditions_list + [\"existing.hash != updates.hash\"]),\r\n",
							"                set={\r\n",
							"                    #\"fechaCarga\": \"existing.fechaCarga\",  # comment to not update\r\n",
							"                    \"fechaDelta\": delta_date_filter,\r\n",
							"                    **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}\r\n",
							"                }\r\n",
							"            ).whenNotMatchedInsert(\r\n",
							"                values={\r\n",
							"                    \"fechaCarga\": \"updates.fechaCarga\",\r\n",
							"                    \"fechaDelta\": delta_date_filter,\r\n",
							"                    **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}\r\n",
							"                }\r\n",
							"            ).execute()\r\n",
							"\r\n",
							"            # Save metadata\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY silver.{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            if last_result:\r\n",
							"                file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
							"                file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
							"            else:\r\n",
							"                print(f\"No hay registros de operaciÃ³n MERGE para la tabla {file['name']}\")\r\n",
							"                file_result['inserted_rows'] = 0\r\n",
							"                file_result['updated_rows'] = 0\r\n",
							"            \r\n",
							"            correct_files.append(file['name'])  # Add to correct files list\r\n",
							"\r\n",
							"\r\n",
							"        else:\r\n",
							"            # Create new Delta Table\r\n",
							"            sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga) \r\n",
							"            sdf = sdf.withColumn(\"fechaDelta\", lit(delta_date_filter))\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"            sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS silver.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE silver.{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE silver.{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"        \r\n",
							"            # Save metadata\r\n",
							"            file_result['status'] = 'Correcto'\r\n",
							"            file_result['inserted_rows'] = sdf.count()  # Count new rows\r\n",
							"            file_result['updated_rows'] = 0  # No updates rows in the firs load\r\n",
							"            correct_files.append(file['name'])  # Add to correct files list\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result) \r\n",
							"        incorrect_files.append(file['name'])  # Add to incorrect files list\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)  # Agregar resultado al final del bucle\r\n",
							"\r\n",
							"\r\n",
							"# Print result to debug\r\n",
							"for result in results:\r\n",
							"    print(f\"Archivo: {result['file_name']}, Estado: {result['status']}, Filas insertadas: {result['inserted_rows']}, Filas actualizadas: {result['updated_rows']}\")\r\n",
							"\r\n",
							"# Save correct and incorrect file names in pipeline variables\r\n",
							"correct_files_variable = correct_files\r\n",
							"incorrect_files_variable = incorrect_files\r\n",
							"\r\n",
							"# Print result to debug\r\n",
							"print(\"Archivos correctos:\", correct_files_variable)\r\n",
							"print(\"Archivos incorrectos:\", incorrect_files_variable)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Save log merge/load**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get current date\r\n",
							"# actual_time = datetime.now()\r\n",
							"# year = actual_time.strftime('%Y')\r\n",
							"# month = actual_time.strftime('%m')\r\n",
							"# day = actual_time.strftime('%d')\r\n",
							"# hour = actual_time.strftime('%H%M%S')\r\n",
							"\r\n",
							"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
							"#log_file_name = f\"Log_{day}_{hour}.json\"\r\n",
							"log_file_name = f\"Log_{executionID}.json\"\r\n",
							"\r\n",
							"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
							"\r\n",
							"# Crear la ruta de destino con la jerarquÃ­a AÃ±o/Mes y el nombre del archivo\r\n",
							"destination_blob_name = f\"{year}/{month}/{log_file_name}\"\r\n",
							"\r\n",
							"# Crear un cliente del blob de destino\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=silver_logs, blob=destination_blob_name)\r\n",
							"\r\n",
							"destination_blob.upload_blob(json_data, overwrite=True)"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Move files to /Processed or /Error and delete from /Pending**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"results_dict = []\r\n",
							"\r\n",
							"for i in correct_files_variable:\r\n",
							"    source_blob_name = i\r\n",
							"    name_part = i.split('_')[0]\r\n",
							"    \r\n",
							"    # Extract datetime\r\n",
							"    date_match = re.search(r'_(\\d{8})\\.parquet$', i)\r\n",
							"    if date_match:\r\n",
							"        date_str = date_match.group(1)\r\n",
							"        year = date_str[:4] \r\n",
							"        month = date_str[4:6]\r\n",
							"        destination_blob_name = f\"{name_part}/{year}/{month}/{i}\"\r\n",
							"    else:\r\n",
							"        destination_blob_name = i\r\n",
							"    \r\n",
							"    # Create the blob clients\r\n",
							"    source_blob = blob_service_client.get_blob_client(container=bronze_pending, blob=source_blob_name)\r\n",
							"    destination_blob_processed = blob_service_client.get_blob_client(container=bronze_processed, blob=destination_blob_name)\r\n",
							"    \r\n",
							"    # Copy\r\n",
							"    copy_properties = destination_blob_processed.start_copy_from_url(source_blob.url)\r\n",
							"    \r\n",
							"    # Check and delele\r\n",
							"    if copy_properties[\"copy_status\"] == \"success\":\r\n",
							"        source_blob.delete_blob()  # Descomentar para eliminar el blob original\r\n",
							"        results_dict.append({\r\n",
							"            \"file\": source_blob_name,\r\n",
							"            \"status\": \"Correct\",\r\n",
							"            \"destination\": f\"bronze/Processed/{destination_blob_name}\" })\r\n",
							"        #results += f\"Archivo {source_blob_name} procesado exitosamente y movido a bronze/Processed. {str(datetime.now())}\\n\"\r\n",
							"    else:\r\n",
							"        results_dict.append({\r\n",
							"            \"file\": source_blob_name,\r\n",
							"            \"status\": \"Fail to copy file\",\r\n",
							"            \"destination\": f\"bronze/Processed/{destination_blob_name}\" })\r\n",
							"\r\n",
							"# Bucle para archivos incorrectos\r\n",
							"for i in incorrect_files_variable:\r\n",
							"    source_blob_name = i\r\n",
							"    name_part = i.split('_')[0]\r\n",
							"\r\n",
							"    # Extraer fecha\r\n",
							"    date_match = re.search(r'_(\\d{8})\\.parquet$', i)\r\n",
							"    if date_match:\r\n",
							"        date_str = date_match.group(1) \r\n",
							"        year = date_str[:4]  \r\n",
							"        month = date_str[4:6] \r\n",
							"        destination_blob_name = f\"{name_part}/{year}/{month}/{i}\"\r\n",
							"    else:\r\n",
							"        destination_blob_name = i  # Si no tiene fecha, dejar el nombre como estÃ¡\r\n",
							"\r\n",
							"    source_blob = blob_service_client.get_blob_client(container=bronze_pending, blob=source_blob_name)\r\n",
							"    destination_blob_error = blob_service_client.get_blob_client(container=bronze_error, blob=destination_blob_name)\r\n",
							"\r\n",
							"    # Copy\r\n",
							"    copy_properties = destination_blob_error.start_copy_from_url(source_blob.url)\r\n",
							"\r\n",
							"    # Check and delele\r\n",
							"    if copy_properties[\"copy_status\"] == \"success\":\r\n",
							"        source_blob.delete_blob()  # Descomentar para eliminar el blob original\r\n",
							"        results_dict.append({\r\n",
							"            \"file\": source_blob_name,\r\n",
							"            \"status\": \"Incorrect\",\r\n",
							"            \"destination\": f\"bronze/Landing/Error/{destination_blob_name}\" })\r\n",
							"        #results += f\"Archivo {source_blob_name} procesado errÃ³neamente y movido a bronze/Landing/Error. {str(datetime.now())} \\n\"\r\n",
							"    else:\r\n",
							"        results_dict.append({\r\n",
							"            \"file\": source_blob_name,\r\n",
							"            \"status\": \"Fail to copy File\",\r\n",
							"            \"destination\": f\"bronze/Landing/Error/{destination_blob_name}\" })"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Save logs parquet files**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Obtener la fecha actual\r\n",
							"# actual_time = datetime.now()\r\n",
							"# year = actual_time.strftime('%Y')\r\n",
							"# month = actual_time.strftime('%m')\r\n",
							"# day = actual_time.strftime('%d')\r\n",
							"# hour = actual_time.strftime('%H%M%S')\r\n",
							"\r\n",
							"#log_file_name = f\"Log_{day}_{hour}.json\"\r\n",
							"log_file_name = f\"Log_{executionID}.json\"\r\n",
							"\r\n",
							"json_data = json.dumps(results_dict, indent=4, ensure_ascii=False)\r\n",
							"\r\n",
							"# Crear la ruta de destino con la jerarquÃ­a AÃ±o/Mes y el nombre del archivo\r\n",
							"destination_blob_name = f\"{year}/{month}/{log_file_name}\"\r\n",
							"\r\n",
							"# Crear un cliente del blob de destino\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=bronze_logs, blob=destination_blob_name)\r\n",
							"\r\n",
							"destination_blob.upload_blob(json_data, overwrite=True)"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Update Configuration CSV**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#delta_date_string = '2024-10-08'\r\n",
							"\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"csv_blob_name = \"ConfiguracionOrigenes.csv\"  # Reemplaza con el nombre de tu archivo\r\n",
							"\r\n",
							"# Leer el archivo CSV desde el Data Lake\r\n",
							"blob_client = blob_service_client.get_blob_client(container=metadata_folder, blob=csv_blob_name)\r\n",
							"\r\n",
							"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
							"df = pd.read_csv(stream, sep=',')  # Especifica el separador aquÃ­\r\n",
							"\r\n",
							"# Limpiar nombres de columnas para eliminar espacios en blanco\r\n",
							"df.columns = df.columns.str.strip()\r\n",
							"\r\n",
							"#landing_files = [item['name'] for item in filtered_dimensionsList]\r\n",
							"for i in landing_files:\r\n",
							"    df.loc[df['FileName'] == i[\"name\"][:i[\"name\"].index('_')], 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"\r\n",
							"# Save DataFrame\r\n",
							"output_stream = io.BytesIO()\r\n",
							"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')\r\n",
							"output_stream.seek(0)  # Regresar al inicio del flujo\r\n",
							"\r\n",
							"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MergeLandingFilesToSilver_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a10e3ba1-286d-4453-8f8b-dec0810b74e9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# path of the data lake container (bronze and silver for this example)\r\n",
							"data_lake_container = 'abfss://mdw@datalake1pgc.dfs.core.windows.net'\r\n",
							"# The ingestion folder where your parquet file are located\r\n",
							"bronze_folder = 'bronze/Landing/Pending'\r\n",
							"# The silver folder where your Delta Tables will be stored\r\n",
							"silver_folder = 'silver/DeltaTables'\r\n",
							"# The name of the table\r\n",
							"#table_name = 'color'\r\n",
							"# The wildcard filter used within the bronze folder to find files\r\n",
							"#source_wildcard = 'color*.parquet'\r\n",
							"# A comma separated string of one or more key columns (for the merge)\r\n",
							"#key_columns_str = 'idColor'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"-- USE default;\r\n",
							"-- DROP DATABASE  silver CASCADE ;\r\n",
							"-- USE default;\r\n",
							"-- CREATE DATABASE IF NOT EXISTS silver;\r\n",
							"-- USE silver;"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Carga de ficheros parquet Landing sobre tablas Delta capa Silver**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Import modules\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format, concat_ws, xxhash64\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
							"import re\r\n",
							"import datetime\r\n",
							"import json"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Merge **desde** Landing a Silver**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"LandingFiles = \r\n",
							"    [\r\n",
							"        {\r\n",
							"            \"name\": \"almacenes_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"articulos_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"categoria_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"codigoPostal_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"color_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"divisa_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"fechas_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"horas_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"linea_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"pais_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"talla_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"tarifa_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"tipoOperacion_20241008.parquet\",\r\n",
							"            \"type\": \"File\"\r\n",
							"        }\r\n",
							"    ]\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"array_as_string = json.dumps(LandingFiles)\r\n",
							"# print(\"Array como cadena de texto:\")\r\n",
							"# print(array_as_string)\r\n",
							"\r\n",
							"# Convertir la cadena de texto de vuelta a un array\r\n",
							"LandingFiles = json.loads(array_as_string)\r\n",
							"# print(\"\\nArray de vuelta al formato original:\")\r\n",
							"# print(array_back_to_original)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Fecha actual en formato yyyymmdd\r\n",
							"#fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
							"fecha_delta = '20241008'\r\n",
							"\r\n",
							"# Inicializar listas para almacenar resultados\r\n",
							"results = []\r\n",
							"correct_files = []\r\n",
							"incorrect_files = []\r\n",
							"\r\n",
							"\r\n",
							"import datetime;\r\n",
							"\r\n",
							"# ct stores current time\r\n",
							"ct = datetime.datetime.now()\r\n",
							"print(&quot;current time:-&quot;, ct)\r\n",
							"\r\n",
							"for file in LandingFiles:\r\n",
							"    file_result = {  # OPCIONAL: aÃ±adir timestamp\r\n",
							"        'file_name': file['name'],\r\n",
							"        'status': 'Incorrecto',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0\r\n",
							"    }\r\n",
							"\r\n",
							"    try:\r\n",
							"        table_name = file[\"name\"][:file[\"name\"].index('_')]  # Extrae la subcadena\r\n",
							"        source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
							"        key_columns_str = f\"id{table_name}\" \r\n",
							"\r\n",
							"        key_columns = key_columns_str\r\n",
							"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"        # Determinar ruta de archivos fuente\r\n",
							"        source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"        delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"\r\n",
							"        # Leer archivo(s) en DataFrame de Spark\r\n",
							"        sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"        sdf.cache()\r\n",
							"        sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")  # Eliminar columnas no deseadas\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            results.append(file_result)  # Agregar resultado sin procesar\r\n",
							"            incorrect_files.append(file['name'])  # Agregar a la lista de archivos incorrectos\r\n",
							"            continue  # Saltar a la siguiente iteraciÃ³n si no hay datos\r\n",
							"\r\n",
							"        # Eliminar columna no deseada\r\n",
							"        sdf = sdf.drop(\"fechaActualizacion\")\r\n",
							"\r\n",
							"        # Comprobar si la tabla Delta existe\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"            existing_columns = delta_table.toDF().columns\r\n",
							"            \r\n",
							"            # AÃ±adir columna fechaDelta\r\n",
							"            sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # Crear conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
							"            update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"            # Merge new data into existing table y captura el conteo\r\n",
							"            merge_result = delta_table.alias(\"existing\").merge(\r\n",
							"                source=sdf.alias(\"updates\"),\r\n",
							"                condition=\" AND \".join(conditions_list)\r\n",
							"            ).whenMatchedUpdate(\r\n",
							"                condition=\" AND \".join(conditions_list + [\"existing.hash != updates.hash\"]),\r\n",
							"                set={\r\n",
							"                    \"fechaCarga\": \"updates.fechaCarga\",\r\n",
							"                    \"fechaDelta\": \"updates.fechaDelta\",\r\n",
							"                    **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}\r\n",
							"                }\r\n",
							"            ).whenNotMatchedInsert(\r\n",
							"                values={\r\n",
							"                    \"fechaCarga\": \"updates.fechaCarga\",\r\n",
							"                    \"fechaDelta\": fecha_delta,\r\n",
							"                    **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}\r\n",
							"                }\r\n",
							"            ).execute()\r\n",
							"\r\n",
							"            # Almacenar conteos de filas\r\n",
							"            file_result['status'] = 'Correcto'\r\n",
							"            file_result['inserted_rows'] = merge_result.get('numInserted', 0)  # Filas insertadas\r\n",
							"            file_result['updated_rows'] = merge_result.get('numUpdated', 0)  # Filas actualizadas\r\n",
							"            correct_files.append(file['name'])  # Agregar a la lista de archivos correctos\r\n",
							"\r\n",
							"        else:\r\n",
							"            # Crear nueva tabla Delta\r\n",
							"            sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga\r\n",
							"            sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))  # Establecer fechaDelta\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"            sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS silver.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE silver.{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE silver.{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"        \r\n",
							"            # Almacenar archivo procesado correctamente\r\n",
							"            file_result['status'] = 'Correcto'\r\n",
							"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"            file_result['updated_rows'] = 0  # Ninguna fila actualizada en la creaciÃ³n\r\n",
							"            correct_files.append(file['name'])  # Agregar a la lista de archivos correctos\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        # Estado ya configurado como 'Incorrecto'\r\n",
							"        results.append(file_result)  # Agregar resultado de fallo al final\r\n",
							"        incorrect_files.append(file['name'])  # Agregar a la lista de archivos incorrectos\r\n",
							"        continue  # Continuar con la siguiente iteraciÃ³n\r\n",
							"\r\n",
							"    results.append(file_result)  # Agregar resultado al final del bucle\r\n",
							"\r\n",
							"# Convertir resultados a DataFrame\r\n",
							"results_df = spark.createDataFrame(results)\r\n",
							"\r\n",
							"# Ruta donde quieres guardar el archivo\r\n",
							"output_path = \"/ruta/en/tu/datalake/results.txt\"  # Cambia esta ruta segÃºn tu configuraciÃ³n\r\n",
							"\r\n",
							"# Escribir los resultados en un archivo TXT\r\n",
							"#results_df.coalesce(1).write.mode('overwrite').text(output_path)\r\n",
							"\r\n",
							"print(f\"Resultados guardados en: {output_path}\")\r\n",
							"\r\n",
							"# Imprimir resultados en consola\r\n",
							"for result in results:\r\n",
							"    print(f\"Archivo: {result['file_name']}, Estado: {result['status']}, Filas insertadas: {result['inserted_rows']}, Filas actualizadas: {result['updated_rows']}\")\r\n",
							"\r\n",
							"# Guardar nombres de archivos correctos e incorrectos en variables de la pipeline\r\n",
							"correct_files_variable = correct_files\r\n",
							"incorrect_files_variable = incorrect_files\r\n",
							"\r\n",
							"# Si necesitas ver las variables\r\n",
							"print(\"Archivos correctos:\", correct_files_variable)\r\n",
							"print(\"Archivos incorrectos:\", incorrect_files_variable)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Movemos ficheros de Pending a Processed o Error**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"# Crear el BlobServiceClient\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# Definir los paths de origen y destino\r\n",
							"source_container_name = \"etl/bronze/Landing\"\r\n",
							"#source_container_name = \"etl/bronze/Processed\"\r\n",
							"destination_container_name = \"etl/bronze/Processed\"\r\n",
							"#destination_container_name = \"etl/bronze/Landing\"\r\n",
							"source_blob_name = \"almacenes_20241008.parquet\"\r\n",
							"destination_blob_name = \"almacenes_20241008.parquet\"\r\n",
							"\r\n",
							"# Copiar el archivo al nuevo destino\r\n",
							"source_blob = blob_service_client.get_blob_client(container=source_container_name, blob=source_blob_name)\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=destination_container_name, blob=destination_blob_name)\r\n",
							"\r\n",
							"# Iniciar la copia del blob\r\n",
							"#copy_properties = destination_blob.start_copy_from_url(source_blob.url)\r\n",
							"\r\n",
							"# Verificar que la copia ha sido completada\r\n",
							"print(\"Estado de la copia:\", copy_properties[\"copy_status\"])\r\n",
							"\r\n",
							"#table_name = source_blob_name.index('_')\r\n",
							"name_part = source_blob_name.split('_')[0]\r\n",
							"\r\n",
							"results = ''\r\n",
							"error_messages = ''\r\n",
							"\r\n",
							"date_match = re.search(r'_(\\d{8})\\.parquet$', source_blob_name)\r\n",
							"if date_match:\r\n",
							"    date_str = date_match.group(1)  # '20241008'\r\n",
							"    year = date_str[:4]  # '2024'\r\n",
							"    month = date_str[4:6]  # '10'\r\n",
							"\r\n",
							"    # Crear la ruta de destino respetando la jerarquÃ­a aÃ±o/mes\r\n",
							"    destination_blob_name = f\"{name_part}/{year}/{month}/{source_blob_name}\"\r\n",
							"\r\n",
							"    # Copiar el archivo al nuevo destino\r\n",
							"    source_blob = blob_service_client.get_blob_client(container=source_container_name, blob=source_blob_name)\r\n",
							"    destination_blob = blob_service_client.get_blob_client(container=destination_container_name, blob=destination_blob_name)\r\n",
							"\r\n",
							"    # Iniciar la copia del blob\r\n",
							"    copy_properties = destination_blob.start_copy_from_url(source_blob.url)\r\n",
							"\r\n",
							"    # Verificar que la copia ha sido completada\r\n",
							"    print(f\"Estado de la copia para {source_blob_name}: {copy_properties['copy_status']}\")\r\n",
							"\r\n",
							"    # Eliminar el blob original si la copia fue exitosa\r\n",
							"    if copy_properties[\"copy_status\"] == \"success\":\r\n",
							"        #source_blob.delete_blob()  # Descomentar para eliminar el blob original\r\n",
							"        print(f\"Archivo {source_blob_name} movido exitosamente a {destination_blob_name}.\")\r\n",
							"        results = (f\"Archivo {source_blob_name} movido exitosamente a {destination_blob_name}.\")\r\n",
							"    else:\r\n",
							"        print(f\"Error al copiar el archivo {source_blob_name}.\")\r\n",
							"        error_messages = (f\"No se pudo extraer la fecha del nombre del archivo: {source_blob_name}\")\r\n",
							"else:\r\n",
							"    print(f\"No se pudo extraer la fecha del nombre del archivo: {source_blob_name}\")\r\n",
							"\r\n",
							"\r\n",
							"# Combina todos los resultados en un solo texto\r\n",
							"final_output = \" | \".join(results + error_messages) \r\n",
							"# Guardar el resultado en un archivo .txt en la carpeta Processed\r\n",
							"output_blob_name = \"resultado_movimiento.txt\"\r\n",
							"#utput_blob_name = f\"etl/bronze/Logs/{output_file_name}\"\r\n",
							"destination_container_name2 = \"etl/bronze/Logs/\"\r\n",
							"# Crear un BlobClient para el archivo de salida\r\n",
							"output_blob = blob_service_client.get_blob_client(container=destination_container_name2, blob=output_blob_name)\r\n",
							"\r\n",
							"# Subir el contenido al blob\r\n",
							"output_blob.upload_blob(final_output, overwrite=True)\r\n",
							"\r\n",
							"print(f\"Resultados guardados en {output_blob_name}.\")\r\n",
							"\r\n",
							"\r\n",
							"## Revisar lo ultimo del formato del text de log"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"from azure.storage.blob import BlobServiceClient\r\n",
							"import io\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format\r\n",
							"\r\n",
							"\r\n",
							"# Configurar la conexiÃ³n\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# Definir los nombres de los contenedores y blobs\r\n",
							"container_name = \"etl/bronze/Configuration\"\r\n",
							"csv_blob_name = \"ConfiguracionOrigenes.csv\"  # Reemplaza con el nombre de tu archivo\r\n",
							"\r\n",
							"# Leer el archivo CSV desde el Data Lake\r\n",
							"blob_client = blob_service_client.get_blob_client(container=container_name, blob=csv_blob_name)\r\n",
							"\r\n",
							"\r\n",
							"try:\r\n",
							"    # Descargar el blob y leerlo en un DataFrame de pandas\r\n",
							"    stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
							"    df = pd.read_csv(stream, sep=';')  # Especifica el separador aquÃ­\r\n",
							"    print(\"Archivo CSV leÃ­do exitosamente.\")\r\n",
							"except Exception as e:\r\n",
							"    print(f\"Error al leer el archivo CSV: {e}\")\r\n",
							"    exit()\r\n",
							"\r\n",
							"# Mostrar el contenido original\r\n",
							"print(\"Contenido original del CSV:\")\r\n",
							"print(df)\r\n",
							"\r\n",
							"# Limpiar nombres de columnas para eliminar espacios en blanco\r\n",
							"df.columns = df.columns.str.strip()\r\n",
							"\r\n",
							"# Verificar si 'TableName' estÃ¡ presente en el DataFrame\r\n",
							"if 'TableName' in df.columns:\r\n",
							"    # Editar el DataFrame basÃ¡ndose en la condiciÃ³n\r\n",
							"    df.loc[df['TableName'] == 'vw_Articulos', 'DataBaseName'] = 'm'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"else:\r\n",
							"    print(\"La columna 'TableName' no se encuentra en el DataFrame.\")\r\n",
							"\r\n",
							"# Mostrar el contenido editado\r\n",
							"print(\"Contenido editado del CSV:\")\r\n",
							"print(df)\r\n",
							"\r\n",
							"# Limpiar filas vacÃ­as si existen\r\n",
							"df.dropna(how='all', inplace=True)  # Eliminar filas completamente vacÃ­as\r\n",
							"\r\n",
							"# Guardar el DataFrame modificado en un nuevo flujo de bytes\r\n",
							"output_stream = io.BytesIO()\r\n",
							"df.to_csv(output_stream, index=False, sep=';', line_terminator='\\n')  # Usar line_terminator para evitar lÃ­neas en blanco\r\n",
							"output_stream.seek(0)  # Regresar al inicio del flujo\r\n",
							"\r\n",
							"# Sobrescribir el archivo CSV en el Data Lake\r\n",
							"try:\r\n",
							"    blob_client.upload_blob(output_stream.getvalue(), overwrite=True)\r\n",
							"    print(f\"Archivo CSV sobrescrito exitosamente en {container_name}/{csv_blob_name}.\")\r\n",
							"except Exception as e:\r\n",
							"    print(f\"Error al sobrescribir el archivo CSV: {e}\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ### Prueba::\r\n",
							"\r\n",
							"# # Supongamos que tienes las variables necesarias\r\n",
							"# # Fecha actual en formato yyyymmdd\r\n",
							"# fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
							"# #fecha_delta = '20241011'\r\n",
							"# # Convert comma separated string with keys to array\r\n",
							"# key_columns = key_columns_str.split(',')\r\n",
							"\r\n",
							"# # Convert array with keys to where-clause for merge statement\r\n",
							"# conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"# # Determine path of source files from ingest layer\r\n",
							"# source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"\r\n",
							"# # Determine path of Delta Lake Table \r\n",
							"# delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"\r\n",
							"# # Read file(s) in spark data frame\r\n",
							"# sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"# sdf.cache()\r\n",
							"\r\n",
							"# # Check countRows > 0 - Stop Notebook \r\n",
							"# # if sdf.count() == 0:\r\n",
							"# #     print(\"no procesar\")\r\n",
							"\r\n",
							"# if sdf.limit(1).count() == 0:\r\n",
							"#     print(\"no procesar\")\r\n",
							"\r\n",
							"# else:\r\n",
							"#     # Eliminar las columnas que no se desean\r\n",
							"#     sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")\r\n",
							"\r\n",
							"#     # Check if the Delta Table exists\r\n",
							"#     if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"#         # Read the existing Delta Table\r\n",
							"#         delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"\r\n",
							"#         #ALTER TABLE sales SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true');\r\n",
							"#         spark.sql(f\"ALTER TABLE default.{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"\r\n",
							"#         # Get the schema of the existing Delta table\r\n",
							"#         existing_columns = delta_table.toDF().columns\r\n",
							"        \r\n",
							"#         # AÃ±adir columna fechaDelta al DataFrame\r\n",
							"#         sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))\r\n",
							"\r\n",
							"#         # Crear el conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
							"#         update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"#         # Merge new data into existing table\r\n",
							"#         delta_table.alias(\"existing\").merge(\r\n",
							"#             source=sdf.alias(\"updates\"),\r\n",
							"#             condition=\" AND \".join(conditions_list)\r\n",
							"#         ).whenMatchedUpdate(\r\n",
							"#             condition=\" OR \".join([f\"existing.{col} != updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')]),\r\n",
							"#             set={\r\n",
							"#                 \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"#                 \"fechaDelta\": \"updates.fechaDelta\",  # Usar fechaDelta del DataFrame\r\n",
							"#                 **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"#             }\r\n",
							"#         ).whenNotMatchedInsert(\r\n",
							"#             values={\r\n",
							"#                 \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"#                 \"fechaDelta\": fecha_delta,  # Establecer fechaDelta al insertar\r\n",
							"#                 **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"#             }\r\n",
							"#         ).execute()\r\n",
							"#     else:\r\n",
							"#         # Crear nueva tabla Delta con nuevos datos, incluyendo la columna fechaDelta\r\n",
							"#         sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga del DataFrame\r\n",
							"#         sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))  # Establecer fechaDelta al crear la tabla\r\n",
							"#         sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"# spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Crear manualmente Delta Tables**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# # path of the data lake container (bronze and silver for this example)\r\n",
							"# data_lake_container = 'abfss://mdw@datalake1pgc.dfs.core.windows.net'\r\n",
							"# # The ingestion folder where your parquet file are located\r\n",
							"# bronze_folder = 'bronze/Landing/'\r\n",
							"# # The silver folder where your Delta Tables will be stored\r\n",
							"# silver_folder = 'silver/DeltaSTG'\r\n",
							"# # The name of the table\r\n",
							"#table_name = 'lineaventa'\r\n",
							"# # The wildcard filter used within the bronze folder to find files\r\n",
							"# #source_wildcard = 'color*.parquet'\r\n",
							"# # A comma separated string of one or more key columns (for the merge)\r\n",
							"# #key_columns_str = 'idColor'\r\n",
							"\r\n",
							"# #source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"\r\n",
							"# # Determine path of Delta Lake Table \r\n",
							"# delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"#spark.sql(f'DROP TABLE default.{table_name}')\r\n",
							"# spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ### Copia:\r\n",
							"\r\n",
							"# # Supongamos que tienes las variables necesarias\r\n",
							"# # Fecha actual en formato yyyymmdd\r\n",
							"# fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
							"# #fecha_delta = '20241011'\r\n",
							"# # Convert comma separated string with keys to array\r\n",
							"# key_columns = key_columns_str.split(',')\r\n",
							"\r\n",
							"# # Convert array with keys to where-clause for merge statement\r\n",
							"# conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"# # Determine path of source files from ingest layer\r\n",
							"# source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"\r\n",
							"# # Determine path of Delta Lake Table \r\n",
							"# delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"\r\n",
							"# # Read file(s) in spark data frame\r\n",
							"# sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"\r\n",
							"# # Check countRows > 0 - Stop Notebook \r\n",
							"# # CREAR DELTA TABLE DE COLOR\r\n",
							"\r\n",
							"# # Eliminar las columnas que no se desean\r\n",
							"# sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")\r\n",
							"\r\n",
							"# # Check if the Delta Table exists\r\n",
							"# if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"#     # Read the existing Delta Table\r\n",
							"#     delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"\r\n",
							"#     # Get the schema of the existing Delta table\r\n",
							"#     existing_columns = delta_table.toDF().columns\r\n",
							"    \r\n",
							"#     # AÃ±adir columna fechaDelta al DataFrame\r\n",
							"#     sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))\r\n",
							"\r\n",
							"#     # Crear el conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
							"#     update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"#     # Merge new data into existing table\r\n",
							"#     delta_table.alias(\"existing\").merge(\r\n",
							"#         source=sdf.alias(\"updates\"),\r\n",
							"#         condition=\" AND \".join(conditions_list)\r\n",
							"#     ).whenMatchedUpdate(\r\n",
							"#         condition=\" OR \".join([f\"existing.{col} != updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')]),\r\n",
							"#         set={\r\n",
							"#             \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"#             \"fechaDelta\": \"updates.fechaDelta\",  # Usar fechaDelta del DataFrame\r\n",
							"#             **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"#         }\r\n",
							"#     ).whenNotMatchedInsert(\r\n",
							"#         values={\r\n",
							"#             \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"#             \"fechaDelta\": fecha_delta,  # Establecer fechaDelta al insertar\r\n",
							"#             **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"#         }\r\n",
							"#     ).execute()\r\n",
							"# else:\r\n",
							"#     # Crear nueva tabla Delta con nuevos datos, incluyendo la columna fechaDelta\r\n",
							"#     sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga del DataFrame\r\n",
							"#     sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))  # Establecer fechaDelta al crear la tabla\r\n",
							"#     sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"# spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Actualizar CSV Configuracion**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# cuenta =  'datalake1pgc'\r\n",
							"# contendor = 'mdw'\r\n",
							"# archivo = '/bronze/Configuration'\r\n",
							"# #archivo = \r\n",
							"# #archivo2 = '/bronze/Configuration/ConfiguracionOrigenes\r\n",
							"# table_name = 'color'\r\n",
							"# #ruta = 'abfss://%s@%s.dfs.core.windows.net/%s' % (contendor, cuenta, archivo)\r\n",
							"# ruta = f'abfss://{contendor}@{cuenta}.dfs.core.windows.net/{archivo}'\r\n",
							"# #ruta2 = f'abfss://{contendor}@{cuenta}.dfs.core.windows.net/{archivo}'\r\n",
							"# # Leer CSV\r\n",
							"# #print(ruta)\r\n",
							"# df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(ruta)\r\n",
							"# #Generamos fecha en formato 'yyyyMMdd'\r\n",
							"# fecha_actual = date_format(current_date(), 'yyyy-MM-dd')\r\n",
							"# fecha_actual = '2022-01-18'\r\n",
							"# # Modificar CSV\r\n",
							"# df_modificado = df.withColumn(\r\n",
							"#     \"UpdateDate\",\r\n",
							"#     when(df[\"Filename\"] == table_name, fecha_actual).otherwise(df[\"UpdateDate\"])\r\n",
							"# )\r\n",
							"# Sobreescrir CSV\r\n",
							"#df_modificado.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ruta)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df_modificado.show()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df_modificado.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ruta)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/debug_LoadSCD2AndFact_7f3c121e-befe-4bac-bfff-52d6d92fb98a')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c851b305-0619-4b32-9b9c-ab58a45f8278"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters_overwritten"
							]
						},
						"source": [
							"# This cell is generated from runtime parameters. Learn more: https://go.microsoft.com/fwlink/?linkid=2161015\n",
							"data_lake_container = \"abfss://mdw@datalake1pgc.dfs.core.windows.net\"\n",
							"gold_folder_logs = \"gold/Logs\"\n",
							"container_name = \"mdw\"\n",
							"metadata_folder = \"mdw/bronze/Configuration\"\n",
							"tables_to_load = \"[{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Articles\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Currency\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Date\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Hours\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"OperationType\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"PostalCode\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Tariff\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Warehouse\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"}]\"\n",
							"golden_folder_dimensions = \"gold/Dimensions\"\n",
							"golden_folder_fact = \"gold/Fact\"\n",
							""
						],
						"outputs": [],
						"execution_count": 73
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Import modules**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format,concat_ws,xxhash64,current_timestamp\r\n",
							"from pyspark.sql import SparkSession,Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"import json\r\n",
							"import re\r\n",
							"from datetime import datetime\r\n",
							"from azure.storage.blob import BlobServiceClient\r\n",
							"import io\r\n",
							"import pandas as pd\r\n",
							"import csv"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# %%sql\r\n",
							"# USE default;\r\n",
							"# DROP DATABASE IF EXISTS   gold CASCADE ;\r\n",
							"# USE default;\r\n",
							"# CREATE DATABASE IF NOT EXISTS gold;\r\n",
							"# USE gold;"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"source": [
							"# %%sql\r\n",
							"# USE gold;\r\n",
							"# DROP TABLE IF EXISTS fact_Sales;"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"# factList = [ \r\n",
							"#         {\r\n",
							"#             \"name\": \"Sales\",\r\n",
							"#             \"query\": \"\"\"\r\n",
							"#                     SELECT  idDesgloseVenta AS idSales,\r\n",
							"#                         d.idArticulo AS idArticles,\r\n",
							"#                         c.idAlmacen AS idWarehouse,\r\n",
							"#                         c.idCliente AS idClient,\r\n",
							"#                         c.idCodigoPostal AS idPostalCode,\r\n",
							"#                         c.idDivisa AS idCurrency,\r\n",
							"#                         c.idTarifa AS idTariff,\r\n",
							"#                         c.idTipoOperacion AS idOperationType,\r\n",
							"#                         c.idHora AS idHours,\r\n",
							"#                         c.idFecha AS idDate,\r\n",
							"#                         f.fecha AS  date,\r\n",
							"#                         CAST(CONCAT(DATE_FORMAT(f.fecha, 'yyyy-MM-dd'), ' ', h.horaCompleta) AS TIMESTAMP) AS dateOp,\r\n",
							"#                         COALESCE(c.codigoTicket, 'D') AS ticketNumber,\r\n",
							"#                         COALESCE(d.Cantidad, 0) AS quantity,\r\n",
							"#                         COALESCE(d.PrecioUnitario, 0)  AS unitPrice,\r\n",
							"#                         COALESCE(d.CosteUnitario, 0)  AS unitCost,\r\n",
							"#                         COALESCE(d.importeBruto, 0)  AS amtTotalEuros,\r\n",
							"#                         COALESCE(d.importeNeto, 0)  AS amtNetEuros,\r\n",
							"#                         COALESCE(d.importeDescuento, 0)  AS amtTotalEurosDiscount,\r\n",
							"#                         COALESCE(d.importeNetoDescuento, 0) AS amtNetEurosDiscount,\r\n",
							"#                         CASE WHEN idTarifa IN (2,3) THEN 1\r\n",
							"#                             ELSE 0 \r\n",
							"#                         END AS discountSale,\r\n",
							"#                         CASE WHEN idTarifa = 0 THEN 0\r\n",
							"#                             ELSE (d.importeBruto - d.importeDescuento) \r\n",
							"#                         END AS amtTotalDiscount,\r\n",
							"#                         CASE WHEN idTarifa = 0 THEN 0 \r\n",
							"#                             ELSE (d.importeNeto - d.importeNetoDescuento) \r\n",
							"#                         END AS amtNetDiscount,\r\n",
							"#                         GREATEST(d.fechaCarga, c.fechaCarga,) AS loadDate,\r\n",
							"#                         GREATEST(d.fechaDelta, c.fechaDelta,) AS deltaDate\r\n",
							"#                     FROM silver.desgloseVenta AS d\r\n",
							"#                     INNER JOIN silver.cabeceraVenta AS c\r\n",
							"#                         ON d.idcabecerasVentas = c.idcabeceraVenta\r\n",
							"#                     LEFT JOIN silver.Fechas  AS f\r\n",
							"#                         On f.idFechas = c.idFecha\r\n",
							"#                     LEFT JOIN silver.horas  AS h\r\n",
							"#                         On h.idHoras = c.idHora\r\n",
							"#                     WHERE GREATEST(d.fechaDelta, c.fechaDelta) >=\r\n",
							"#                      \"\"\"\r\n",
							"#         }\r\n",
							"#     ]"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"dimensionsList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Articles\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idArticulos                                                                                             AS idArticles,\r\n",
							"                            COALESCE(nombre, 'D')                                                                                   AS name,\r\n",
							"                            COALESCE(descripcion, 'D')                                                                              AS description,\r\n",
							"                            COALESCE(codigoReferencia, 'D')                                                                         AS externalCode,\r\n",
							"                            COALESCE(t.talla, 'D')                                                                                  AS size,\r\n",
							"                            COALESCE( t.numeroTalla, -1)                                                                            AS numSize,\r\n",
							"                            COALESCE(co.color, 'D')                                                                                 AS colour,\r\n",
							"                            COALESCE(ca.categoria, 'D')                                                                             AS category,\r\n",
							"                            COALESCE(l.codigoLinea, 'D')                                                                            AS codLine,\r\n",
							"                            COALESCE(l.Linea, 'D')                                                                                  AS line, \r\n",
							"                            CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                                 WHEN a.idCategoria IN (2,4,6,7,9) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                                 ELSE CAST(CONCAT('PV',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                            END                                                                                                     AS season,              \r\n",
							"                            GREATEST(a.fechaCarga, t.fechaCarga, co.fechaCarga, ca.fechaCarga, l.fechaCarga)                        AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta)                        AS deltaDate\r\n",
							"                    FROM silver.articulos AS a\r\n",
							"                        LEFT JOIN silver.talla AS t\r\n",
							"                            ON t.idTalla = a.idTalla\r\n",
							"                        LEFT JOIN silver.color AS co\r\n",
							"                            ON co.idColor = a.idColor\r\n",
							"                        LEFT JOIN silver.categoria AS ca -- select * from silver.categoria\r\n",
							"                            ON ca.idCategoria = a.idCategoria\r\n",
							"                        LEFT JOIN silver.Linea AS l\r\n",
							"                            ON l.idLinea = a.idLinea\r\n",
							"                    WHERE GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        # {\r\n",
							"        #     \"name\": \"Client\",\r\n",
							"        #     \"query\": \"\"\"              \r\n",
							"        #             SELECT  idCliente                                           AS idClient,\r\n",
							"        #                     COALESCE(c.nombre, 'D')                             AS name,\r\n",
							"        #                     COALESCE(apellido1, 'D')                            AS lastName1,\r\n",
							"        #                     COALESCE(apellido2, 'D')                            AS lastName2, \r\n",
							"        #                     COALESCE(email, 'D')                                AS email,\r\n",
							"        #                     COALESCE(telefono, 'D')                             AS phoneNumber,\r\n",
							"        #                     COALESCE(CAST(cumpleanos AS STRING), '1900-01-01')  AS birthDay,       \r\n",
							"        #                     YEAR(current_date()) - YEAR(CAST(cumpleanos AS DATE)) \r\n",
							"        #                         - CASE \r\n",
							"        #                             WHEN MONTH(current_date()) < MONTH(CAST(cumpleanos AS DATE)) \r\n",
							"        #                                 OR (MONTH(current_date()) = MONTH(CAST(cumpleanos AS DATE)) AND DAY(current_date()) < DAY(CAST(cumpleanos AS DATE)))\r\n",
							"        #                             THEN 1\r\n",
							"        #                             ELSE 0\r\n",
							"        #                     END                                                 AS age,\r\n",
							"        #                     CASE WHEN hombre = 1 THEN 'Hombre' \r\n",
							"        #                          ELSE 'Mujer' \r\n",
							"        #                     END                                                 AS gender,\r\n",
							"        #                     COALESCE(p.nombre, 'D')                             AS country,\r\n",
							"        #                     COALESCE(p.codigoPais, 'D')                         AS countryCode,\r\n",
							"        #                     COALESCE(cp.region, 'D')                            AS region,\r\n",
							"        #                     COALESCE(c.Direcion, 'D')                           AS address,  \r\n",
							"        #                     COALESCE(codigoPostal, 'D')                         AS postalCode,\r\n",
							"        #                     COALESCE(activo, false)                             AS active,  -- Cambiado 0 a false aquÃ­\r\n",
							"        #                     GREATEST(c.fechaCarga, p.fechaCarga, cp.fechaCarga) AS loadDate,\r\n",
							"        #                     GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) AS deltaDate\r\n",
							"        #                 FROM silver.cliente AS c\r\n",
							"        #                     LEFT JOIN silver.pais AS p \r\n",
							"        #                         ON c.idPais = p.idPais\r\n",
							"        #                     INNER JOIN silver.codigoPostal AS cp \r\n",
							"        #                         ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\r\n",
							"        #                 WHERE GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) >= \r\n",
							"        #              \"\"\"\r\n",
							"        # }\r\n",
							"        #,\r\n",
							"        {\r\n",
							"            \"name\": \"Currency\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idDivisa              AS idCurrency,\r\n",
							"                            COALESCE(nombre, 'D') AS name,\r\n",
							"                            COALESCE(Divisa, 'D') AS currency,\r\n",
							"                            fechaCarga            AS loadDate,\r\n",
							"                            fechaDelta            AS deltaDate\r\n",
							"                    FROM silver.divisa\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Date\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idFechas                                  AS idDate,\r\n",
							"                            ClaveFecha                                AS dateKey,\r\n",
							"                            Fecha                                     AS date,\r\n",
							"                            COALESCE(Mes, 'D')                        AS month,\r\n",
							"                            COALESCE(NumeroMes, -1)                   AS monthNumber,\r\n",
							"                            COALESCE(Ano, -1)                         AS year,\r\n",
							"                            COALESCE(NumeroSemana, -1)                AS weekNumber,\r\n",
							"                            COALESCE(DiaSemana, 'D')                  AS dayWeek,\r\n",
							"                            COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\r\n",
							"                            COALESCE(DiaAno, -1)                      AS yearDay,\r\n",
							"                            CASE WHEN Trimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Trimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Trimestre = 3 THEN 'Q3'\r\n",
							"                                ELSE '-1' END                         AS quarter,\r\n",
							"                            CASE WHEN Cuatrimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Cuatrimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Cuatrimestre = 3 THEN 'Q3'\r\n",
							"                                WHEN Cuatrimestre = 4 THEN 'Q4'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS quadrimester,\r\n",
							"                            CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\r\n",
							"                                WHEN Cuatrimestre IN (2,4) THEN 'S2'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS semester,\r\n",
							"                            fechaCarga                                AS loadDate,\r\n",
							"                            fechaDelta                                AS deltaDate\r\n",
							"                    FROM silver.fechas\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Hours\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idHoras                     AS idHours,\r\n",
							"                            COALESCE(Hora, -1)          AS hour,\r\n",
							"                            COALESCE(Minuto, -1)        AS minute,\r\n",
							"                            COALESCE(HoraCompleta, 'D') AS fullHour,\r\n",
							"                            fechaCarga                  AS loadDate,\r\n",
							"                            fechaDelta                  AS deltaDate\r\n",
							"                    FROM silver.horas\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"OperationType\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTipoOperacion          AS idOperationType,\r\n",
							"                            COALESCE(Operacion, 'D') AS operation,\r\n",
							"                            fechaCarga               AS loadDate,\r\n",
							"                            fechaDelta               AS deltaDate\r\n",
							"                    FROM silver.tipoOperacion\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"PostalCode\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idCodigoPostal                       AS idPostalCode,\r\n",
							"                            COALESCE(codigoPostal, 'D')          AS postalCode,\r\n",
							"                            COALESCE(region, 'D')                AS region,\r\n",
							"                            COALESCE(c.codigoPais, 'D')          AS countryCode,\r\n",
							"                            COALESCE(nombre, 'D')                AS country,\r\n",
							"                            GREATEST(c.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(c.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.codigoPostal AS c\r\n",
							"                    LEFT JOIN silver.pais AS p\r\n",
							"                        ON p.codigoPais = c.codigoPais\r\n",
							"                    WHERE GREATEST(c.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Tariff\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTarifa              AS idTariff,\r\n",
							"                            COALESCE(Tarifa, 'D') AS tariff,\r\n",
							"                            fechaCarga            AS loadDate,\r\n",
							"                            fechaDelta            AS deltaDate       \r\n",
							"                    FROM silver.tarifa\r\n",
							"                    WHERE fechaDelta >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Warehouse\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idAlmacenes                          AS idWarehouse,\r\n",
							"                            COALESCE(a.Nombre, 'D')              AS warehouse,\r\n",
							"                            COALESCE(a.codigoAlmacen, 'D')       AS externalCode,\r\n",
							"                            COALESCE(p.codigoPais, 'D')          AS countryCode, \r\n",
							"                            COALESCE(p.nombre, 'D')              AS country,\r\n",
							"                            COALESCE(a.ciudad, 'D')              AS city,\r\n",
							"                            COALESCE(a.Direccion, 'D')           AS address,\r\n",
							"                            COALESCE(a.descripcion, 'D')         AS description,\r\n",
							"                            GREATEST(a.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.almacenes AS a\r\n",
							"                    LEFT JOIN silver.pais AS p\r\n",
							"                        ON p.idpais = a.idPais\r\n",
							"                    --WHERE a.fechaDelta <> 20241011\r\n",
							"                    --UNION \r\n",
							"                    --select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\r\n",
							"                    --UNION \r\n",
							"                    --select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\r\n",
							"                    WHERE GREATEST(a.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Configuration**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"factList = []"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"source": [
							"# parameter string to array\r\n",
							"tables_to_load = json.loads(tables_to_load)\r\n",
							"\r\n",
							"today_datetime = datetime.now()\r\n",
							"\r\n",
							"delta_date_filter = (today_datetime.strftime('%Y%m%d')) # 'yyyyMMdd'\r\n",
							"#delta_date_filter = '20241008'\r\n",
							"\r\n",
							"delta_date_string = today_datetime.strftime('%Y-%m-%d') # 'yyyy-mm-dd'\r\n",
							"#delta_date_string = '2024-10-08'\r\n",
							"\r\n",
							"# Convertir cada valor del CSV en un diccionario estructurado\r\n",
							"dict_tables_to_load = []\r\n",
							"\r\n",
							"for row in tables_to_load:\r\n",
							"    #csv_data = row[\"SchemaName,TableName,UpdateDate,Load\"]\r\n",
							"    \r\n",
							"    # Usar el lector de csv para separar los campos\r\n",
							"    #reader = csv.reader(io.StringIO(csv_data))\r\n",
							"    \r\n",
							"    # for schema, table, update_date, load in reader:\r\n",
							"    #     dict_tables_to_load.append({\r\n",
							"    #         \"SchemaName\": schema,\r\n",
							"    #         \"TableName\": table,\r\n",
							"    #         \"UpdateDate\": update_date,\r\n",
							"    #         \"Load\": load\r\n",
							"    #     })\r\n",
							"    dict_tables_to_load.append({\r\n",
							"    \"SchemaName\": row[\"SchemaName\"],\r\n",
							"    \"TableName\": row[\"TableName\"],\r\n",
							"    \"UpdateDate\": row[\"UpdateDate\"],\r\n",
							"    \"Load\": row[\"Load\"]\r\n",
							"})\r\n",
							"\r\n",
							"# Variables para almacenar los datos de 'dim' y 'fact'\r\n",
							"dim_tables = []\r\n",
							"fact_tables = []\r\n",
							"\r\n",
							"# Clasificar los datos\r\n",
							"for row in dict_tables_to_load:\r\n",
							"    if row['SchemaName'] == 'dim':\r\n",
							"        dim_tables.append(row)\r\n",
							"    elif row['SchemaName'] == 'fact':\r\n",
							"        fact_tables.append(row)\r\n",
							"\r\n",
							"\r\n",
							"load_y_tables = {table['TableName'] for table in dim_tables if table['Load'] == 'Y'}\r\n",
							"# Filtrar el dimensionsList para obtener solo las tablas con Load='Y' y devolver una lista de diccionarios\r\n",
							"filtered_dimensionsList = [\r\n",
							"    dim for dim in dimensionsList if dim[\"name\"] in load_y_tables\r\n",
							"]\r\n",
							"\r\n",
							"load_y_tables2 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
							"#print(load_y_tables2)\r\n",
							"# Filtrar el dimensionsList para obtener solo las tablas con Load='Y' y devolver una lista de diccionarios\r\n",
							"filtered_factList = [\r\n",
							"    dim for dim in factList if dim[\"name\"] in load_y_tables2\r\n",
							"]"
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Load SCD2 Dimensions**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Carga o Merge Dimensiones\r\n",
							"results = []\r\n",
							"exception = []\r\n",
							"\r\n",
							"for file in dimensionsList:\r\n",
							"    file_result = {\r\n",
							"        'table': f\"fact_{file['name']}\",\r\n",
							"        'status': 'Incorrect',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now()) \r\n",
							"    }\r\n",
							"\r\n",
							"    try:\r\n",
							"\r\n",
							"        table_name = file[\"name\"].split('_')[0]\r\n",
							"        source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
							"        key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
							"\r\n",
							"        key_columns = key_columns_str.split(',')\r\n",
							"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"        #delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Dimensions/{table_name}\"\r\n",
							"        delta_table_path_dimensions = f\"{data_lake_container}/{golden_folder_dimensions}/{table_name}\"\r\n",
							"        # Leer archivo(s) en DataFrame de Spark\r\n",
							"        # Meter filtro deltaDate\r\n",
							"\r\n",
							"        df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
							"        sdf = df.cache()\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"There is no new data\"\r\n",
							"            results.append(file_result)\r\n",
							"            continue\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"        \r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path_dimensions):\r\n",
							"            # AÃ±adir las columnas necesarias\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            todas_las_columnas = sdf.columns\r\n",
							"            columnas_al_principio = [f\"id{table_name}\"]\r\n",
							"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
							"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
							"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
							"            sdf = sdf.select(*nuevo_orden_columnas)\r\n",
							"\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_view\")\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                MERGE INTO gold.dim_{table_name}  \r\n",
							"                AS existing\r\n",
							"                USING temp_view AS updates\r\n",
							"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
							"                WHEN MATCHED AND existing.isCurrent = 1 AND existing.hash != updates.hash THEN\r\n",
							"                UPDATE SET\r\n",
							"                    existing.toDate = current_date(),\r\n",
							"                    existing.isCurrent = 0\r\n",
							"            \"\"\")\r\n",
							"\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                INSERT INTO gold.dim_{table_name}    \r\n",
							"                SELECT \r\n",
							"                    {next_surrogate_key} + row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS sk{table_name}, \r\n",
							"                    updates.*\r\n",
							"                FROM temp_view AS updates\r\n",
							"                LEFT JOIN gold.dim_{table_name}  \r\n",
							"                AS existing\r\n",
							"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
							"                WHERE existing.{key_columns_str} IS NULL OR existing.isCurrent = 0 OR existing.hash != updates.hash\r\n",
							"            \"\"\")\r\n",
							"\r\n",
							"            # Almacenar conteos de filas\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.dim_{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            file_result['inserted_rows'] = last_result[0].operationMetrics[\"numTargetRowsInserted\"]\r\n",
							"            file_result['updated_rows'] = last_result[0].operationMetrics[\"numTargetRowsUpdated\"]\r\n",
							"\r\n",
							"\r\n",
							"        else: # Crear nueva tabla Delta\r\n",
							"      \r\n",
							"            last_surrogate_key = 0  # Iniciar con 0 si la tabla no existe\r\n",
							"            #sdf = sdf.withColumn(f\"sk{table_name}\", F.monotonically_increasing_id() + last_surrogate_key + 1)  # Asigna un valor Ãºnico a cada fila.\r\n",
							"            sdf = sdf.withColumn(f\"sk{table_name}\", F.col(f\"id{table_name}\"))\r\n",
							"            # AÃ±adir las columnas necesarias\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            # Hash\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\" ,'fechaCarga', 'fechaDelta','fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # Reorganizar la estructura de la tabla\r\n",
							"            todas_las_columnas = sdf.columns\r\n",
							"            columnas_al_principio = [f\"sk{table_name}\", f\"id{table_name}\"]\r\n",
							"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
							"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
							"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
							"            sdf_reordenado = sdf.select(*nuevo_orden_columnas)\r\n",
							"            \r\n",
							"            # Crear la tabla Delta\r\n",
							"            sdf_reordenado.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path_dimensions)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.dim_{table_name} USING DELTA LOCATION \\'{delta_table_path_dimensions}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            #spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"            #spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')\")\r\n",
							"\r\n",
							"            # Almacenar archivo procesado correctamente\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"            file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)"
						],
						"outputs": [],
						"execution_count": 69
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Load Fact Table**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Carga FULL tabla/s de hechos\r\n",
							"\r\n",
							"# Inicializar listas para almacenar resultados\r\n",
							"# results = []\r\n",
							"# exception = []\r\n",
							"\r\n",
							"\r\n",
							"for file in factList:\r\n",
							"    file_result = {\r\n",
							"        'table': f\"fact_{file['name']}\",\r\n",
							"        'status': 'Incorrect',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now())  \r\n",
							"    }\r\n",
							"    table_name = file[\"name\"].split('_')[0]\r\n",
							"\r\n",
							"    # Definir la ruta de la tabla Delta\r\n",
							"    delta_table_path_fact = f\"{data_lake_container}/{golden_folder_fact}/{table_name}\"\r\n",
							"    #delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Fact/{table_name}\"\r\n",
							"    #file[\"query\"] = file[\"query\"].strip() + f\" '{delta_date_filter}'\"\r\n",
							"    #df = spark.sql(file[\"query\"]) \r\n",
							"    df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
							"    sdf = df.cache()\r\n",
							"\r\n",
							"    try:\r\n",
							"        # Leer el archivo CSV en un DataFrame de Spark\r\n",
							"        #sdf = spark.read.csv(f\"gs://your-bucket/{file['name']}\", header=True, inferSchema=True)\r\n",
							"\r\n",
							"        # Verificar si el DataFrame estÃ¡ vacÃ­o\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"There is no new data\"\r\n",
							"            results.append(file_result)\r\n",
							"            continue\r\n",
							"\r\n",
							"        # AÃ±adir columnas necesarias\r\n",
							"        # sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss')) \\\r\n",
							"        #          .withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss')) \\\r\n",
							"        #          .withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"        # Truncar la tabla Delta si existe\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path_fact):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path_fact)\r\n",
							"            delta_table.delete()  # Truncar la tabla Delta\r\n",
							"\r\n",
							"            #last_row number para incremental\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                INSERT INTO gold.fact_{table_name}  \r\n",
							"                SELECT ROW_NUMBER() OVER(ORDER BY s.dateOP) AS skSales, s.*, \r\n",
							"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
							"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
							"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
							"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
							"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
							"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
							"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
							"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
							"                        COALESCE(d.skDate, -1) AS skDate\r\n",
							"                FROM temp_sales_view as s\r\n",
							"                LEFT JOIN gold.dim_Articles AS a\r\n",
							"                    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
							"                LEFT JOIN gold.dim_Client AS cl\r\n",
							"                    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
							"                LEFT JOIN gold.dim_Currency AS cu\r\n",
							"                    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
							"                LEFT JOIN gold.dim_Date AS d\r\n",
							"                    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
							"                LEFT JOIN gold.dim_Hours AS h\r\n",
							"                    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
							"                LEFT JOIN gold.dim_OperationType AS o\r\n",
							"                    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
							"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
							"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
							"                LEFT JOIN gold.dim_Tariff AS t\r\n",
							"                    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
							"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
							"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
							"                \"\"\")\r\n",
							"        else:\r\n",
							"\r\n",
							"            #window_spec = Window.orderBy(\"dateOp\", \"idArticles\")\r\n",
							"            #sdf = sdf.withColumn(f\"sk{table_name}\", F.row_number().over(window_spec))\r\n",
							"            #todas_las_columnas = sdf.columns\r\n",
							"            #columnas_al_principio = [f\"sk{table_name}\"]  # Suponiendo que la tabla tiene una columna idWarehouse\r\n",
							"            #otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio]\r\n",
							"            #nuevo_orden_columnas = columnas_al_principio + otras_columnas\r\n",
							"            #sdf = sdf.select(*nuevo_orden_columnas)\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
							"            sdf = spark.sql(f\"\"\" \r\n",
							"                SELECT ROW_NUMBER() OVER(ORDER BY s.dateOP) AS skSales, s.*, \r\n",
							"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
							"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
							"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
							"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
							"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
							"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
							"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
							"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
							"                        COALESCE(d.skDate, -1) AS skDate\r\n",
							"                FROM temp_sales_view AS s\r\n",
							"                LEFT JOIN gold.dim_Articles AS a\r\n",
							"                    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
							"                LEFT JOIN gold.dim_Client AS cl\r\n",
							"                    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
							"                LEFT JOIN gold.dim_Currency AS cu\r\n",
							"                    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
							"                LEFT JOIN gold.dim_Date AS d\r\n",
							"                    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
							"                LEFT JOIN gold.dim_Hours AS h\r\n",
							"                    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
							"                LEFT JOIN gold.dim_OperationType AS o\r\n",
							"                    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
							"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
							"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
							"                LEFT JOIN gold.dim_Tariff AS t\r\n",
							"                    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
							"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
							"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
							"                \"\"\")\r\n",
							"\r\n",
							"        # Cargar datos en la tabla Delta\r\n",
							"            sdf.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_fact)\r\n",
							"            #sdf.write.format(\"delta\").mode(\"append\").save(delta_table_path)  # valorar carga incremental mÃ¡s adelante\r\n",
							"\r\n",
							"            # Crear la tabla Delta en caso de que no exista\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.fact_{table_name} USING DELTA LOCATION \\'{delta_table_path_fact}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"            \r\n",
							"        # Almacenar conteos de filas\r\n",
							"        file_result['status'] = 'Correct'\r\n",
							"        file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Guardar ficheros Logs**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Obtener la fecha actual\r\n",
							"fecha_actual = datetime.now()\r\n",
							"year = fecha_actual.strftime('%Y')  # AÃ±o actual (por ejemplo, '2024')\r\n",
							"month = fecha_actual.strftime('%m')  # Mes actual (por ejemplo, '10')\r\n",
							"day = fecha_actual.strftime('%d')  # DÃ­a actual (por ejemplo, '24')\r\n",
							"hora = fecha_actual.strftime('%H%M%S')  # Hora actual en formato HHMMSS (por ejemplo, '235959')\r\n",
							"\r\n",
							"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
							"archivo_nombre = f\"Log_{day}_{hora}.json\"\r\n",
							"\r\n",
							"# Reemplaza con tu cadena de conexiÃ³n a Azure\r\n",
							"\r\n",
							"# Cliente de servicio de blobs\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# Convetimos el diccionario a formato json\r\n",
							"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
							"\r\n",
							"# Crear la ruta de destino con la jerarquÃ­a AÃ±o/Mes y el nombre del archivo\r\n",
							"destination_blob_name = f\"{gold_folder_logs}/{year}/{month}/{archivo_nombre}\"\r\n",
							"\r\n",
							"# Crear un cliente del blob de destino\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=container_name, blob=destination_blob_name)\r\n",
							"\r\n",
							"destination_blob.upload_blob(json_data, overwrite=True)"
						],
						"outputs": [],
						"execution_count": 74
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Actualizar fichero metadata**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Configurar la conexiÃ³n\r\n",
							"\r\n",
							"#delta_date_string = '1900-01-01'\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# Definir los nombres de los contenedores y blobs\r\n",
							"#container_name = \"etl/bronze/Configuration\"\r\n",
							"#metadata_folder = \"mdw/bronze/Configuration\"\r\n",
							"#container_name = metadata_folder\r\n",
							"csv_blob_name = \"ConfiguracionReporting.csv\"  # Reemplaza con el nombre de tu archivo\r\n",
							"\r\n",
							"# Leer el archivo CSV desde el Data Lake\r\n",
							"blob_client = blob_service_client.get_blob_client(container=metadata_folder, blob=csv_blob_name)\r\n",
							"\r\n",
							"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
							"df = pd.read_csv(stream, sep=',')  # Especifica el separador aquÃ­\r\n",
							"\r\n",
							"# Limpiar nombres de columnas para eliminar espacios en blanco\r\n",
							"df.columns = df.columns.str.strip()\r\n",
							"\r\n",
							"names_dimensionsList = [item['name'] for item in filtered_dimensionsList]\r\n",
							"for i in names_dimensionsList:\r\n",
							"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"\r\n",
							"names_factList = [item['name'] for item in filtered_factList]\r\n",
							"for i in names_factList:\r\n",
							"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"\r\n",
							"\r\n",
							"# Guardar el DataFrame modificado en un nuevo flujo de bytes\r\n",
							"output_stream = io.BytesIO()\r\n",
							"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')  # Usar line_terminator para evitar lÃ­neas en blanco\r\n",
							"output_stream.seek(0)  # Regresar al inicio del flujo\r\n",
							"\r\n",
							"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
						],
						"outputs": [],
						"execution_count": 75
					},
					{
						"cell_type": "code",
						"source": [
							"        # # Comprobar si la tabla Delta existe\r\n",
							"        # if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"        #     delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"        #     existing_columns = delta_table.toDF().columns\r\n",
							"            \r\n",
							"        #     # Obtener la Ãºltima surrogate key para crear nuevas\r\n",
							"        #     last_surrogate_key = delta_table.toDF().agg(F.max(f\"sk{table_name}\")).collect()[0][0] or 0\r\n",
							"        #     next_surrogate_key = last_surrogate_key + 1\r\n",
							"            \r\n",
							"        #     # Merge new data into existing table con lÃ³gica SCD Tipo 2\r\n",
							"        #     delta_table.alias(\"existing\").merge(\r\n",
							"        #         source=sdf.alias(\"updates\"),\r\n",
							"        #         condition=\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])\r\n",
							"        #     ).whenMatchedUpdate(\r\n",
							"        #         condition=\"existing.isCurrent = 1 AND existing.hash != updates.hash\",\r\n",
							"        #         set={\r\n",
							"        #             \"toDate\": F.current_date(),\r\n",
							"        #             \"isCurrent\": F.lit(0)\r\n",
							"        #         }\r\n",
							"        #     ).whenNotMatchedInsert(\r\n",
							"        #         values={\r\n",
							"        #             f\"sk{table_name}\": F.expr(f\"{next_surrogate_key} + row_number() over (order by (select null)) - 1\"),  # Aumenta la surrogate key en base a la fila\r\n",
							"        #             **{f\"updates.{col}\": f\"updates.{col}\" for col in existing_columns}\r\n",
							"        #         }\r\n",
							"        #     ).execute()\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/debug_MergeLandingFilesToSilver_aa1d925b-91fb-4ece-b6fc-6dc66a2ad29f_emj')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5d0a9eac-6b39-4054-891f-464e33704032"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters_overwritten"
							]
						},
						"source": [
							"# This cell is generated from runtime parameters. Learn more: https://go.microsoft.com/fwlink/?linkid=2161015\n",
							"data_lake_container = \"abfss://mdw@datalake1pgc.dfs.core.windows.net\"\n",
							"bronze_landing = \"bronze/Landing/Pending\"\n",
							"silver_folder = \"silver/DeltaTables\"\n",
							"landing_files = \"[{\\\"name\\\":\\\"almacenes_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"articulos_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"categoria_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"codigoPostal_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"color_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"divisa_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"fechas_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"horas_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"linea_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"pais_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"talla_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"tarifa_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"},{\\\"name\\\":\\\"tipoOperacion_20241008.parquet\\\",\\\"type\\\":\\\"File\\\"}]\"\n",
							"bronze_error = \"mdw/bronze/Landing/Error\"\n",
							"bronze_processed = \"mdw/bronze/Processed\"\n",
							"bronze_logs = \"mdw/bronze/Logs\"\n",
							"silver_logs = \"mdw/silver/Logs\"\n",
							"container_name = \"mdw\"\n",
							"bronze_pending = \"mdw/bronze/Landing/Pending\"\n",
							"metadata_folder = 'mdw/bronze/Configuration'"
						],
						"outputs": [],
						"execution_count": 125
					},
					{
						"cell_type": "code",
						"source": [
							"# path of the data lake container (bronze and silver for this example)\r\n",
							"#data_lake_container = 'abfss://mdw@datalake1pgc.dfs.core.windows.net'\r\n",
							"# The ingestion folder where your parquet file are located\r\n",
							"#bronze_landing = 'bronze/Landing/Pending'\r\n",
							"# The silver folder where your Delta Tables will be stored\r\n",
							"#silver_folder = 'silver/DeltaTables'\r\n",
							"# The name of the table\r\n",
							"#table_name = 'color'\r\n",
							"# The wildcard filter used within the bronze folder to find files\r\n",
							"#source_wildcard = 'color*.parquet'\r\n",
							"# A comma separated string of one or more key columns (for the merge)\r\n",
							"#key_columns_str = 'idColor'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# %%sql\r\n",
							"#  USE default;\r\n",
							"#  DROP DATABASE  silver CASCADE ;\r\n",
							"#  USE default;\r\n",
							"#  CREATE DATABASE IF NOT EXISTS silver;\r\n",
							"#  USE silver;"
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#%%sql\r\n",
							"#USE silver;\r\n",
							"#DROP TABLE  silver.fechas"
						],
						"outputs": [],
						"execution_count": 103
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Import modules**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Import modules\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format, concat_ws, xxhash64\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
							"import re\r\n",
							"import json\r\n",
							"from datetime import datetime\r\n",
							"import pandas as pd\r\n",
							"import io\r\n",
							"from pyspark.sql import functions as F"
						],
						"outputs": [],
						"execution_count": 126
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Configuration**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"# Fecha actual en formato yyyymmdd\r\n",
							"\r\n",
							"fecha_hoy = datetime.now()\r\n",
							"# Formato cadena 'yyyy-mm-dd'\r\n",
							"delta_date_filter = (fecha_hoy.strftime('%Y%m%d'))\r\n",
							"#delta_date_filter = '20241008'\r\n",
							"\r\n",
							"# Formato cadena 'yyyy-mm-dd'\r\n",
							"delta_date_string = fecha_hoy.strftime('%Y-%m-%d')\r\n",
							"\r\n",
							"landing_files = json.loads(landing_files)  # Convertimos en array la variable\r\n",
							"\r\n",
							"# Inicializar listas para almacenar resultados\r\n",
							"results = []\r\n",
							"correct_files = []\r\n",
							"incorrect_files = []"
						],
						"outputs": [],
						"execution_count": 127
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Load/Merge parquet Landing files to silver**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"for file in landing_files:\r\n",
							"    file_result = {  # OPCIONAL: aÃ±adir timestamp\r\n",
							"        'table': file[\"name\"][:file[\"name\"].index('_')],  \r\n",
							"        'file_name': file['name'],\r\n",
							"        'status': 'Incorrecto',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now())  \r\n",
							"    }\r\n",
							"\r\n",
							"    try:\r\n",
							"        table_name = file[\"name\"][:file[\"name\"].index('_')]  # Extrae la subcadena\r\n",
							"        source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
							"        key_columns_str = f\"id{table_name}\" \r\n",
							"\r\n",
							"        source_path = data_lake_container + '/' + bronze_landing + '/' + source_wildcard\r\n",
							"        delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"        sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"        sdf.cache()\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"File with no data\"\r\n",
							"            results.append(file_result)\r\n",
							"            correct_files.append(file['name'])\r\n",
							"            continue\r\n",
							"\r\n",
							"        key_columns = key_columns_str.split(',')\r\n",
							"        #key_columns = key_columns_str\r\n",
							"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"\r\n",
							"        # Leer archivo(s) en DataFrame de Spark\r\n",
							"\r\n",
							"        sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")  # Eliminar columnas no deseadas\r\n",
							"\r\n",
							"        # Eliminar columna no deseada\r\n",
							"        #sdf = sdf.drop(\"fechaActualizacion\")\r\n",
							"\r\n",
							"        # Comprobar si la tabla Delta existe\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"            existing_columns = delta_table.toDF().columns\r\n",
							"            \r\n",
							"            # AÃ±adir columna fechaDelta\r\n",
							"            sdf = sdf.withColumn(\"fechaDelta\", lit(delta_date_filter))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # Crear conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
							"            update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"            # Merge new data into existing table y captura el conteo\r\n",
							"            merge_result = delta_table.alias(\"existing\").merge(\r\n",
							"                source=sdf.alias(\"updates\"),\r\n",
							"                condition=\" AND \".join(conditions_list)\r\n",
							"            ).whenMatchedUpdate(\r\n",
							"                condition=\" AND \".join(conditions_list + [\"existing.hash != updates.hash\"]),\r\n",
							"                set={\r\n",
							"                    #\"fechaCarga\": \"existing.fechaCarga\",  #updates\r\n",
							"                    \"fechaDelta\": delta_date_filter,\r\n",
							"                    **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}\r\n",
							"                }\r\n",
							"            ).whenNotMatchedInsert(\r\n",
							"                values={\r\n",
							"                    \"fechaCarga\": \"updates.fechaCarga\",\r\n",
							"                    \"fechaDelta\": delta_date_filter,\r\n",
							"                    **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}\r\n",
							"                }\r\n",
							"            ).execute()\r\n",
							"\r\n",
							"            # Almacenar conteos de filas\r\n",
							"            # file_result['status'] = 'Correcto'\r\n",
							"            # file_result['inserted_rows'] = merge_result.get('numInserted', 0)  # Filas insertadas\r\n",
							"            # file_result['updated_rows'] = merge_result.get('numUpdated', 0)  # Filas actualizadas\r\n",
							"            #correct_files.append(file['name'])  # Agregar a la lista de archivos correctos\r\n",
							"\r\n",
							"            # Almacenar conteos de filas\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY silver.{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            file_result['inserted_rows'] = last_result[0].operationMetrics[\"numTargetRowsInserted\"]\r\n",
							"            file_result['updated_rows'] = last_result[0].operationMetrics[\"numTargetRowsUpdated\"]\r\n",
							"            correct_files.append(file['name'])  # Agregar a la lista de archivos correctos\r\n",
							"\r\n",
							"        else:\r\n",
							"            # Crear nueva tabla Delta\r\n",
							"            sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga\r\n",
							"            sdf = sdf.withColumn(\"fechaDelta\", lit(delta_date_filter))  # Establecer fechaDelta\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"            sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS silver.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE silver.{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE silver.{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"        \r\n",
							"            # Almacenar archivo procesado correctamente\r\n",
							"            file_result['status'] = 'Correcto'\r\n",
							"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"            file_result['updated_rows'] = 0  # Ninguna fila actualizada en la creaciÃ³n\r\n",
							"            correct_files.append(file['name'])  # Agregar a la lista de archivos correctos\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        # Estado ya configurado como 'Incorrecto'\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)  # Agregar resultado de fallo al final\r\n",
							"        incorrect_files.append(file['name'])  # Agregar a la lista de archivos incorrectos\r\n",
							"        continue  # Continuar con la siguiente iteraciÃ³n\r\n",
							"\r\n",
							"    results.append(file_result)  # Agregar resultado al final del bucle\r\n",
							"\r\n",
							"\r\n",
							"# Imprimir resultados en consola\r\n",
							"for result in results:\r\n",
							"    print(f\"Archivo: {result['file_name']}, Estado: {result['status']}, Filas insertadas: {result['inserted_rows']}, Filas actualizadas: {result['updated_rows']}\")\r\n",
							"\r\n",
							"# Guardar nombres de archivos correctos e incorrectos en variables de la pipeline\r\n",
							"correct_files_variable = correct_files\r\n",
							"incorrect_files_variable = incorrect_files\r\n",
							"\r\n",
							"# Si necesitas ver las variables\r\n",
							"print(\"Archivos correctos:\", correct_files_variable)\r\n",
							"print(\"Archivos incorrectos:\", incorrect_files_variable)"
						],
						"outputs": [],
						"execution_count": 128
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Save log merge/load**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Obtener la fecha actual\r\n",
							"fecha_actual = datetime.now()\r\n",
							"year = fecha_actual.strftime('%Y')  # AÃ±o actual (por ejemplo, '2024')\r\n",
							"month = fecha_actual.strftime('%m')  # Mes actual (por ejemplo, '10')\r\n",
							"day = fecha_actual.strftime('%d')  # DÃ­a actual (por ejemplo, '24')\r\n",
							"hora = fecha_actual.strftime('%H%M%S')  # Hora actual en formato HHMMSS (por ejemplo, '235959')\r\n",
							"\r\n",
							"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
							"archivo_nombre = f\"Log_{day}_{hora}.json\"\r\n",
							"\r\n",
							"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
							"\r\n",
							"# Crear la ruta de destino con la jerarquÃ­a AÃ±o/Mes y el nombre del archivo\r\n",
							"destination_blob_name = f\"{silver_logs}/{year}/{month}/{archivo_nombre}\"\r\n",
							"\r\n",
							"# Crear un cliente del blob de destino\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=container_name, blob=destination_blob_name)\r\n",
							"\r\n",
							"destination_blob.upload_blob(json_data, overwrite=True)"
						],
						"outputs": [],
						"execution_count": 129
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Move files from Pending to Processed or Error**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"results = ''\r\n",
							"error_messages = ''\r\n",
							"\r\n",
							"# Bucle para archivos correctos\r\n",
							"for i in correct_files_variable:\r\n",
							"    source_blob_name = i\r\n",
							"    name_part = i.split('_')[0]\r\n",
							"    \r\n",
							"    # Extraer fecha\r\n",
							"    date_match = re.search(r'_(\\d{8})\\.parquet$', i)\r\n",
							"    if date_match:\r\n",
							"        date_str = date_match.group(1)  # '20241008'\r\n",
							"        year = date_str[:4]  # '2024'\r\n",
							"        month = date_str[4:6]  # '10'\r\n",
							"        destination_blob_name = f\"{name_part}/{year}/{month}/{i}\"\r\n",
							"    else:\r\n",
							"        destination_blob_name = i  # Si no tiene fecha, dejar el nombre como estÃ¡\r\n",
							"    \r\n",
							"    # Crear los clientes de blob\r\n",
							"    source_blob = blob_service_client.get_blob_client(container=bronze_pending, blob=source_blob_name)\r\n",
							"    destination_blob_processed = blob_service_client.get_blob_client(container=bronze_processed, blob=destination_blob_name)\r\n",
							"    \r\n",
							"    # Iniciar la copia\r\n",
							"    copy_properties = destination_blob_processed.start_copy_from_url(source_blob.url)\r\n",
							"    \r\n",
							"    # Verificar el estado de la copia\r\n",
							"    if copy_properties[\"copy_status\"] == \"success\":\r\n",
							"        #source_blob.delete_blob()  # Descomentar para eliminar el blob original\r\n",
							"        results += f\"Archivo {source_blob_name} procesado exitosamente y movido a bronze/Processed. {str(datetime.now())}\\n\"\r\n",
							"    else:\r\n",
							"        error_messages += f\"Error al copiar el archivo {source_blob_name}.\\n\"\r\n",
							"\r\n",
							"# Bucle para archivos incorrectos\r\n",
							"for i in incorrect_files_variable:\r\n",
							"    source_blob_name = i\r\n",
							"    source_blob = blob_service_client.get_blob_client(container=bronze_pending, blob=source_blob_name)\r\n",
							"    destination_blob_error = blob_service_client.get_blob_client(container=bronze_error, blob=source_blob_name)\r\n",
							"\r\n",
							"    # Iniciar la copia\r\n",
							"    copy_properties = destination_blob_error.start_copy_from_url(source_blob.url)\r\n",
							"\r\n",
							"    # Verificar el estado de la copia\r\n",
							"    if copy_properties[\"copy_status\"] == \"success\":\r\n",
							"        #source_blob.delete_blob()  # Descomentar para eliminar el blob original\r\n",
							"        results += f\"Archivo {source_blob_name} procesado errÃ³neamente y movido a bronze/Landing/Error. {str(datetime.now())} \\n\"\r\n",
							"    else:\r\n",
							"        error_messages += f\"Error al procesar y copiar el archivo {source_blob_name}.\\n\"\r\n",
							"\r\n",
							"# Combina todos los resultados en un solo texto\r\n",
							"final_output = results + error_messages "
						],
						"outputs": [],
						"execution_count": 130
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Save logs files**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"fecha_actual = datetime.now()\r\n",
							"year = fecha_actual.strftime('%Y')  # AÃ±o actual (por ejemplo, '2024')\r\n",
							"month = fecha_actual.strftime('%m')  # Mes actual (por ejemplo, '10')\r\n",
							"day = fecha_actual.strftime('%d')  # DÃ­a actual (por ejemplo, '24')\r\n",
							"hora = fecha_actual.strftime('%H%M%S')  # Hora actual en formato HHMMSS (por ejemplo, '235959')\r\n",
							"\r\n",
							"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
							"output_blob_name = f\"Log_{day}_{hora}.json\"\r\n",
							"output_blob_name = f\"{year}/{month}/{output_blob_name}\"\r\n",
							"#output_blob_name = \"logs.txt\"\r\n",
							"output_blob = blob_service_client.get_blob_client(container=bronze_logs, blob=output_blob_name)\r\n",
							"\r\n",
							"# Subir el contenido al blob\r\n",
							"output_blob.upload_blob(final_output, overwrite=True)"
						],
						"outputs": [],
						"execution_count": 131
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Update Configuration CSV**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Configurar la conexiÃ³n\r\n",
							"#delta_date_string = '2024-10-08'\r\n",
							"\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# Definir los nombres de los contenedores y blobs\r\n",
							"#container_name = \"etl/bronze/Configuration\"\r\n",
							"#metadata_folder = \"mdw/bronze/Configuration\"\r\n",
							"container_name = metadata_folder\r\n",
							"csv_blob_name = \"ConfiguracionOrigenes.csv\"  # Reemplaza con el nombre de tu archivo\r\n",
							"\r\n",
							"# Leer el archivo CSV desde el Data Lake\r\n",
							"blob_client = blob_service_client.get_blob_client(container=container_name, blob=csv_blob_name)\r\n",
							"\r\n",
							"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
							"df = pd.read_csv(stream, sep=',')  # Especifica el separador aquÃ­\r\n",
							"\r\n",
							"# Limpiar nombres de columnas para eliminar espacios en blanco\r\n",
							"df.columns = df.columns.str.strip()\r\n",
							"\r\n",
							"#landing_files = [item['name'] for item in filtered_dimensionsList]\r\n",
							"for i in landing_files:\r\n",
							"    df.loc[df['FileName'] == i[\"name\"][:i[\"name\"].index('_')], 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"\r\n",
							"# Guardar el DataFrame modificado en un nuevo flujo de bytes\r\n",
							"output_stream = io.BytesIO()\r\n",
							"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')  # Usar line_terminator para evitar lÃ­neas en blanco\r\n",
							"output_stream.seek(0)  # Regresar al inicio del flujo\r\n",
							"\r\n",
							"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
						],
						"outputs": [],
						"execution_count": 132
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test_notebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a0cf62bb-704b-41ab-9208-31be1c1530e3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters_overwritten"
							]
						},
						"source": [
							"# This cell is generated from runtime parameters. Learn more: https://go.microsoft.com/fwlink/?linkid=2161015\n",
							"data_lake_container = \"abfss://mdw@datalake1pgc.dfs.core.windows.net\"\n",
							"gold_folder_logs = \"gold/Logs\"\n",
							"container_name = \"mdw\"\n",
							"metadata_folder = \"mdw/bronze/Configuration\"\n",
							"tables_to_load = \"[{\\\"SchemaName,TableName,UpdateDate,Load\\\":\\\"dim,Articles,2024-09-08,Y\\\"},{\\\"SchemaName,TableName,UpdateDate,Load\\\":\\\"dim,Client,1990-01-01,Y\\\"},{\\\"SchemaName,TableName,UpdateDate,Load\\\":\\\"dim,Currency,1990-01-01,Y\\\"},{\\\"SchemaName,TableName,UpdateDate,Load\\\":\\\"dim,Date,1990-01-01,Y\\\"},{\\\"SchemaName,TableName,UpdateDate,Load\\\":\\\"dim,Hours,1990-01-01,Y\\\"},{\\\"SchemaName,TableName,UpdateDate,Load\\\":\\\"dim,OperationType,1990-01-01,Y\\\"},{\\\"SchemaName,TableName,UpdateDate,Load\\\":\\\"dim,PostalCode,1990-01-01,Y\\\"},{\\\"SchemaName,TableName,UpdateDate,Load\\\":\\\"dim,Tariff,1990-01-01,Y\\\"},{\\\"SchemaName,TableName,UpdateDate,Load\\\":\\\"dim,Warehouse,1990-01-01,Y\\\"},{\\\"SchemaName,TableName,UpdateDate,Load\\\":\\\"fact,Sales,2024-09-08,Y\\\"}]\"\n",
							"golden_folder_dimensions = \"gold/Dimensions\"\n",
							"golden_folder_fact = \"gold/Fact\"\n",
							""
						],
						"outputs": [],
						"execution_count": 114
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Import modules**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format,concat_ws,xxhash64,current_timestamp\r\n",
							"from pyspark.sql import SparkSession,Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"import json\r\n",
							"import re\r\n",
							"from datetime import datetime\r\n",
							"from azure.storage.blob import BlobServiceClient\r\n",
							"import io\r\n",
							"import pandas as pd\r\n",
							"import csv"
						],
						"outputs": [],
						"execution_count": 115
					},
					{
						"cell_type": "code",
						"source": [
							"# %%sql\r\n",
							"# USE default;\r\n",
							"# DROP DATABASE  gold CASCADE ;\r\n",
							"# USE default;\r\n",
							"# CREATE DATABASE IF NOT EXISTS gold;\r\n",
							"# USE gold;"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# %%sql\r\n",
							"# USE gold;\r\n",
							"# DROP TABLE IF EXISTS fact_Sales;"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"factList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Sales\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idDesgloseVenta AS idSales,\r\n",
							"                        d.idArticulo AS idArticles,\r\n",
							"                        c.idAlmacen AS idWarehouse,\r\n",
							"                        c.idCliente AS idClient,\r\n",
							"                        c.idCodigoPostal AS idPostalCode,\r\n",
							"                        c.idDivisa AS idCurrency,\r\n",
							"                        c.idTarifa AS idTariff,\r\n",
							"                        c.idTipoOperacion AS idOperationType,\r\n",
							"                        c.idHora AS idHours,\r\n",
							"                        c.idFecha AS idDate,\r\n",
							"                        f.fecha AS  date,\r\n",
							"                        CAST(CONCAT(DATE_FORMAT(f.fecha, 'yyyy-MM-dd'), ' ', h.horaCompleta) AS TIMESTAMP) AS dateOp,\r\n",
							"                        COALESCE(c.codigoTicket, 'D') AS ticketNumber,\r\n",
							"                        COALESCE(d.Cantidad, 0) AS quantity,\r\n",
							"                        COALESCE(d.PrecioUnitario, 0)  AS unitPrice,\r\n",
							"                        COALESCE(d.CosteUnitario, 0)  AS unitCost,\r\n",
							"                        COALESCE(d.importeBruto, 0)  AS amtTotalEuros,\r\n",
							"                        COALESCE(d.importeNeto, 0)  AS amtNetEuros,\r\n",
							"                        COALESCE(d.importeDescuento, 0)  AS amtTotalEurosDiscount,\r\n",
							"                        COALESCE(d.importeNetoDescuento, 0) AS amtNetEurosDiscount,\r\n",
							"                        CASE WHEN idTarifa IN (2,3) THEN 1\r\n",
							"                            ELSE 0 \r\n",
							"                        END AS discountSale,\r\n",
							"                        CASE WHEN idTarifa = 0 THEN 0\r\n",
							"                            ELSE (d.importeBruto - d.importeDescuento) \r\n",
							"                        END AS amtTotalDiscount,\r\n",
							"                        CASE WHEN idTarifa = 0 THEN 0 \r\n",
							"                            ELSE (d.importeNeto - d.importeNetoDescuento) \r\n",
							"                        END AS amtNetDiscount,\r\n",
							"                        GREATEST(d.fechaCarga, c.fechaCarga,) AS loadDate,\r\n",
							"                        GREATEST(d.fechaDelta, c.fechaDelta,) AS deltaDate\r\n",
							"                    FROM default.desgloseVenta AS d\r\n",
							"                    INNER JOIN default.cabeceraVenta AS c\r\n",
							"                        ON d.idcabecerasVentas = c.idcabeceraVenta\r\n",
							"                    LEFT JOIN default.Fechas  AS f\r\n",
							"                        On f.idFechas = c.idFecha\r\n",
							"                    LEFT JOIN default.horas  AS h\r\n",
							"                        On h.idHoras = c.idHora\r\n",
							"                    WHERE GREATEST(d.fechaDelta, c.fechaDelta) >=\r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 116
					},
					{
						"cell_type": "code",
						"source": [
							"dimensionsList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Articles\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idArticulos                                                                                             AS idArticles,\r\n",
							"                            COALESCE(nombre, 'D')                                                                                   AS name,\r\n",
							"                            COALESCE(descripcion, 'D')                                                                              AS description,\r\n",
							"                            COALESCE(codigoReferencia, 'D')                                                                         AS externalCode,\r\n",
							"                            COALESCE(t.talla, 'D')                                                                                  AS size,\r\n",
							"                            COALESCE( t.numeroTalla, -1)                                                                            AS numSize,\r\n",
							"                            COALESCE(co.color, 'D')                                                                                 AS colour,\r\n",
							"                            COALESCE(ca.categoria, 'D')                                                                             AS category,\r\n",
							"                            COALESCE(l.codigoLinea, 'D')                                                                            AS codLine,\r\n",
							"                            COALESCE(l.Linea, 'D')                                                                                  AS line, \r\n",
							"                            CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                                 WHEN a.idCategoria IN (2,4,6,7,9) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                                 ELSE CAST(CONCAT('PV',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                            END                                                                                                     AS season,              \r\n",
							"                            GREATEST(a.fechaCarga, t.fechaCarga, co.fechaCarga, ca.fechaCarga, l.fechaCarga)                        AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta)                        AS deltaDate\r\n",
							"                    FROM default.articulos AS a\r\n",
							"                        LEFT JOIN default.talla AS t\r\n",
							"                            ON t.idTalla = a.idTalla\r\n",
							"                        LEFT JOIN default.color AS co\r\n",
							"                            ON co.idColor = a.idColor\r\n",
							"                        LEFT JOIN default.categoria AS ca -- select * from default.categoria\r\n",
							"                            ON ca.idCategoria = a.idCategoria\r\n",
							"                        LEFT JOIN default.Linea AS l\r\n",
							"                            ON l.idLinea = a.idLinea\r\n",
							"                    WHERE GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta) >= \r\n",
							"                    -- where  a.fechaCarga  < '20241009'\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Client\",\r\n",
							"            \"query\": \"\"\"              \r\n",
							"                    SELECT  idCliente                                           AS idClient,\r\n",
							"                            COALESCE(c.nombre, 'D')                             AS name,\r\n",
							"                            COALESCE(apellido1, 'D')                            AS lastName1,\r\n",
							"                            COALESCE(apellido2, 'D')                            AS lastName2, \r\n",
							"                            COALESCE(email, 'D')                                AS email,\r\n",
							"                            COALESCE(telefono, 'D')                             AS phoneNumber,\r\n",
							"                            COALESCE(CAST(cumpleanos AS STRING), '1900-01-01')  AS birthDay,       \r\n",
							"                            YEAR(current_date()) - YEAR(CAST(cumpleanos AS DATE)) \r\n",
							"                                - CASE \r\n",
							"                                    WHEN MONTH(current_date()) < MONTH(CAST(cumpleanos AS DATE)) \r\n",
							"                                        OR (MONTH(current_date()) = MONTH(CAST(cumpleanos AS DATE)) AND DAY(current_date()) < DAY(CAST(cumpleanos AS DATE)))\r\n",
							"                                    THEN 1\r\n",
							"                                    ELSE 0\r\n",
							"                            END                                                 AS age,\r\n",
							"                            CASE WHEN hombre = 1 THEN 'Hombre' \r\n",
							"                                 ELSE 'Mujer' \r\n",
							"                            END                                                 AS gender,\r\n",
							"                            COALESCE(p.nombre, 'D')                             AS country,\r\n",
							"                            COALESCE(p.codigoPais, 'D')                         AS countryCode,\r\n",
							"                            COALESCE(cp.region, 'D')                            AS region,\r\n",
							"                            COALESCE(c.Direcion, 'D')                           AS address,  \r\n",
							"                            COALESCE(codigoPostal, 'D')                         AS postalCode,\r\n",
							"                            COALESCE(activo, false)                             AS active,  -- Cambiado 0 a false aquÃ­\r\n",
							"                            GREATEST(c.fechaCarga, p.fechaCarga, cp.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) AS deltaDate\r\n",
							"                        FROM default.cliente AS c\r\n",
							"                            LEFT JOIN default.pais AS p \r\n",
							"                                ON c.idPais = p.idPais\r\n",
							"                            INNER JOIN default.codigoPostal AS cp \r\n",
							"                                ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\r\n",
							"                        WHERE GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Currency\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idDivisa              AS idCurrency,\r\n",
							"                            COALESCE(nombre, 'D') AS name,\r\n",
							"                            COALESCE(Divisa, 'D') AS currency,\r\n",
							"                            fechaCarga            AS loadDate,\r\n",
							"                            fechaDelta            AS deltaDate\r\n",
							"                    FROM default.divisa\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Date\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idFechas                                  AS idDate,\r\n",
							"                            ClaveFecha                                AS dateKey,\r\n",
							"                            Fecha                                     AS date,\r\n",
							"                            COALESCE(Mes, 'D')                        AS month,\r\n",
							"                            COALESCE(NumeroMes, -1)                   AS monthNumber,\r\n",
							"                            COALESCE(Ano, -1)                         AS year,\r\n",
							"                            COALESCE(NumeroSemana, -1)                AS weekNumber,\r\n",
							"                            COALESCE(DiaSemana, 'D')                  AS dayWeek,\r\n",
							"                            COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\r\n",
							"                            COALESCE(DiaAno, -1)                      AS yearDay,\r\n",
							"                            CASE WHEN Trimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Trimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Trimestre = 3 THEN 'Q3'\r\n",
							"                                ELSE '-1' END                         AS quarter,\r\n",
							"                            CASE WHEN Cuatrimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Cuatrimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Cuatrimestre = 3 THEN 'Q3'\r\n",
							"                                WHEN Cuatrimestre = 4 THEN 'Q4'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS quadrimester,\r\n",
							"                            CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\r\n",
							"                                WHEN Cuatrimestre IN (2,4) THEN 'S2'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS semester,\r\n",
							"                            fechaCarga                                AS loadDate,\r\n",
							"                            fechaDelta                                AS deltaDate\r\n",
							"                    FROM default.fechas\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Hours\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idHoras                     AS idHours,\r\n",
							"                            COALESCE(Hora, -1)          AS hour,\r\n",
							"                            COALESCE(Minuto, -1)        AS minute,\r\n",
							"                            COALESCE(HoraCompleta, 'D') AS fullHour,\r\n",
							"                            fechaCarga                  AS loadDate,\r\n",
							"                            fechaDelta                  AS deltaDate\r\n",
							"                    FROM default.horas\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"OperationType\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTipoOperacion          AS idOperationType,\r\n",
							"                            COALESCE(Operacion, 'D') AS operation,\r\n",
							"                            fechaCarga               AS loadDate,\r\n",
							"                            fechaDelta               AS deltaDate\r\n",
							"                    FROM default.tipoOperacion\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"PostalCode\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idCodigoPostal                       AS idPostalCode,\r\n",
							"                            COALESCE(codigoPostal, 'D')          AS postalCode,\r\n",
							"                            COALESCE(region, 'D')                AS region,\r\n",
							"                            COALESCE(c.codigoPais, 'D')          AS countryCode,\r\n",
							"                            COALESCE(nombre, 'D')                AS country,\r\n",
							"                            GREATEST(c.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(c.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM default.codigoPostal AS c\r\n",
							"                    LEFT JOIN default.pais AS p\r\n",
							"                        ON p.codigoPais = c.codigoPais\r\n",
							"                    WHERE GREATEST(c.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Tariff\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTarifa              AS idTariff,\r\n",
							"                            COALESCE(Tarifa, 'D') AS tariff,\r\n",
							"                            fechaCarga            AS loadDate,\r\n",
							"                            fechaDelta            AS deltaDate       \r\n",
							"                    FROM default.tarifa\r\n",
							"                    WHERE fechaDelta >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Warehouse\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idAlmacenes                          AS idWarehouse,\r\n",
							"                            COALESCE(a.Nombre, 'D')              AS warehouse,\r\n",
							"                            COALESCE(a.codigoAlmacen, 'D')       AS externalCode,\r\n",
							"                            COALESCE(p.codigoPais, 'D')          AS countryCode, \r\n",
							"                            COALESCE(p.nombre, 'D')              AS country,\r\n",
							"                            COALESCE(a.ciudad, 'D')              AS city,\r\n",
							"                            COALESCE(a.Direccion, 'D')           AS address,\r\n",
							"                            COALESCE(a.descripcion, 'D')         AS description,\r\n",
							"                            GREATEST(a.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM default.almacenes AS a\r\n",
							"                    LEFT JOIN default.pais AS p\r\n",
							"                        ON p.idpais = a.idPais\r\n",
							"                    --WHERE a.fechaDelta <> 20241011\r\n",
							"                    --UNION \r\n",
							"                    --select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\r\n",
							"                    --UNION \r\n",
							"                    --select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\r\n",
							"                    WHERE GREATEST(a.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"\r\n",
							"        ,        {\r\n",
							"            \"name\": \"Inventada\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idAlmacenes                          AS idWarehouse,\r\n",
							"                            COALESCE(a.Nombre, 'D')              AS warehouse,\r\n",
							"                            COALESCE(a.codigoAlmacen, 'D')       AS externalCode,\r\n",
							"                            COALESCE(p.codigoPais, 'D')          AS countryCode, \r\n",
							"                            COALESCE(p.nombre, 'D')              AS country,\r\n",
							"                            COALESCE(a.ciudad, 'D')              AS city,\r\n",
							"                            COALESCE(a.Direccion, 'D')           AS address,\r\n",
							"                            COALESCE(a.descripcion, 'D')         AS description,\r\n",
							"                            GREATEST(a.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM default.almacenes AS a\r\n",
							"                    LEFT JOIN default.pais AS p\r\n",
							"                        ON p.idpais = a.idPais\r\n",
							"                    --WHERE a.fechaDelta <> 20241011\r\n",
							"                    --UNION \r\n",
							"                    --select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\r\n",
							"                    --UNION \r\n",
							"                    --select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\r\n",
							"                    WHERE GREATEST(a.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 117
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Configuration**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"#fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
							"#fecha_delta = '20241008'\r\n",
							"#tables_to_load = json.loads(tables_to_load)  # Convertimos en array la variable\r\n",
							"\r\n",
							"tables_to_load = json.loads(tables_to_load)\r\n",
							"\r\n",
							"fecha_hoy = datetime.now()\r\n",
							"# Formato cadena 'yyyy-mm-dd'\r\n",
							"delta_date_filter = (fecha_hoy.strftime('%Y%m%d'))\r\n",
							"#fecha_delta = '20241008'\r\n",
							"\r\n",
							"# Formato cadena 'yyyy-mm-dd'\r\n",
							"delta_date_string = fecha_hoy.strftime('%Y-%m-%d')\r\n",
							"\r\n",
							"# Convertir cada valor del CSV en un diccionario estructurado\r\n",
							"dict_tables_to_load = []\r\n",
							"\r\n",
							"for row in tables_to_load:\r\n",
							"    csv_data = row[\"SchemaName,TableName,UpdateDate,Load\"]\r\n",
							"    \r\n",
							"    # Usar el lector de csv para separar los campos\r\n",
							"    reader = csv.reader(io.StringIO(csv_data))\r\n",
							"    \r\n",
							"    for schema, table, update_date, load in reader:\r\n",
							"        dict_tables_to_load.append({\r\n",
							"            \"SchemaName\": schema,\r\n",
							"            \"TableName\": table,\r\n",
							"            \"UpdateDate\": update_date,\r\n",
							"            \"Load\": load\r\n",
							"        })\r\n",
							"\r\n",
							"# Variables para almacenar los datos de 'dim' y 'fact'\r\n",
							"dim_tables = []\r\n",
							"fact_tables = []\r\n",
							"\r\n",
							"# Clasificar los datos\r\n",
							"for row in dict_tables_to_load:\r\n",
							"    if row['SchemaName'] == 'dim':\r\n",
							"        dim_tables.append(row)\r\n",
							"    elif row['SchemaName'] == 'fact':\r\n",
							"        fact_tables.append(row)\r\n",
							"\r\n",
							"\r\n",
							"load_y_tables = {table['TableName'] for table in dim_tables if table['Load'] == 'Y'}\r\n",
							"#print(load_y_tables)\r\n",
							"# Filtrar el dimensionsList para obtener solo las tablas con Load='Y' y devolver una lista de diccionarios\r\n",
							"filtered_dimensionsList = [\r\n",
							"    dim for dim in dimensionsList if dim[\"name\"] in load_y_tables\r\n",
							"]\r\n",
							"\r\n",
							"\r\n",
							"load_y_tables2 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
							"#print(load_y_tables2)\r\n",
							"# Filtrar el dimensionsList para obtener solo las tablas con Load='Y' y devolver una lista de diccionarios\r\n",
							"filtered_factList = [\r\n",
							"    dim for dim in factList if dim[\"name\"] in load_y_tables2\r\n",
							"]\r\n",
							""
						],
						"outputs": [],
						"execution_count": 118
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Load SCD2 Dimensions**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Carga o Merge Dimensiones\r\n",
							"results = []\r\n",
							"exception = []\r\n",
							"\r\n",
							"for file in dimensionsList:\r\n",
							"    file_result = {\r\n",
							"        'table': f\"dim_{file['name']}\",\r\n",
							"        'status': 'Incorrect',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now()) \r\n",
							"    }\r\n",
							"\r\n",
							"    try:\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"There is no new data\"\r\n",
							"            results.append(file_result)\r\n",
							"            continue\r\n",
							"\r\n",
							"        table_name = file[\"name\"].split('_')[0]\r\n",
							"        source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
							"        key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
							"\r\n",
							"        key_columns = key_columns_str.split(',')\r\n",
							"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"        #delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Dimensions/{table_name}\"\r\n",
							"        delta_table_path_dimensions = f\"{data_lake_container}/{golden_folder_dimensions}/{table_name}\"\r\n",
							"        # Leer archivo(s) en DataFrame de Spark\r\n",
							"        # Meter filtro deltaDate\r\n",
							"\r\n",
							"        df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
							"        sdf = df.cache()\r\n",
							"\r\n",
							"        \r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path_dimensions):\r\n",
							"            # AÃ±adir las columnas necesarias\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            todas_las_columnas = sdf.columns\r\n",
							"            columnas_al_principio = [f\"id{table_name}\"]\r\n",
							"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
							"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
							"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
							"            sdf = sdf.select(*nuevo_orden_columnas)\r\n",
							"\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_view\")\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                MERGE INTO gold.dim_{table_name}  \r\n",
							"                AS existing\r\n",
							"                USING temp_view AS updates\r\n",
							"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
							"                WHEN MATCHED AND existing.isCurrent = 1 AND existing.hash != updates.hash THEN\r\n",
							"                UPDATE SET\r\n",
							"                    existing.toDate = current_date(),\r\n",
							"                    existing.isCurrent = 0\r\n",
							"            \"\"\")\r\n",
							"\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                INSERT INTO gold.dim_{table_name}    \r\n",
							"                SELECT \r\n",
							"                    {next_surrogate_key} + row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS sk{table_name}, \r\n",
							"                    updates.*\r\n",
							"                FROM temp_view AS updates\r\n",
							"                LEFT JOIN gold.dim_{table_name}  \r\n",
							"                AS existing\r\n",
							"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
							"                WHERE existing.{key_columns_str} IS NULL OR existing.isCurrent = 0 OR existing.hash != updates.hash\r\n",
							"            \"\"\")\r\n",
							"\r\n",
							"            # Almacenar conteos de filas\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.dim_{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            file_result['inserted_rows'] = last_result[0].operationMetrics[\"numTargetRowsInserted\"]\r\n",
							"            file_result['updated_rows'] = last_result[0].operationMetrics[\"numTargetRowsUpdated\"]\r\n",
							"\r\n",
							"\r\n",
							"        else: # Crear nueva tabla Delta\r\n",
							"      \r\n",
							"            last_surrogate_key = 0  # Iniciar con 0 si la tabla no existe\r\n",
							"            #sdf = sdf.withColumn(f\"sk{table_name}\", F.monotonically_increasing_id() + last_surrogate_key + 1)  # Asigna un valor Ãºnico a cada fila.\r\n",
							"            sdf = sdf.withColumn(f\"sk{table_name}\", F.col(f\"id{table_name}\"))\r\n",
							"            # AÃ±adir las columnas necesarias\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            # Hash\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\" ,'fechaCarga', 'fechaDelta','fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # Reorganizar la estructura de la tabla\r\n",
							"            todas_las_columnas = sdf.columns\r\n",
							"            columnas_al_principio = [f\"sk{table_name}\", f\"id{table_name}\"]\r\n",
							"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
							"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
							"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
							"            sdf_reordenado = sdf.select(*nuevo_orden_columnas)\r\n",
							"            \r\n",
							"            # Crear la tabla Delta\r\n",
							"            sdf_reordenado.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path_dimensions)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.dim_{table_name} USING DELTA LOCATION \\'{delta_table_path_dimensions}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            #spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"            #spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')\")\r\n",
							"\r\n",
							"            # Almacenar archivo procesado correctamente\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"            file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Load Fact Table**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Carga FULL tabla/s de hechos\r\n",
							"\r\n",
							"# Inicializar listas para almacenar resultados\r\n",
							"# results = []\r\n",
							"# exception = []\r\n",
							"\r\n",
							"\r\n",
							"for file in filtered_factList:\r\n",
							"    file_result = {\r\n",
							"        'table': f\"fact_{file['name']}\",\r\n",
							"        'status': 'Incorrect',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now())  \r\n",
							"    }\r\n",
							"    table_name = file[\"name\"].split('_')[0]\r\n",
							"\r\n",
							"    # Definir la ruta de la tabla Delta\r\n",
							"    delta_table_path_fact = f\"{data_lake_container}/{golden_folder_fact}/{table_name}\"\r\n",
							"    #delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Fact/{table_name}\"\r\n",
							"    #file[\"query\"] = file[\"query\"].strip() + f\" '{delta_date_filter}'\"\r\n",
							"    #df = spark.sql(file[\"query\"]) \r\n",
							"    df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
							"    sdf = df.cache()\r\n",
							"\r\n",
							"    try:\r\n",
							"        # Leer el archivo CSV en un DataFrame de Spark\r\n",
							"        #sdf = spark.read.csv(f\"gs://your-bucket/{file['name']}\", header=True, inferSchema=True)\r\n",
							"\r\n",
							"        # Verificar si el DataFrame estÃ¡ vacÃ­o\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"There is no new data\"\r\n",
							"            results.append(file_result)\r\n",
							"            continue\r\n",
							"\r\n",
							"        # AÃ±adir columnas necesarias\r\n",
							"        # sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss')) \\\r\n",
							"        #          .withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss')) \\\r\n",
							"        #          .withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"        # Truncar la tabla Delta si existe\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path_fact):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path_fact)\r\n",
							"            delta_table.delete()  # Truncar la tabla Delta\r\n",
							"\r\n",
							"            #last_row number para incremental\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                INSERT INTO gold.fact_{table_name}  \r\n",
							"                SELECT ROW_NUMBER() OVER(ORDER BY s.dateOP) AS skSales, s.*, \r\n",
							"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
							"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
							"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
							"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
							"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
							"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
							"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
							"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
							"                        COALESCE(d.skDate, -1) AS skDate\r\n",
							"                FROM temp_sales_view as s\r\n",
							"                LEFT JOIN gold.dim_Articles AS a\r\n",
							"                    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
							"                LEFT JOIN gold.dim_Client AS cl\r\n",
							"                    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
							"                LEFT JOIN gold.dim_Currency AS cu\r\n",
							"                    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
							"                LEFT JOIN gold.dim_Date AS d\r\n",
							"                    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
							"                LEFT JOIN gold.dim_Hours AS h\r\n",
							"                    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
							"                LEFT JOIN gold.dim_OperationType AS o\r\n",
							"                    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
							"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
							"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
							"                LEFT JOIN gold.dim_Tariff AS t\r\n",
							"                    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
							"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
							"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
							"                \"\"\")\r\n",
							"        else:\r\n",
							"\r\n",
							"            #window_spec = Window.orderBy(\"dateOp\", \"idArticles\")\r\n",
							"            #sdf = sdf.withColumn(f\"sk{table_name}\", F.row_number().over(window_spec))\r\n",
							"            #todas_las_columnas = sdf.columns\r\n",
							"            #columnas_al_principio = [f\"sk{table_name}\"]  # Suponiendo que la tabla tiene una columna idWarehouse\r\n",
							"            #otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio]\r\n",
							"            #nuevo_orden_columnas = columnas_al_principio + otras_columnas\r\n",
							"            #sdf = sdf.select(*nuevo_orden_columnas)\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
							"            sdf = spark.sql(f\"\"\" \r\n",
							"                SELECT ROW_NUMBER() OVER(ORDER BY s.dateOP) AS skSales, s.*, \r\n",
							"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
							"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
							"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
							"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
							"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
							"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
							"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
							"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
							"                        COALESCE(d.skDate, -1) AS skDate\r\n",
							"                FROM temp_sales_view AS s\r\n",
							"                LEFT JOIN gold.dim_Articles AS a\r\n",
							"                    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
							"                LEFT JOIN gold.dim_Client AS cl\r\n",
							"                    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
							"                LEFT JOIN gold.dim_Currency AS cu\r\n",
							"                    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
							"                LEFT JOIN gold.dim_Date AS d\r\n",
							"                    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
							"                LEFT JOIN gold.dim_Hours AS h\r\n",
							"                    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
							"                LEFT JOIN gold.dim_OperationType AS o\r\n",
							"                    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
							"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
							"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
							"                LEFT JOIN gold.dim_Tariff AS t\r\n",
							"                    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
							"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
							"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
							"                \"\"\")\r\n",
							"\r\n",
							"        # Cargar datos en la tabla Delta\r\n",
							"            sdf.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_fact)\r\n",
							"            #sdf.write.format(\"delta\").mode(\"append\").save(delta_table_path)  # valorar carga incremental mÃ¡s adelante\r\n",
							"\r\n",
							"            # Crear la tabla Delta en caso de que no exista\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.fact_{table_name} USING DELTA LOCATION \\'{delta_table_path_fact}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"            \r\n",
							"        # Almacenar conteos de filas\r\n",
							"        file_result['status'] = 'Correct'\r\n",
							"        file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Guardar ficheros Logs**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Obtener la fecha actual\r\n",
							"fecha_actual = datetime.now()\r\n",
							"year = fecha_actual.strftime('%Y')  # AÃ±o actual (por ejemplo, '2024')\r\n",
							"month = fecha_actual.strftime('%m')  # Mes actual (por ejemplo, '10')\r\n",
							"day = fecha_actual.strftime('%d')  # DÃ­a actual (por ejemplo, '24')\r\n",
							"hora = fecha_actual.strftime('%H%M%S')  # Hora actual en formato HHMMSS (por ejemplo, '235959')\r\n",
							"\r\n",
							"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
							"archivo_nombre = f\"Log_{day}_{hora}.json\"\r\n",
							"\r\n",
							"# Reemplaza con tu cadena de conexiÃ³n a Azure\r\n",
							"\r\n",
							"# Cliente de servicio de blobs\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# Convetimos el diccionario a formato json\r\n",
							"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
							"\r\n",
							"# Crear la ruta de destino con la jerarquÃ­a AÃ±o/Mes y el nombre del archivo\r\n",
							"destination_blob_name = f\"{gold_folder_logs}/{year}/{month}/{archivo_nombre}\"\r\n",
							"\r\n",
							"# Crear un cliente del blob de destino\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=container_name, blob=destination_blob_name)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Actualizar fichero metadata**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Configurar la conexiÃ³n\r\n",
							"\r\n",
							"#delta_date_string = '1900-01-01'\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# Definir los nombres de los contenedores y blobs\r\n",
							"#container_name = \"etl/bronze/Configuration\"\r\n",
							"#metadata_folder = \"mdw/bronze/Configuration\"\r\n",
							"container_name = metadata_folder\r\n",
							"csv_blob_name = \"ConfiguracionReporting.csv\"  # Reemplaza con el nombre de tu archivo\r\n",
							"\r\n",
							"# Leer el archivo CSV desde el Data Lake\r\n",
							"blob_client = blob_service_client.get_blob_client(container=container_name, blob=csv_blob_name)\r\n",
							"\r\n",
							"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
							"df = pd.read_csv(stream, sep=',')  # Especifica el separador aquÃ­\r\n",
							"\r\n",
							"# Limpiar nombres de columnas para eliminar espacios en blanco\r\n",
							"df.columns = df.columns.str.strip()\r\n",
							"\r\n",
							"names_dimensionsList = [item['name'] for item in filtered_dimensionsList]\r\n",
							"for i in names_dimensionsList:\r\n",
							"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"\r\n",
							"names_factList = [item['name'] for item in filtered_factList]\r\n",
							"for i in names_factList:\r\n",
							"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas segÃºn tu archivo CSV\r\n",
							"\r\n",
							"\r\n",
							"# Guardar el DataFrame modificado en un nuevo flujo de bytes\r\n",
							"output_stream = io.BytesIO()\r\n",
							"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')  # Usar line_terminator para evitar lÃ­neas en blanco\r\n",
							"output_stream.seek(0)  # Regresar al inicio del flujo\r\n",
							"\r\n",
							"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
						],
						"outputs": [],
						"execution_count": 113
					},
					{
						"cell_type": "code",
						"source": [
							"        # # Comprobar si la tabla Delta existe\r\n",
							"        # if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"        #     delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"        #     existing_columns = delta_table.toDF().columns\r\n",
							"            \r\n",
							"        #     # Obtener la Ãºltima surrogate key para crear nuevas\r\n",
							"        #     last_surrogate_key = delta_table.toDF().agg(F.max(f\"sk{table_name}\")).collect()[0][0] or 0\r\n",
							"        #     next_surrogate_key = last_surrogate_key + 1\r\n",
							"            \r\n",
							"        #     # Merge new data into existing table con lÃ³gica SCD Tipo 2\r\n",
							"        #     delta_table.alias(\"existing\").merge(\r\n",
							"        #         source=sdf.alias(\"updates\"),\r\n",
							"        #         condition=\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])\r\n",
							"        #     ).whenMatchedUpdate(\r\n",
							"        #         condition=\"existing.isCurrent = 1 AND existing.hash != updates.hash\",\r\n",
							"        #         set={\r\n",
							"        #             \"toDate\": F.current_date(),\r\n",
							"        #             \"isCurrent\": F.lit(0)\r\n",
							"        #         }\r\n",
							"        #     ).whenNotMatchedInsert(\r\n",
							"        #         values={\r\n",
							"        #             f\"sk{table_name}\": F.expr(f\"{next_surrogate_key} + row_number() over (order by (select null)) - 1\"),  # Aumenta la surrogate key en base a la fila\r\n",
							"        #             **{f\"updates.{col}\": f\"updates.{col}\" for col in existing_columns}\r\n",
							"        #         }\r\n",
							"        #     ).execute()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkTFM')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "francecentral"
		}
	]
}