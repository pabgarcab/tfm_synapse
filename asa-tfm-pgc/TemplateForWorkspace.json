{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Nombre del Ã¡rea de trabajo",
			"defaultValue": "asa-tfm-pgc"
		},
		"asa-tfm-pgc-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"connectionString\"de \"asa-tfm-pgc-WorkspaceDefaultSqlServer\"",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:asa-tfm-pgc.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"ls_ADLG2_Bronze_Landing_accountKey": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"accountKey\"de \"ls_ADLG2_Bronze_Landing\""
		},
		"ls_COLI_ERP_password": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"password\"de \"ls_COLI_ERP\""
		},
		"asa-tfm-pgc-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalake1pgc.dfs.core.windows.net"
		},
		"ls_ADLG2_Bronze_Landing_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalake1pgc.dfs.core.windows.net/"
		},
		"ls_COLI_ERP_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "@{linkedService().NombreServidor}"
		},
		"ls_COLI_ERP_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "@{linkedService().NombreBaseDatos}"
		},
		"ls_COLI_ERP_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "sa"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/1 - SqlToLanding')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "List of Tables to Load",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "Clean Landing",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": false,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "csvConfiguration",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Save Landing Files",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "List of Tables to Load",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('List of tables to load').output.value",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Active For Load",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@not(equals(trim(item().Load), 'N'))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Carga Landing",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "SqlServerSource",
														"sqlReaderQuery": {
															"value": "SELECT *, @{formatDateTime(pipeline().TriggerTime,'yyyyMMdd')} AS fechaCarga, '@{pipeline().RunId}' AS pipelineID \nFROM @{item().SchemaName}.@{item().TableName} \nWHERE @{item().IncrementalColumn} >= CAST(CAST('@{item().UpdateDate}' AS nvarchar) AS datetime2);",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings",
															"copyBehavior": "FlattenHierarchy"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"parallelCopies": 1,
													"dataIntegrationUnits": 4,
													"translator": {
														"type": "TabularTranslator",
														"typeConversion": true,
														"typeConversionSettings": {
															"allowDataTruncation": true,
															"treatBooleanAsNumber": false
														}
													}
												},
												"inputs": [
													{
														"referenceName": "dsSQLGenerico",
														"type": "DatasetReference",
														"parameters": {
															"NombreServidor": "@item().ServerName",
															"NombreBD": "@item().DataBaseName",
															"NombreEsquema": "@item().SchemaName",
															"NombreTabla": "@item().TableName",
															"FechaActualizacion": "@item().UpdateDate",
															"ColumnaIncremental": {
																"value": "@item().IncrementalColumn",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "dsParquetRaw",
														"type": "DatasetReference",
														"parameters": {
															"NombreFichero": "@concat( item().FileName, '_', formatDateTime(utcnow(), 'yyyyMMdd'),'.', item().Format)"
														}
													}
												]
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "Clean Landing",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsParquetRaw",
								"type": "DatasetReference",
								"parameters": {
									"NombreFichero": "*"
								}
							},
							"logStorageSettings": {
								"linkedServiceName": {
									"referenceName": "ls_ADLG2_Bronze_Landing",
									"type": "LinkedServiceReference"
								},
								"path": "mdw/bronze/Logs"
							},
							"enableLogging": true,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"wildcardFileName": "*",
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-16T00:20:34Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/csvConfiguration')]",
				"[concat(variables('workspaceId'), '/datasets/dsParquetRaw')]",
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]",
				"[concat(variables('workspaceId'), '/datasets/dsSQLGenerico')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/1 - SqlToLanding_old')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Listado tablas a cargar",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": false,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "csvConfiguration",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Listado tablas a cargar",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Listado tablas a cargar').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Copiar Tablas de Configuracion",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlServerSource",
											"queryTimeout": "02:00:00",
											"partitionOption": "None"
										},
										"sink": {
											"type": "ParquetSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "ParquetWriteSettings"
											}
										},
										"enableStaging": false,
										"parallelCopies": 1,
										"dataIntegrationUnits": 4,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "dsSQLGenerico",
											"type": "DatasetReference",
											"parameters": {
												"NombreServidor": {
													"value": "@item().ServerName",
													"type": "Expression"
												},
												"NombreBD": {
													"value": "@item().DataBaseName",
													"type": "Expression"
												},
												"NombreEsquema": {
													"value": "@item().SchemaName",
													"type": "Expression"
												},
												"NombreTabla": {
													"value": "@item().TableName",
													"type": "Expression"
												},
												"FechaActualizacion": {
													"value": "@item().UpdateDate",
													"type": "Expression"
												},
												"ColumnaIncremental": {
													"value": "@item().IncrementalColumn",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "dsParquetRaw",
											"type": "DatasetReference",
											"parameters": {
												"NombreFichero": {
													"value": "@concat( item().FileName, '_', formatDateTime(utcnow(), 'yyyyMMdd'),'.', item().Format)\n",
													"type": "Expression"
												}
											}
										}
									]
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-15T16:45:27Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/csvConfiguration')]",
				"[concat(variables('workspaceId'), '/datasets/dsSQLGenerico')]",
				"[concat(variables('workspaceId'), '/datasets/dsParquetRaw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/2 - BronzeToSilver')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "UPSERT Silver",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Metadata Landing",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get Metadata Landing').output.childItems",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Clean Landing",
									"type": "Delete",
									"dependsOn": [
										{
											"activity": "LandingToProcessed",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataset": {
											"referenceName": "dsParquetRaw",
											"type": "DatasetReference",
											"parameters": {
												"NombreFichero": {
													"value": "@item().name",
													"type": "Expression"
												}
											}
										},
										"enableLogging": false,
										"storeSettings": {
											"type": "AzureBlobFSReadSettings",
											"recursive": false,
											"enablePartitionDiscovery": false
										}
									}
								},
								{
									"name": "LandingToProcessed",
									"type": "Copy",
									"dependsOn": [
										{
											"activity": "MergeBronzeToSilver",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "ParquetSource",
											"storeSettings": {
												"type": "AzureBlobFSReadSettings",
												"recursive": false,
												"enablePartitionDiscovery": false
											},
											"formatSettings": {
												"type": "ParquetReadSettings"
											}
										},
										"sink": {
											"type": "ParquetSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "ParquetWriteSettings"
											}
										},
										"enableStaging": false,
										"parallelCopies": 1,
										"dataIntegrationUnits": 4,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "dsParquetRaw",
											"type": "DatasetReference",
											"parameters": {
												"NombreFichero": {
													"value": "@item().name",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "dsParquetProcessed",
											"type": "DatasetReference",
											"parameters": {
												"NombreFichero": {
													"value": "@item().name",
													"type": "Expression"
												},
												"NombreCarpeta": {
													"value": "@concat(\n    substring(item().name, 0, indexOf(item().name, '_')), \n    '/', \n    substring(item().name, add(indexOf(item().name, '_'), 1), 4), \n    '/', \n    substring(item().name, add(indexOf(item().name, '_'), 5), 2), \n    '/'\n)\n",
													"type": "Expression"
												}
											}
										}
									]
								},
								{
									"name": "MergeBronzeToSilver",
									"type": "SynapseNotebook",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebook": {
											"referenceName": "MergeLandingFilesToSilver",
											"type": "NotebookReference"
										},
										"parameters": {
											"data_lake_container": {
												"value": "abfss://mdw@datalake1pgc.dfs.core.windows.net",
												"type": "string"
											},
											"bronze_folder": {
												"value": "bronze/Landing",
												"type": "string"
											},
											"table_name": {
												"value": {
													"value": "@substring(item().name, 0, indexOf(item().name, '_'))",
													"type": "Expression"
												},
												"type": "string"
											},
											"silver_folder": {
												"value": "silver",
												"type": "string"
											},
											"source_wildcard": {
												"value": {
													"value": "@concat(substring(item().name, 0, indexOf(item().name, '_')), '*.parquet')",
													"type": "Expression"
												},
												"type": "string"
											},
											"key_columns_str": {
												"value": {
													"value": "@concat('id',substring(item().name, 0, indexOf(item().name, '_')))",
													"type": "Expression"
												},
												"type": "string"
											}
										},
										"snapshot": true,
										"sparkPool": {
											"referenceName": "sparkTFM",
											"type": "BigDataPoolReference"
										},
										"executorSize": "Small",
										"conf": {
											"spark.dynamicAllocation.enabled": false,
											"spark.dynamicAllocation.minExecutors": 2,
											"spark.dynamicAllocation.maxExecutors": 2
										},
										"driverSize": "Small",
										"numExecutors": 2
									}
								}
							]
						}
					},
					{
						"name": "Get Metadata Landing",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsRawEntidades",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "ParquetReadSettings"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsRawEntidades')]",
				"[concat(variables('workspaceId'), '/datasets/dsParquetRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsParquetProcessed')]",
				"[concat(variables('workspaceId'), '/notebooks/MergeLandingFilesToSilver')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkTFM')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/3 - SilverToSCD2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Lista ficheros Landing",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsSilverSCD",
								"type": "DatasetReference",
								"parameters": {
									"NombreFichero": "*"
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "ParquetReadSettings"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [
							{
								"activity": "Lista ficheros Landing",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"activities": [
								{
									"name": "Delete1",
									"type": "Delete",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataset": {
											"referenceName": "dsSilverSCD",
											"type": "DatasetReference",
											"parameters": {}
										},
										"enableLogging": true,
										"storeSettings": {
											"type": "AzureBlobFSReadSettings",
											"recursive": true,
											"enablePartitionDiscovery": false
										}
									}
								},
								{
									"name": "Script1",
									"type": "Script",
									"dependsOn": [
										{
											"activity": "Delete1",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"linkedServiceName": {
										"referenceName": "asa-tfm-pgc-WorkspaceDefaultSqlServer",
										"type": "LinkedServiceReference",
										"parameters": {
											"DBName": "SilverlessSTG"
										}
									},
									"typeProperties": {
										"scripts": [
											{
												"type": "NonQuery",
												"text": ""
											}
										],
										"scriptBlockExecutionTimeout": "02:00:00"
									}
								},
								{
									"name": "Delete1_copy1",
									"type": "Delete",
									"dependsOn": [
										{
											"activity": "Script1",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"enableLogging": true
									}
								},
								{
									"name": "Script1_copy1",
									"type": "Script",
									"dependsOn": [
										{
											"activity": "Delete1_copy1",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"scripts": [],
										"scriptBlockExecutionTimeout": "02:00:00"
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"queryReloadSCD": {
						"type": "string"
					},
					"queryReloadGoldCETAs": {
						"type": "string"
					}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-14T15:15:28Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsSilverSCD')]",
				"[concat(variables('workspaceId'), '/linkedServices/asa-tfm-pgc-WorkspaceDefaultSqlServer')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/4 - SilverToFact')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Lista ficheros Landing",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsRawEntidades",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "ParquetReadSettings"
							}
						}
					},
					{
						"name": "Delete1",
						"type": "Delete",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [
							{
								"activity": "Lista ficheros Landing",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"enableLogging": true
						}
					},
					{
						"name": "Proccess SC2",
						"type": "SqlPoolStoredProcedure",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [
							{
								"activity": "Delete1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {}
					},
					{
						"name": "DROP and Load CETAs",
						"type": "SqlPoolStoredProcedure",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [
							{
								"activity": "Proccess SC2",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-14T15:15:28Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsRawEntidades')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ETL')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "1",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "1 - SqlToLanding",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "3",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "1_copy1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "3 - SilverToSCD2",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "4",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "3",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "4 - SilverToFact",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "1_copy1",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "2 - BronzeToSilver",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-15T16:46:15Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/1 - SqlToLanding')]",
				"[concat(variables('workspaceId'), '/pipelines/3 - SilverToSCD2')]",
				"[concat(variables('workspaceId'), '/pipelines/4 - SilverToFact')]",
				"[concat(variables('workspaceId'), '/pipelines/2 - BronzeToSilver')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvConfiguration')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "ConfiguracionOrigenes.csv",
						"folderPath": "bronze/Configuration",
						"fileSystem": "mdw"
					},
					"columnDelimiter": ";",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ServerName;DataBaseName;SchemaName;TableName;PathName;FileName;Load",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetProcessed')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					},
					"NombreCarpeta": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@concat('bronze/Processed/', dataset().NombreCarpeta)",
							"type": "Expression"
						},
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetRaw')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": "bronze/Landing",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsRawEntidades')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "bronze/Landing",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSQLGenerico')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_COLI_ERP",
					"type": "LinkedServiceReference",
					"parameters": {
						"NombreServidor": {
							"value": "@dataset().NombreServidor",
							"type": "Expression"
						},
						"NombreBaseDatos": {
							"value": "@dataset().NombreBD",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"NombreServidor": {
						"type": "string"
					},
					"NombreBD": {
						"type": "string"
					},
					"NombreEsquema": {
						"type": "string"
					},
					"NombreTabla": {
						"type": "string"
					},
					"FechaActualizacion": {
						"type": "string"
					},
					"ColumnaIncremental": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServerTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().NombreEsquema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().NombreTabla",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_COLI_ERP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSilverSCD')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "silver/SCD",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-tfm-pgc-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('asa-tfm-pgc-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-tfm-pgc-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('asa-tfm-pgc-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_ADLG2_Bronze_Landing')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ls_ADLG2_Bronze_Landing_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('ls_ADLG2_Bronze_Landing_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_COLI_ERP')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"NombreServidor": {
						"type": "string"
					},
					"NombreBaseDatos": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServer",
				"typeProperties": {
					"server": "[parameters('ls_COLI_ERP_properties_typeProperties_server')]",
					"database": "[parameters('ls_COLI_ERP_properties_typeProperties_database')]",
					"encrypt": "mandatory",
					"trustServerCertificate": true,
					"authenticationType": "SQL",
					"userName": "[parameters('ls_COLI_ERP_properties_typeProperties_userName')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('ls_COLI_ERP_password')]"
					}
				},
				"connectVia": {
					"referenceName": "IR-Coli-ERP",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/IR-Coli-ERP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IR-Coli-ERP')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure GoldelessSTG')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Configuration"
				},
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity' \nGO\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA dim AUTHORIZATION dbo\nGO\nCREATE SCHEMA fact AUTHORIZATION dbo",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure SilverlessSTG')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Configuration"
				},
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity'\nGO\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA etl AUTHORIZATION dbo;\nGO\nCREATE SCHEMA silver AUTHORIZATION dbo;\nGO\nCREATE SCHEMA scd AUTHORIZATION dbo;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Dimensions Silver')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "---------------\n-- Dim Date --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Date] AS \nSELECT  idFechas AS idDate,\n        ClaveFecha AS dateKey,\n        Fecha AS date,\n        COALESCE(Mes, 'D')  AS month,\n        COALESCE(NumeroMes, -1)  AS monthNumber,\n        COALESCE(Ano, -1)  AS year,\n        COALESCE(NumeroSemana, -1)  AS weekNumber,\n        COALESCE(DiaSemana, 'D') AS dayWeek,\n        COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\n        COALESCE(DiaAno, -1) AS yearDay,\n        CASE WHEN Trimestre = 1 THEN 'Q1'\n             WHEN Trimestre = 2 THEN 'Q2'\n             WHEN Trimestre = 3 THEN 'Q3'\n             ELSE '-1' END AS quarter,\n        CASE WHEN Cuatrimestre = 1 THEN 'Q1'\n             WHEN Cuatrimestre = 2 THEN 'Q2'\n             WHEN Cuatrimestre = 3 THEN 'Q3'\n             WHEN Cuatrimestre = 4 THEN 'Q4'\n             ELSE '-1' \n        END                                       AS quadrimester,\n        CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\n            WHEN Cuatrimestre IN (2,4) THEN 'S2'\n            ELSE '-1' \n        END                                       AS semester,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[fechas]\n\n------------------\n-- Dim Articles --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Articles] AS\nSELECT  idArticulos                                                                         AS idArticles,\n        COALESCE(nombre, 'D')                                                               AS name,\n        COALESCE(descripcion, 'D')                                                          AS description,\n        COALESCE(codigoReferencia, 'D')                                                     AS externalCode,\n        COALESCE(t.talla, 'D')                                                              AS size,\n        COALESCE( t.numeroTalla, -1)                                                        AS numSize,\n        COALESCE(co.color, 'D')                                                             AS colour,\n        COALESCE(ca.categoria, 'D')                                                         AS category,\n        COALESCE(l.codigoLinea, 'D')                                                        AS codLine,\n        COALESCE(l.Linea, 'D')                                                              AS line, \n        CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN 'OI'+CAST((a.idTemporada) AS nvarchar)\n             WHEN a.idCategoria IN (2,4,6,7,9) THEN 'OI'+CAST(a.idTemporada AS nvarchar)\n             ELSE 'PV'+CAST(a.idTemporada AS nvarchar)\n        END                                                                                 AS season,\n        a.fechaCarga                                                                        AS loadDate,\n        a.fechaDelta                                                                        AS deltaDate\nFROM [default].[dbo].[articulos] AS a\nLEFT JOIN [default].[dbo].[talla] AS t\n    ON t.idTalla = a.idTalla\nLEFT JOIN [default].[dbo].[color] AS co\n   ON co.idColor = a.idColor\nLEFT JOIN [default].[dbo].[categoria] AS ca -- select * from [default].[dbo].[categoria]\n    ON ca.idCategoria = a.idCategoria\nLEFT JOIN [default].[dbo].[Linea] AS l\n    ON l.idLinea = a.idLinea\n--where  a.fechaCarga  < '20241009'\n-------------------\n-- Dim Warehouse --\n-------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Warehouse] AS\nSELECT  idAlmacenes AS idWarehouse,\n        COALESCE(a.Nombre, 'D') AS warehouse,\n        COALESCE(a.codigoAlmacen, 'D') AS externalCode,\n        COALESCE(p.codigoPais, 'D') AS countryCode, \n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(a.ciudad, 'D') AS city,\n        COALESCE(a.Direccion, 'D') AS address,\n        COALESCE(a.descripcion, 'D') AS description,\n        a.fechaCarga AS loadDate,\n        a.fechaDelta AS deltaDate\nFROM [default].[dbo].[almacenes] AS a\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.idpais = a.idPais\n  -- WHERE a.fechaDelta <> 20241011\n--UNION \n--select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\n--UNION \n--select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\n\n\n---------------------\n-- Dim Postal Code --\n---------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_PostalCodes] AS\nSELECT  idCodigoPostal AS idPostalCodes,\n        COALESCE(codigoPostal, 'D') AS postalCode,\n        COALESCE(region, 'D') AS region,\n        COALESCE(c.codigoPais, 'D') AS countryCode,\n        COALESCE(nombre, 'D') AS country,\n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[codigoPostal] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.codigoPais = c.codigoPais\n\n------------------\n-- Dim Currency --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Currency] AS\nSELECT  idDivisa AS idCurrency,\n        COALESCE(nombre, 'D') AS name,\n        COALESCE(Divisa, 'D') AS currency,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[divisa]\n\n---------------\n-- Dim Hours --\n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Hours] AS\nSELECT  idHoras AS idHours,\n        COALESCE(Hora, -1) AS hour,\n        COALESCE(Minuto, -1) AS minute,\n        COALESCE(HoraCompleta, 'D') as fullHour,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[horas]\n\n----------------\n-- Dim Tariff --\n----------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Tariff] AS\nSELECT  idTarifa AS idTariff,\n        COALESCE(Tarifa, 'D') AS tariff,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate       \nFROM [default].[dbo].[tarifa]\n\n------------------------\n-- Dim Operation Type --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_OperationType] AS\nSELECT  idTipoOperacion AS idOperationType,\n        COALESCE(Operacion, 'D') AS operation,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[tipoOperacion]\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/First Load Gold CETAS SCD2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SCD"
				},
				"content": {
					"query": "---------------\n-- Articles  --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Articles]\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idArticles) idSkArticles,\n\tidArticles,\n\tname,\n\texternalcode,\n\tsize,\n\tnumSize,\n\tcolour,\n\tcategory, \n\tcodLine,\n\tline, \n\tdescription,\n\tseason, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent, \n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idArticles AS NVARCHAR)+ name + description COLLATE DATABASE_DEFAULT + externalcode + size + CAST(numSize AS NVARCHAR) + colour + category + codLine + line + season)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Articles]\n\n\n--------------\n-- Currency --\n--------------\n-- drop EXTERNAL TABLE [dim].[Currency]\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idCurrency) AS idSkCurrency,\n\tidCurrency,\n\tname,\n\tcurrency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idCurrency AS NVARCHAR) + name + currency)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Currency]\n\n\n----------\n-- Date --\n----------\n-- drop EXTERNAL TABLE [dim].[Date]\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n    HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n    + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Date]\n\n\n-----------\n-- Hours --\n-----------\n-- drop EXTERNAL TABLE [dim].[Hours]\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Hours]\n\n\n-------------------\n-- OperationType --\n-------------------\n-- drop EXTERNAL TABLE [dim].[OperationType]\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_OperationType]\n\n\n-----------------\n-- PostalCodes --\n-----------------\n-- drop EXTERNAL TABLE [dim].[PostalCodes]\n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idPostalCodes, \n\t'D' \t\t AS postalCode,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_PostalCodes]\n\n\n------------\n-- Tariff --\n------------\n-- drop EXTERNAL TABLE [dim].[Tariff]\n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Tariff]\n\n\n---------------\n-- Warehouse --\n---------------\n-- drop EXTERNAL TABLE [dim].[Warehouse]\n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idWarehouse) AS idSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Warehouse]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load SCD2 Silver CETAs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SCD"
				},
				"content": {
					"query": "---------------\n-- Articles --\n---------------\n\nDROP EXTERNAL TABLE  scd.Articles\nGO\n\nCREATE EXTERNAL TABLE  scd.Articles \nWITH\n(\n\tLOCATION = 'silver/SCD/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Articles]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idArticles AS NVARCHAR)+ s.name + s.description COLLATE DATABASE_DEFAULT + s.externalcode + s.size + CAST(s.numSize AS NVARCHAR) + s.colour + s.category + s.codLine + s.line + s.season)) AS [$hash]\n    FROM etl.vw_dim_Articles s\n    LEFT JOIN current_data c\n        ON s.idArticles = c.idArticles\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkArticles), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Articles]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idArticles) + max_key AS new_idSkArticles, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idArticles = c.idArticles\n    CROSS JOIN max_surrogate_key\n    WHERE c.idArticles IS NULL  OR (c.[$hash] != chg.[$hash] )\n), --select * from new_or_updated_data order by idarticles\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkarticles AS new_idSkArticles,\n        c.idArticles,\n        c.name,\n        c.externalCode,\n        c.size,\n        c.numSize,\n        c.colour,\n        c.category,\n        c.codLine,\n        c.line,\n        c.description,\n        c.season,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idArticles = chg.idArticles\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkArticles, -- Mantener la clave subrogada original\n    c.idArticles,\n    c.name,\n    c.externalCode,\n    c.size,\n    c.numSize,\n    c.colour,\n    c.category,\n    c.codLine,\n    c.line,\n    c.description,\n    c.season,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idArticles = chg.idArticles\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkArticles;\nGO\n\n---------------\n-- Currency --\n---------------\n\n\nDROP EXTERNAL TABLE  scd.Currency \nGO\n\nCREATE EXTERNAL TABLE  scd.Currency \nWITH\n(\n\tLOCATION = 'silver/SCD/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Currency]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idCurrency AS NVARCHAR) + s.name + s.currency)) AS [$hash]\n    FROM etl.vw_dim_Currency s\n    LEFT JOIN current_data c\n        ON s.idCurrency = c.idCurrency\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkCurrency), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Currency]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idCurrency) + max_key AS new_idSkCurrency, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idCurrency = c.idCurrency\n    CROSS JOIN max_surrogate_key\n    WHERE c.idCurrency IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkCurrency AS new_idSkCurrency,\n        c.idCurrency,\n        c.name,\n        c.currency,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idCurrency = chg.idCurrency\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkCurrency, -- Mantener la clave subrogada original\n    c.idCurrency,\n    c.name,\n    c.currency,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idCurrency = chg.idCurrency\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkCurrency;\nGO\n\n\n------------\n-- Client --\n------------\n-- DROP EXTERNAL TABLE  scd.Client \n-- GO\n\n-- CREATE EXTERNAL TABLE  scd.Client \n-- WITH\n-- (\n-- \tLOCATION = 'silver/SCD/Client', \n-- \tDATA_SOURCE = datalake1pgc,\n-- \tFILE_FORMAT = ParquetFileFormat\n-- ) AS \n-- WITH current_data AS (\n--     SELECT *\n--     FROM [GoldenlessDWH].[dim].[Currency]\n--     WHERE isCurrent = 1\n-- ),\n-- -- Identificar los registros que han cambiado o son nuevos\n-- changes AS (\n--     SELECT\n--         s.*,\n--         HASHBYTES('SHA2_256',(CAST(s.idCurrency AS NVARCHAR) + s.name + s.currency)) AS [$hash]\n--     FROM etl.vw_dim_Currency s\n--     LEFT JOIN current_data c\n--         ON s.idCurrency = c.idCurrency\n-- ),\n-- -- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\n-- max_surrogate_key AS (\n--     SELECT COALESCE(MAX(idSkCurrency), 0) AS max_key\n--     FROM [GoldenlessDWH].[dim].[Currency]\n-- ),\n-- -- Calcular claves subrogadas para los registros nuevos o modificados\n-- new_or_updated_data AS (\n--     SELECT\n--         ROW_NUMBER() OVER (ORDER BY chg.idCurrency) + max_key AS new_idSkCurrency, -- Asignar nueva clave subrogada\n--         chg.*,\n--         1 AS isCurrent,\n--         GETDATE() AS fromDate,\n--         '9999-12-31' AS toDate\n--     FROM changes chg\n--     LEFT JOIN current_data c\n--         ON chg.idCurrency = c.idCurrency\n--     CROSS JOIN max_surrogate_key\n--     WHERE c.idCurrency IS NULL  OR (c.[$hash] != chg.[$hash] )\n-- ),\n-- -- Obtener los registros que no han cambiado (mantener clave subrogada)\n-- unchanged_data AS (\n--     SELECT\n--         c.idSkCurrency AS new_idSkCurrency,\n--         c.idCurrency,\n--         c.name,\n--         c.currency,\n--         c.fromDate,\n--         c.toDate,\n--         c.isCurrent,\n--         c.loadDate,\n--         c.deltaDate,\n--         c.[$hash]\n--     FROM current_data c\n--     JOIN changes chg\n--         ON c.idCurrency = chg.idCurrency\n--     -- Seleccionar solo las filas que no han cambiado\n--     WHERE c.[$hash] = chg.[$hash]\n-- ),\n-- -- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\n-- final AS (\n-- SELECT\n--     c.idSkCurrency, -- Mantener la clave subrogada original\n--     c.idCurrency,\n--     c.name,\n--     c.currency,\n--     c.fromDate,\n--     GETDATE() AS toDate,\n--     0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n--     c.loadDate,\n--     c.deltaDate,\n--     c.[$hash]\n-- FROM current_data c\n-- JOIN changes chg\n--     ON c.idCurrency = chg.idCurrency\n-- WHERE c.[$hash] != chg.[$hash]\n\n-- UNION ALL\n\n-- -- Insertar los registros nuevos o modificados con nuevas claves subrogadas\n-- SELECT\n--     new_idSkCurrency, \n--     idCurrency,\n--     name,\n--     currency,\n--     fromDate,\n--     toDate,\n--     isCurrent,\n--     loadDate,\n--     deltaDate,\n--     [$hash]\n-- FROM new_or_updated_data\n\n-- UNION ALL\n\n-- -- Insertar los registros que no han cambiado\n-- SELECT\n--     new_idSkCurrency, \n--     idCurrency,\n--     name,\n--     currency,\n--     fromDate,\n--     toDate,\n--     isCurrent,\n--     loadDate,\n--     deltaDate,\n--     [$hash]\n-- FROM unchanged_data\n-- )\n-- SELECT     \n--     * \n-- FROM final\n-- ORDER BY idSkCurrency;\n-- GO\n\n----------\n-- Date --\n----------\n\nGO\n\nDROP EXTERNAL TABLE  scd.Date \n\nGO\n\nCREATE EXTERNAL TABLE  scd.Date \nWITH\n(\n\tLOCATION = 'silver/SCD/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Date]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idDate AS NVARCHAR) + CAST(s.dateKey AS NVARCHAR) + CAST(s.date AS NVARCHAR) + s.month COLLATE DATABASE_DEFAULT + CAST(s.monthNumber AS NVARCHAR) + CAST(s.year AS NVARCHAR) \n        + CAST(s.weekNumber AS NVARCHAR) + s.dayWeek COLLATE DATABASE_DEFAULT + CAST(s.dayWeekNumber AS NVARCHAR) + CAST(s.yearDay AS NVARCHAR) + s.quarter + s.quadrimester + s.semester )) AS [$hash]\n    FROM etl.vw_dim_Date s\n    LEFT JOIN current_data c\n        ON s.idDate = c.idDate\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkDate), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Date]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idDate) + max_key AS new_idSkDate, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idDate = c.idDate\n    CROSS JOIN max_surrogate_key\n    WHERE c.idDate IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkDate AS new_idSkDate,\n        c.idDate,\n        c.dateKey,\n        c.date,\n        c.month,\n        c.monthNumber,\n        c.year,\n        c.weekNumber,\n        c.dayWeek,\n        c.dayWeekNumber,\n        c.yearDay,\n        c.quarter,\n        c.quadrimester,\n        c.semester,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idDate = chg.idDate\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkDate, -- Mantener la clave subrogada original\n    c.idDate,\n\tc.dateKey,\n\tc.date,\n\tc.month,\n\tc.monthNumber,\n\tc.year,\n\tc.weekNumber,\n\tc.dayWeek,\n\tc.dayWeekNumber,\n\tc.yearDay,\n\tc.quarter,\n\tc.quadrimester,\n\tc.semester,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idDate = chg.idDate\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkDate;\nGO\n\n\n---------------\n-- Hours --\n---------------\nDROP EXTERNAL TABLE  scd.Hours \nGO\n\nCREATE EXTERNAL TABLE  scd.Hours \nWITH\n(\n\tLOCATION = 'silver/SCD/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Hours]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idHours AS NVARCHAR) + CAST(s.hour AS NVARCHAR) + CAST(s.minute AS NVARCHAR) + s.fullHour)) AS [$hash]\n    FROM etl.vw_dim_Hours s\n    LEFT JOIN current_data c\n        ON s.idHours = c.idHours\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkHours), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Hours]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idHours) + max_key AS new_idSkHours, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idHours = c.idHours\n    CROSS JOIN max_surrogate_key\n    WHERE c.idHours IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkHours AS new_idSkHours,\n        c.idHours,\n        c.hour,\n        c.minute,\n        c.fullHour,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idHours = chg.idHours\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkHours, -- Mantener la clave subrogada original\n\tc.idHours,\n\tc.hour,\n\tc.minute,\n\tc.fullHour,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idHours = chg.idHours\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkHours;\nGO\n\n\n-------------------\n-- OperationType --\n-------------------\n\nGO\n\nDROP EXTERNAL TABLE  scd.OperationType \n\nGO\n\nCREATE EXTERNAL TABLE  scd.OperationType \nWITH\n(\n\tLOCATION = 'silver/SCD/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[OperationType]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idOperationType AS NVARCHAR) + s.operation)) AS [$hash]\n    FROM etl.vw_dim_OperationType s\n    LEFT JOIN current_data c\n        ON s.idOperationType = c.idOperationType\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkOperationType), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[OperationType]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idOperationType) + max_key AS new_idSkOperationType, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idOperationType = c.idOperationType\n    CROSS JOIN max_surrogate_key\n    WHERE c.idOperationType IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkOperationType AS new_idSkOperationType,\n        c.idOperationType,\n        c.operation,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idOperationType = chg.idOperationType\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkOperationType, -- Mantener la clave subrogada original\n    c.idOperationType,\n    c.operation,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idOperationType = chg.idOperationType\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkOperationType,\n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkOperationType, \n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkOperationType;\nGO\n\n\n-----------------\n-- PostalCodes --\n-----------------\n\nGO\n\nDROP EXTERNAL TABLE  scd.PostalCodes \n\nGO\n\nCREATE EXTERNAL TABLE  scd.PostalCodes \nWITH\n(\n\tLOCATION = 'silver/SCD/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idPostalCodes AS NVARCHAR) + s.postalCode + s.region + s.countryCode + s.country)) AS [$hash]\n    FROM etl.vw_dim_PostalCodes s\n    LEFT JOIN current_data c\n        ON s.idPostalCodes = c.idPostalCodes\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkPostalCodes), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idPostalCodes) + max_key AS new_idSkPostalCodes, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idPostalCodes = c.idPostalCodes\n    CROSS JOIN max_surrogate_key\n    WHERE c.idPostalCodes IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkPostalCodes AS new_idSkPostalCodes,\n        c.idPostalCodes,\n        c.postalCode,\n        c.region,\n        c.countryCode,\n        c.country,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idPostalCodes = chg.idPostalCodes\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkPostalCodes, -- Mantener la clave subrogada original\n    c.idPostalCodes,\n    c.postalCode,\n    c.region,\n    c.countryCode,\n    c.country,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idPostalCodes = chg.idPostalCodes\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkPostalCodes,\n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkPostalCodes, \n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkPostalCodes;\nGO\n\n\n------------\n-- Tariff --\n------------\n\nGO\n\nDROP EXTERNAL TABLE  scd.Tariff \n\nGO\n\nCREATE EXTERNAL TABLE  scd.Tariff \nWITH\n(\n\tLOCATION = 'silver/SCD/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Tariff]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idTariff AS NVARCHAR) + s.tariff)) AS [$hash]\n    FROM etl.vw_dim_Tariff s\n    LEFT JOIN current_data c\n        ON s.idTariff = c.idTariff\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkTariff), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Tariff]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idTariff) + max_key AS new_idSkTariff, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idTariff = c.idTariff\n    CROSS JOIN max_surrogate_key\n    WHERE c.idTariff IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkTariff AS new_idSkTariff,\n        c.idTariff,\n        c.tariff,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idTariff = chg.idTariff\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkTariff, -- Mantener la clave subrogada original\n    c.idTariff,\n    c.tariff,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idTariff = chg.idTariff\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkTariff;\nGO\n\n\n---------------\n-- Warehouse --\n---------------\n\nGO\n\nDROP EXTERNAL TABLE  scd.Warehouse \n\nGO\n\nCREATE EXTERNAL TABLE  scd.Warehouse \nWITH\n(\n\tLOCATION = 'silver/SCD/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.idWarehouse,\n        s.warehouse,\n        s.externalCode,\n        s.countryCode,\n        s.country,\n        s.city,\n        s.address,\n        s.description,\n        s.loadDate,\n        s.deltaDate,\n        HASHBYTES('SHA2_256',(CAST(s.idWarehouse AS NVARCHAR) + s.warehouse + s.externalcode + s.countryCode + s.country + s.city + s.address + s.description)) AS [$hash]\n    FROM etl.vw_dim_Warehouse s\n    LEFT JOIN current_data c\n        ON s.idWarehouse = c.idWarehouse\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkWarehouse), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idWarehouse) + max_key AS new_idSkWarehouse, -- Asignar nueva clave subrogada\n        chg.idWarehouse,\n        chg.warehouse,\n        chg.externalCode,\n        chg.countryCode,\n        chg.country,\n        chg.city,\n        chg.address,\n        chg.description,\n        chg.loadDate,\n        chg.deltaDate,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate,\n        chg.[$hash]\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idWarehouse = c.idWarehouse\n    CROSS JOIN max_surrogate_key\n    WHERE c.idWarehouse IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkWarehouse AS new_idSkWarehouse,\n        c.idWarehouse,\n        c.warehouse,\n        c.externalCode,\n        c.countryCode,\n        c.country,\n        c.city,\n        c.address,\n        c.description,\n        c.loadDate,\n        c.deltaDate,\n        c.isCurrent,\n        c.fromDate,\n        c.toDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idWarehouse = chg.idWarehouse\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\nfinal AS (\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nSELECT\n    c.idSkWarehouse, -- Mantener la clave subrogada original\n    c.idWarehouse,\n    c.warehouse,\n    c.externalCode,\n    c.countryCode,\n    c.country,\n    c.city,\n    c.address,\n    c.description,\n    c.loadDate,\n    c.deltaDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.fromDate,\n    GETDATE() AS toDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idWarehouse = chg.idWarehouse\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM unchanged_data\n\n)\nselect     idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM final\nORDER BY idSkWarehouse;\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Re-Load Gold CETAs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SCD"
				},
				"content": {
					"query": "---------------\n-- Articles  --\n---------------\nDROP EXTERNAL TABLE [dim].[Articles]\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash]\nUNION\nSELECT *\nFROM [SilverlessSTG].[scd].[Articles]\n) a ORDER BY idSkArticles\n\n\n--------------\n-- Currency --\n--------------\nDROP EXTERNAL TABLE [dim].[Currency]\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2)  AS toDate,\t\t\n\t1 \t\t\t AS isCurrent, \t\t \t\t\t \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Currency]\n) C ORDER BY idSkCurrency\n\n\n----------\n-- Date --\n----------\nDROP EXTERNAL TABLE [dim].[Date]\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n        HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n        + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Date]\n) d ORDER BY idSkDate\n\n-----------\n-- Hours --\n-----------\nDROP EXTERNAL TABLE [dim].[Hours]\n\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Hours]\n) h ORDER BY idSkHours\n\n-------------------\n-- OperationType --\n-------------------\nDROP EXTERNAL TABLE [dim].[OperationType]\nGO\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_OperationType]\n) o ORDER BY idSkOperationType\n\n-----------------\n-- PostalCodes --\n-----------------\nDROP EXTERNAL TABLE [dim].[PostalCodes]\n\n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idpostalCode, \n\t'D' \t\t AS postalCodes,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_PostalCodes]\n) p ORDER BY idSkPostalCodes\n\n------------\n-- Tariff --\n------------\nDROP EXTERNAL TABLE [dim].[Tariff]\n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Tariff]\n) T ORDER BY idSkTariff\n\n---------------\n-- Warehouse --\n---------------\nDROP EXTERNAL TABLE [dim].[Warehouse]\n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM ( \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tidSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\tisCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Warehouse]\n) W ORDER BY idSkWarehouse",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/old_SCD2 Load Silver Warehouse CETAs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- CREATE EXTERNAL TABLE staging_scd2 \n-- WITH\n-- (\n-- \tLOCATION = 'gold/SCD/Warehouse', \n-- \tDATA_SOURCE = datalake1pgc,\n-- \tFILE_FORMAT = ParquetFileFormat\n-- ) AS \nWITH current_data AS (\n    -- Seleccionar solo las filas actuales\n    SELECT *\n    FROM SilverlessSTG.[dim].[Warehouse]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.idWarehouse,\n        s.warehouse,\n        s.externalCode,\n\t\ts.countryCode,\n\t\ts.country,\n\t\ts.city,\n\t\ts.address,\n\t\ts.description,\n\t\ts.loadDate,\n\t\ts.deltaDate,\n        c.idWarehouse AS current_idWarehouse,\n        c.warehouse AS current_warehous,\n        c.externalCode AS current_externalCode,\n\t\tc.countryCode AS current_countryCode,\n\t\tc.country AS current_country,\n\t\tc.city AS current_city,\n\t\tc.address AS current_address,\n\t\tc.description AS current_description,\n\t\tc.loadDate AS current_loadDate,\n\t\tc.deltaDate AS current_deltaDate\n    FROM etl.vw_dim_Warehouse s\n    LEFT JOIN current_data c\n        ON s.idWarehouse = c.idWarehouse\n) --select * from CHANGES\n-- CTAS para generar las versiones antiguas (historical records)\n\n,final AS (\nSELECT\n\tc.idWarehouse,\n\tc.warehouse,\n\tc.externalCode,\n\tc.countryCode,\n\tc.country,\n\tc.city,\n\tc.address,\n\tc.description,\n\tc.loadDate,\n\tc.deltaDate,  \n\tc.fromDate,\n    GETDATE() AS toDate, -- Marcar la fecha de fin para las versiones anteriores\n    0 AS isCurrent -- Ya no son actuales\nFROM current_data c\nJOIN changes chg\n    ON c.idWarehouse = chg.idWarehouse\n    -- Identificar cuando cambiÃ³ algÃºn valor\n    WHERE (c.warehouse != chg.warehouse OR c.externalCode != chg.externalCode OR c.countryCode != chg.countryCode OR c.country != chg.country\n\t\t\tOR c.city != chg.city OR c.address != chg.address OR c.description != chg.description)\n\nUNION ALL\n\n-- Insertar las nuevas versiones o los registros que no existÃ­an antes\nSELECT\n\tchg.idWarehouse,\n\tchg.warehouse,\n\tchg.externalCode,\n\tchg.countryCode,\n\tchg.country,\n\tchg.city,\n\tchg.address,\n\tchg.description,\n\tchg.loadDate,\n\tchg.deltaDate,\n    GETDATE() AS fromDate,\n    '9999-12-31' AS toDate, -- Para los registros actuales, se deja abierto\n    1 AS isCurrent\nFROM changes chg\nLEFT JOIN current_data c\n    ON chg.idWarehouse = c.idWarehouse\nWHERE c.idWarehouse IS NULL OR   (c.warehouse != chg.warehouse OR c.externalCode != chg.externalCode OR c.countryCode != chg.countryCode OR c.country != chg.country\n\t\t\tOR c.city != chg.city OR c.address != chg.address OR c.description != chg.description)\n\n\nUNION ALL\n\n-- Insertar las filas que no han cambiado (mantenerlas con isCurrent = 1)\nSELECT\n\t--ROW_NUMBER () OVER (ORDER BY c.idWarehouse, fromDate) idSkWarehouse,\n    c.idWarehouse,\n    c.warehouse,\n    c.externalCode,\n    c.countryCode,\n    c.country,\n    c.city,\n    c.address,\n    c.description,\n    c.loadDate,\n    c.deltaDate,\n    c.fromDate,\n    c.toDate, -- Mantener su toDate abierto (9999-12-31)\n    c.isCurrent -- Mantener isCurrent = 1\nFROM current_data c\nJOIN changes chg\n    ON c.idWarehouse = chg.idWarehouse\n-- Seleccionar solo las filas que no han cambiado\nWHERE (c.warehouse = chg.warehouse AND c.externalCode = chg.externalCode AND c.countryCode = chg.countryCode AND c.country = chg.country\n        AND c.city = chg.city AND c.address = chg.address AND c.description = chg.description)\n)\n\nselect ROW_NUMBER () OVER (ORDER BY idWarehouse,fromDate desc) idSkWarehouse,\n* from final c;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MergeLandingFilesToSilver')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c3de3cf3-3147-4236-a81f-46c4ecdffd44"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# # path of the data lake container (bronze and silver for this example)\r\n",
							"# data_lake_container = 'abfss://mdw@datalake1pgc.dfs.core.windows.net'\r\n",
							"# # The ingestion folder where your parquet file are located\r\n",
							"# bronze_folder = 'bronze/Landing'\r\n",
							"# # The silver folder where your Delta Tables will be stored\r\n",
							"# silver_folder = 'silver'\r\n",
							"# # The name of the table\r\n",
							"# table_name = 'color'\r\n",
							"# # The wildcard filter used within the bronze folder to find files\r\n",
							"# source_wildcard = 'color*.parquet'\r\n",
							"# # A comma separated string of one or more key columns (for the merge)\r\n",
							"# key_columns_str = 'idColor'"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Carga de ficheros parquet Landing sobre tablas Delta capa Silver**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Import modules\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format\r\n",
							"from pyspark.sql import SparkSession"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Prueba::\r\n",
							"\r\n",
							"# Supongamos que tienes las variables necesarias\r\n",
							"# Fecha actual en formato yyyymmdd\r\n",
							"fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
							"#fecha_delta = '20241011'\r\n",
							"# Convert comma separated string with keys to array\r\n",
							"key_columns = key_columns_str.split(',')\r\n",
							"\r\n",
							"# Convert array with keys to where-clause for merge statement\r\n",
							"conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"# Determine path of source files from ingest layer\r\n",
							"source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"\r\n",
							"# Determine path of Delta Lake Table \r\n",
							"delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"\r\n",
							"# Read file(s) in spark data frame\r\n",
							"sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"\r\n",
							"# Check countRows > 0 - Stop Notebook \r\n",
							"# CREAR DELTA TABLE DE COLOR\r\n",
							"\r\n",
							"if sdf.count() == 0:\r\n",
							"    print(\"no procesar\")\r\n",
							"else:\r\n",
							"    # Eliminar las columnas que no se desean\r\n",
							"    sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")\r\n",
							"\r\n",
							"    # Check if the Delta Table exists\r\n",
							"    if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"        # Read the existing Delta Table\r\n",
							"        delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"\r\n",
							"        # Get the schema of the existing Delta table\r\n",
							"        existing_columns = delta_table.toDF().columns\r\n",
							"        \r\n",
							"        # AÃ±adir columna fechaDelta al DataFrame\r\n",
							"        sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))\r\n",
							"\r\n",
							"        # Crear el conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
							"        update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"        # Merge new data into existing table\r\n",
							"        delta_table.alias(\"existing\").merge(\r\n",
							"            source=sdf.alias(\"updates\"),\r\n",
							"            condition=\" AND \".join(conditions_list)\r\n",
							"        ).whenMatchedUpdate(\r\n",
							"            condition=\" OR \".join([f\"existing.{col} != updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')]),\r\n",
							"            set={\r\n",
							"                \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"                \"fechaDelta\": \"updates.fechaDelta\",  # Usar fechaDelta del DataFrame\r\n",
							"                **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"            }\r\n",
							"        ).whenNotMatchedInsert(\r\n",
							"            values={\r\n",
							"                \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"                \"fechaDelta\": fecha_delta,  # Establecer fechaDelta al insertar\r\n",
							"                **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"            }\r\n",
							"        ).execute()\r\n",
							"    else:\r\n",
							"        # Crear nueva tabla Delta con nuevos datos, incluyendo la columna fechaDelta\r\n",
							"        sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga del DataFrame\r\n",
							"        sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))  # Establecer fechaDelta al crear la tabla\r\n",
							"        sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"    spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Crear manualmente Delta Tables**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"\r\n",
							"# # Determine path of Delta Lake Table \r\n",
							"# delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"# #spark.sql(f'DROP TABLE default.color')\r\n",
							"# spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ### Copia:\r\n",
							"\r\n",
							"# # Supongamos que tienes las variables necesarias\r\n",
							"# # Fecha actual en formato yyyymmdd\r\n",
							"# fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
							"# #fecha_delta = '20241011'\r\n",
							"# # Convert comma separated string with keys to array\r\n",
							"# key_columns = key_columns_str.split(',')\r\n",
							"\r\n",
							"# # Convert array with keys to where-clause for merge statement\r\n",
							"# conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"# # Determine path of source files from ingest layer\r\n",
							"# source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"\r\n",
							"# # Determine path of Delta Lake Table \r\n",
							"# delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"\r\n",
							"# # Read file(s) in spark data frame\r\n",
							"# sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"\r\n",
							"# # Check countRows > 0 - Stop Notebook \r\n",
							"# # CREAR DELTA TABLE DE COLOR\r\n",
							"\r\n",
							"# # Eliminar las columnas que no se desean\r\n",
							"# sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")\r\n",
							"\r\n",
							"# # Check if the Delta Table exists\r\n",
							"# if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"#     # Read the existing Delta Table\r\n",
							"#     delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"\r\n",
							"#     # Get the schema of the existing Delta table\r\n",
							"#     existing_columns = delta_table.toDF().columns\r\n",
							"    \r\n",
							"#     # AÃ±adir columna fechaDelta al DataFrame\r\n",
							"#     sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))\r\n",
							"\r\n",
							"#     # Crear el conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
							"#     update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"#     # Merge new data into existing table\r\n",
							"#     delta_table.alias(\"existing\").merge(\r\n",
							"#         source=sdf.alias(\"updates\"),\r\n",
							"#         condition=\" AND \".join(conditions_list)\r\n",
							"#     ).whenMatchedUpdate(\r\n",
							"#         condition=\" OR \".join([f\"existing.{col} != updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')]),\r\n",
							"#         set={\r\n",
							"#             \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"#             \"fechaDelta\": \"updates.fechaDelta\",  # Usar fechaDelta del DataFrame\r\n",
							"#             **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"#         }\r\n",
							"#     ).whenNotMatchedInsert(\r\n",
							"#         values={\r\n",
							"#             \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"#             \"fechaDelta\": fecha_delta,  # Establecer fechaDelta al insertar\r\n",
							"#             **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"#         }\r\n",
							"#     ).execute()\r\n",
							"# else:\r\n",
							"#     # Crear nueva tabla Delta con nuevos datos, incluyendo la columna fechaDelta\r\n",
							"#     sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga del DataFrame\r\n",
							"#     sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))  # Establecer fechaDelta al crear la tabla\r\n",
							"#     sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"# spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Actualizar CSV Configuracion**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# cuenta =  'datalake1pgc'\r\n",
							"# contendor = 'mdw'\r\n",
							"# archivo = '/bronze/Configuration/ConfiguracionOrigenes.csv'\r\n",
							"\r\n",
							"# #ruta = 'abfss://%s@%s.dfs.core.windows.net/%s' % (contendor, cuenta, archivo)\r\n",
							"# ruta = f'abfss://{contendor}@{cuenta}.dfs.core.windows.net/{archivo}'\r\n",
							"# # Leer CSV\r\n",
							"# #print(ruta)\r\n",
							"# df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(ruta)\r\n",
							"# #Generamos fecha en formato 'yyyyMMdd'\r\n",
							"# fecha_actual = date_format(current_date(), 'yyyy-MM-dd')\r\n",
							"# fecha_actual = '2022-01-17'\r\n",
							"# # Modificar CSV\r\n",
							"# df_modificado = df.withColumn(\r\n",
							"#     \"UpdateDate\",\r\n",
							"#     when(df[\"Filename\"] == table_name, fecha_actual).otherwise(df[\"UpdateDate\"])\r\n",
							"# )\r\n",
							"# # Sobreescrir CSV\r\n",
							"# #df_modificado.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ruta)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df_modificado.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ruta)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkTFM')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "francecentral"
		}
	]
}