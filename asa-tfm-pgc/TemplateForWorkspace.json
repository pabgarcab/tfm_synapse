{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Nombre del Ã¡rea de trabajo",
			"defaultValue": "asa-tfm-pgc"
		},
		"asa-tfm-pgc-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"connectionString\"de \"asa-tfm-pgc-WorkspaceDefaultSqlServer\"",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:asa-tfm-pgc.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"ls_ADLG2_Bronze_Landing_accountKey": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"accountKey\"de \"ls_ADLG2_Bronze_Landing\""
		},
		"ls_COLI_ERP_password": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"password\"de \"ls_COLI_ERP\""
		},
		"asa-tfm-pgc-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalake1pgc.dfs.core.windows.net"
		},
		"ls_ADLG2_Bronze_Landing_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalake1pgc.dfs.core.windows.net/"
		},
		"ls_COLI_ERP_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "@{linkedService().NombreServidor}"
		},
		"ls_COLI_ERP_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "@{linkedService().NombreBaseDatos}"
		},
		"ls_COLI_ERP_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "sa"
		},
		"ls_Severless_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "asa-tfm-pgc-ondemand.sql.azuresynapse.net"
		},
		"ls_Severless_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "@{linkedService().DBName}"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/0 - Backup DWH')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Clean Backup Folder",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsSilverFact",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Clean Fact Gold Folder",
						"type": "Delete",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [
							{
								"activity": "Load Dim Backup",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsGoldFact",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Load Dim Backup",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "Clean Backup Folder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[silver].[FactSales]"
						},
						"linkedServiceName": {
							"referenceName": "ls_Severless",
							"type": "LinkedServiceReference",
							"parameters": {
								"DBName": "SilverlessSTG"
							}
						}
					},
					{
						"name": "Load Fact Backup",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "Clean Fact Gold Folder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[gold].[FactSales]"
						},
						"linkedServiceName": {
							"referenceName": "ls_Severless",
							"type": "LinkedServiceReference",
							"parameters": {
								"DBName": "GoldenlessDWH"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-14T15:15:28Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsSilverFact')]",
				"[concat(variables('workspaceId'), '/datasets/dsGoldFact')]",
				"[concat(variables('workspaceId'), '/linkedServices/ls_Severless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/1 - SqlToLanding')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "List of Tables to Load",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "Get CSV name",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"fileListPath": {
										"value": "@activity('Get CSV name').output",
										"type": "Expression"
									},
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "dsConfiguration",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Save Landing Files",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "List of Tables to Load",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('List of tables to load').output.value",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Active For Load",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@not(equals(trim(item().Load), 'N'))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Load Landing",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "SqlServerSource",
														"sqlReaderQuery": {
															"value": "SELECT *, @{formatDateTime(pipeline().TriggerTime,'yyyyMMdd')} AS fechaCarga, '@{pipeline().RunId}' AS pipelineID \nFROM @{item().SchemaName}.@{item().TableName} \nWHERE @{item().IncrementalColumn} >= CAST(CAST('@{item().UpdateDate}' AS nvarchar) AS datetime2);",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings",
															"copyBehavior": "FlattenHierarchy"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"parallelCopies": 1,
													"dataIntegrationUnits": 4,
													"translator": {
														"type": "TabularTranslator",
														"typeConversion": true,
														"typeConversionSettings": {
															"allowDataTruncation": true,
															"treatBooleanAsNumber": false
														}
													}
												},
												"inputs": [
													{
														"referenceName": "dsSQLGenerico",
														"type": "DatasetReference",
														"parameters": {
															"NombreServidor": "@item().ServerName",
															"NombreBD": "@item().DataBaseName",
															"NombreEsquema": "@item().SchemaName",
															"NombreTabla": "@item().TableName",
															"FechaActualizacion": "@item().UpdateDate",
															"ColumnaIncremental": {
																"value": "@item().IncrementalColumn",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "dsParquetRaw",
														"type": "DatasetReference",
														"parameters": {
															"NombreFichero": "@concat( item().FileName, '_', formatDateTime(utcnow(), 'yyyyMMdd'),'.', item().Format)"
														}
													}
												]
											},
											{
												"name": "Save Metadata",
												"type": "SqlServerStoredProcedure",
												"state": "Inactive",
												"onInactiveMarkAs": "Succeeded",
												"dependsOn": [
													{
														"activity": "Load Landing",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {}
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "Get CSV name",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsConfiguration",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "DelimitedTextReadSettings"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-16T00:20:34Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsConfiguration')]",
				"[concat(variables('workspaceId'), '/datasets/dsSQLGenerico')]",
				"[concat(variables('workspaceId'), '/datasets/dsParquetRaw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/2 - BronzeToSilver')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "UPSERT Silver",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Metadata Landing",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get Metadata Landing').output.childItems",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Clean Landing",
									"type": "Delete",
									"dependsOn": [
										{
											"activity": "LandingToProcessed",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataset": {
											"referenceName": "dsParquetRaw",
											"type": "DatasetReference",
											"parameters": {
												"NombreFichero": {
													"value": "@item().name",
													"type": "Expression"
												}
											}
										},
										"enableLogging": false,
										"storeSettings": {
											"type": "AzureBlobFSReadSettings",
											"recursive": false,
											"enablePartitionDiscovery": false
										}
									}
								},
								{
									"name": "LandingToProcessed",
									"type": "Copy",
									"dependsOn": [
										{
											"activity": "MergeBronzeToSilver",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "ParquetSource",
											"storeSettings": {
												"type": "AzureBlobFSReadSettings",
												"recursive": false,
												"enablePartitionDiscovery": false
											},
											"formatSettings": {
												"type": "ParquetReadSettings"
											}
										},
										"sink": {
											"type": "ParquetSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "ParquetWriteSettings"
											}
										},
										"enableStaging": false,
										"parallelCopies": 1,
										"dataIntegrationUnits": 4,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "dsParquetRaw",
											"type": "DatasetReference",
											"parameters": {
												"NombreFichero": {
													"value": "@item().name",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "dsParquetProcessed",
											"type": "DatasetReference",
											"parameters": {
												"NombreFichero": {
													"value": "@item().name",
													"type": "Expression"
												},
												"NombreCarpeta": {
													"value": "@concat(\n    substring(item().name, 0, indexOf(item().name, '_')), \n    '/', \n    substring(item().name, add(indexOf(item().name, '_'), 1), 4), \n    '/', \n    substring(item().name, add(indexOf(item().name, '_'), 5), 2), \n    '/'\n)\n",
													"type": "Expression"
												}
											}
										}
									]
								},
								{
									"name": "MergeBronzeToSilver",
									"type": "SynapseNotebook",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebook": {
											"referenceName": "MergeLandingFilesToSilver",
											"type": "NotebookReference"
										},
										"parameters": {
											"data_lake_container": {
												"value": {
													"value": "@pipeline().parameters.ADLG2",
													"type": "Expression"
												},
												"type": "string"
											},
											"bronze_folder": {
												"value": {
													"value": "@pipeline().parameters.BronzeFolder",
													"type": "Expression"
												},
												"type": "string"
											},
											"table_name": {
												"value": {
													"value": "@substring(item().name, 0, indexOf(item().name, '_'))",
													"type": "Expression"
												},
												"type": "string"
											},
											"silver_folder": {
												"value": {
													"value": "@pipeline().parameters.SilverFolder",
													"type": "Expression"
												},
												"type": "string"
											},
											"source_wildcard": {
												"value": {
													"value": "@concat(substring(item().name, 0, indexOf(item().name, '_')), '*.parquet')",
													"type": "Expression"
												},
												"type": "string"
											},
											"key_columns_str": {
												"value": {
													"value": "@concat('id',substring(item().name, 0, indexOf(item().name, '_')))",
													"type": "Expression"
												},
												"type": "string"
											}
										},
										"snapshot": true,
										"sparkPool": {
											"referenceName": "sparkTFM",
											"type": "BigDataPoolReference"
										},
										"executorSize": "Small",
										"conf": {
											"spark.dynamicAllocation.enabled": false,
											"spark.dynamicAllocation.minExecutors": 2,
											"spark.dynamicAllocation.maxExecutors": 2
										},
										"driverSize": "Small",
										"numExecutors": 2
									}
								},
								{
									"name": "PendingToError",
									"type": "Copy",
									"dependsOn": [
										{
											"activity": "MergeBronzeToSilver",
											"dependencyConditions": [
												"Failed"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "ParquetSource",
											"storeSettings": {
												"type": "AzureBlobFSReadSettings",
												"recursive": false,
												"enablePartitionDiscovery": false
											},
											"formatSettings": {
												"type": "ParquetReadSettings"
											}
										},
										"sink": {
											"type": "ParquetSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "ParquetWriteSettings"
											}
										},
										"enableStaging": false,
										"parallelCopies": 1,
										"dataIntegrationUnits": 4,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "dsParquetRaw",
											"type": "DatasetReference",
											"parameters": {
												"NombreFichero": "@item().name"
											}
										}
									],
									"outputs": [
										{
											"referenceName": "dsParquetError",
											"type": "DatasetReference",
											"parameters": {
												"NombreFichero": "@item().name",
												"NombreCarpeta": "@concat(     substring(item().name, 0, indexOf(item().name, '_')),      '/',      substring(item().name, add(indexOf(item().name, '_'), 1), 4),      '/',      substring(item().name, add(indexOf(item().name, '_'), 5), 2),      '/' )"
											}
										}
									]
								}
							]
						}
					},
					{
						"name": "Get Metadata Landing",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsRawEntidades",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "ParquetReadSettings"
							}
						}
					},
					{
						"name": "Delete SUCCESS File",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "UPSERT Silver",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsConfiguration",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"wildcardFileName": {
									"value": "@variables('ficheroEliminar')",
									"type": "Expression"
								},
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"ADLG2": {
						"type": "string",
						"defaultValue": "abfss://mdw@datalake1pgc.dfs.core.windows.net"
					},
					"BronzeFolder": {
						"type": "string",
						"defaultValue": "bronze/Landing/Pending"
					},
					"SilverFolder": {
						"type": "string",
						"defaultValue": "silver/DeltaSTG"
					}
				},
				"variables": {
					"ficheroEliminar": {
						"type": "String",
						"defaultValue": "_SUCCESS"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsRawEntidades')]",
				"[concat(variables('workspaceId'), '/datasets/dsConfiguration')]",
				"[concat(variables('workspaceId'), '/datasets/dsParquetRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsParquetProcessed')]",
				"[concat(variables('workspaceId'), '/notebooks/MergeLandingFilesToSilver')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkTFM')]",
				"[concat(variables('workspaceId'), '/datasets/dsParquetError')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/3 - SilverToSCD2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Delete Dimensions Folders",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Load SCD Dimensions",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsGoldDimensions",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Delete SCD2 Folders",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsSilverSCD",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Load SCD Dimensions",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "Delete SCD2 Folders",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[silver].[DimSCD]"
						},
						"linkedServiceName": {
							"referenceName": "ls_Severless",
							"type": "LinkedServiceReference",
							"parameters": {
								"DBName": "SilverlessSTG"
							}
						}
					},
					{
						"name": "Load Gold Dimensions",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "Delete Dimensions Folders",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[gold].[ReloadDimSCD]"
						},
						"linkedServiceName": {
							"referenceName": "ls_Severless",
							"type": "LinkedServiceReference",
							"parameters": {
								"DBName": "GoldenlessDWH"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-14T15:15:28Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsGoldDimensions')]",
				"[concat(variables('workspaceId'), '/datasets/dsSilverSCD')]",
				"[concat(variables('workspaceId'), '/linkedServices/ls_Severless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/4 - SilverToFact')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Clean Fact Silver Folder",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsSilverFact",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Clean Fact Gold Folder",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Load Silver Fact",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsGoldFact",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Load Silver Fact",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "Clean Fact Silver Folder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[silver].[FactSales]"
						},
						"linkedServiceName": {
							"referenceName": "ls_Severless",
							"type": "LinkedServiceReference",
							"parameters": {
								"DBName": "SilverlessSTG"
							}
						}
					},
					{
						"name": "Load Gold Fact",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "Clean Fact Gold Folder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[gold].[FactSales]"
						},
						"linkedServiceName": {
							"referenceName": "ls_Severless",
							"type": "LinkedServiceReference",
							"parameters": {
								"DBName": "GoldenlessDWH"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-14T15:15:28Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsSilverFact')]",
				"[concat(variables('workspaceId'), '/datasets/dsGoldFact')]",
				"[concat(variables('workspaceId'), '/linkedServices/ls_Severless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Orchestrator')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "1",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "0",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "1 - SqlToLanding",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "3",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "2",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "3 - SilverToSCD2",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "4",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "3",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "4 - SilverToFact",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "2",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "2 - BronzeToSilver",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "0",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "0 - Backup DWH",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-15T16:46:15Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/1 - SqlToLanding')]",
				"[concat(variables('workspaceId'), '/pipelines/3 - SilverToSCD2')]",
				"[concat(variables('workspaceId'), '/pipelines/4 - SilverToFact')]",
				"[concat(variables('workspaceId'), '/pipelines/2 - BronzeToSilver')]",
				"[concat(variables('workspaceId'), '/pipelines/0 - Backup DWH')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvConfiguration')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreCSV": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreCSV",
							"type": "Expression"
						},
						"folderPath": "bronze/Configuration",
						"fileSystem": "mdw"
					},
					"columnDelimiter": ";",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ServerName;DataBaseName;SchemaName;TableName;PathName;FileName;Load",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsConfiguration')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "bronze/Configuration",
						"fileSystem": "mdw"
					},
					"columnDelimiter": ";",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ServerName;DataBaseName;SchemaName;TableName;PathName;FileName;Load",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsGoldDimensions')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "gold/Dimensions",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsGoldFact')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "gold/Fact",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetError')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					},
					"NombreCarpeta": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@concat('bronze/Error/', dataset().NombreCarpeta)",
							"type": "Expression"
						},
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetProcessed')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					},
					"NombreCarpeta": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@concat('bronze/Processed/', dataset().NombreCarpeta)",
							"type": "Expression"
						},
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetRaw')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": "bronze/Landing/Pending",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsRawEntidades')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "bronze/Landing/Pending",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSQLGenerico')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_COLI_ERP",
					"type": "LinkedServiceReference",
					"parameters": {
						"NombreServidor": {
							"value": "@dataset().NombreServidor",
							"type": "Expression"
						},
						"NombreBaseDatos": {
							"value": "@dataset().NombreBD",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"NombreServidor": {
						"type": "string"
					},
					"NombreBD": {
						"type": "string"
					},
					"NombreEsquema": {
						"type": "string"
					},
					"NombreTabla": {
						"type": "string"
					},
					"FechaActualizacion": {
						"type": "string"
					},
					"ColumnaIncremental": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServerTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().NombreEsquema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().NombreTabla",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_COLI_ERP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSilverFact')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "silver/Fact",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSilverSCD')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "silver/SCD",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-tfm-pgc-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('asa-tfm-pgc-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-tfm-pgc-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('asa-tfm-pgc-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_ADLG2_Bronze_Landing')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ls_ADLG2_Bronze_Landing_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('ls_ADLG2_Bronze_Landing_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_COLI_ERP')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"NombreServidor": {
						"type": "string"
					},
					"NombreBaseDatos": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServer",
				"typeProperties": {
					"server": "[parameters('ls_COLI_ERP_properties_typeProperties_server')]",
					"database": "[parameters('ls_COLI_ERP_properties_typeProperties_database')]",
					"encrypt": "mandatory",
					"trustServerCertificate": true,
					"authenticationType": "SQL",
					"userName": "[parameters('ls_COLI_ERP_properties_typeProperties_userName')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('ls_COLI_ERP_password')]"
					}
				},
				"connectVia": {
					"referenceName": "IR-Coli-ERP",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/IR-Coli-ERP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_Severless')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"server": "[parameters('ls_Severless_properties_typeProperties_server')]",
					"database": "[parameters('ls_Severless_properties_typeProperties_database')]",
					"encrypt": "mandatory",
					"trustServerCertificate": false,
					"authenticationType": "SystemAssignedManagedIdentity"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Carga Diaria')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Orchestrator",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 24,
						"startTime": "2024-10-16T04:00:00",
						"timeZone": "Romance Standard Time"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Orchestrator')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IR-Coli-ERP')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential1')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {
					"resourceId": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.ManagedIdentity/userAssignedIdentities/asa-tfm-pgc"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure BackuplessSTG')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Configuration"
				},
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity' \nGO\n\nCREATE USER [asa-tfm-pgc] FROM EXTERNAL PROVIDER;\nGO\nALTER ROLE db_datareader ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_datawriter ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_owner ADD MEMBER [asa-tfm-pgc];\nGO\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA bk AUTHORIZATION dbo\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "BackuplessDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure GoldelessSTG')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Configuration"
				},
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity' \nGO\nCREATE DATABASE SCOPED CREDENTIAL [https://datalake1pgc.dfs.core.windows.net] \nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'SAS Token';\nGO\nCREATE USER [asa-tfm-pgc] FROM EXTERNAL PROVIDER;\nGO\nALTER ROLE db_datareader ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_datawriter ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_owner ADD MEMBER [asa-tfm-pgc];\nGO\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA dim AUTHORIZATION dbo\nGO\nCREATE SCHEMA fact AUTHORIZATION dbo\nGO\nCREATE SCHEMA gold AUTHORIZATION dbo\n\nCREATE SCHEMA etl AUTHORIZATION dbo",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure SilverlessSTG')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Configuration"
				},
				"content": {
					"query": "--CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity'\nGO\n\n--CREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'User identity'\nGO\n\nSELECT * FROM sys.database_scoped_credentials;\n\n\n--GRANT ALTER ANY CREDENTIAL TO [dbo];\n\nGRANT CONTROL ON DATABASE::[SilverlessSTG]  TO [asa-tfm-pgc]\nGRANT CONTROL ON DATABASE::[SilverlessSTG]  TO [dbo]\n\n\n\n--GRANT CONTROL ON DATABASE SCOPED CREDENTIAL::[SilverlessSTG]  TO [asa-tfm-pgc]\n\nGO\nCREATE USER [asa-tfm-pgc] FROM EXTERNAL PROVIDER;\nGO\nALTER ROLE db_datareader ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_datawriter ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_owner ADD MEMBER [asa-tfm-pgc];\nGO\n\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\n\n\n\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA etl AUTHORIZATION dbo;\nGO\nCREATE SCHEMA silver AUTHORIZATION dbo;\nGO\nCREATE SCHEMA scd AUTHORIZATION dbo;\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Dimensions Gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Views Silver"
				},
				"content": {
					"query": "---------------\n-- Dim Date --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Date] AS \nSELECT  idFechas AS idDate,\n        ClaveFecha AS dateKey,\n        Fecha AS date,\n        COALESCE(Mes, 'D')  AS month,\n        COALESCE(NumeroMes, -1)  AS monthNumber,\n        COALESCE(Ano, -1)  AS year,\n        COALESCE(NumeroSemana, -1)  AS weekNumber,\n        COALESCE(DiaSemana, 'D') AS dayWeek,\n        COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\n        COALESCE(DiaAno, -1) AS yearDay,\n        CASE WHEN Trimestre = 1 THEN 'Q1'\n             WHEN Trimestre = 2 THEN 'Q2'\n             WHEN Trimestre = 3 THEN 'Q3'\n             ELSE '-1' END AS quarter,\n        CASE WHEN Cuatrimestre = 1 THEN 'Q1'\n             WHEN Cuatrimestre = 2 THEN 'Q2'\n             WHEN Cuatrimestre = 3 THEN 'Q3'\n             WHEN Cuatrimestre = 4 THEN 'Q4'\n             ELSE '-1' \n        END                                       AS quadrimester,\n        CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\n            WHEN Cuatrimestre IN (2,4) THEN 'S2'\n            ELSE '-1' \n        END                                       AS semester,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[fechas]\n\n\n------------------\n-- Dim Articles --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Articles] AS\nSELECT  idArticulos                                                                         AS idArticles,\n        COALESCE(nombre, 'D')                                                               AS name,\n        COALESCE(descripcion, 'D')                                                          AS description,\n        COALESCE(codigoReferencia, 'D')                                                     AS externalCode,\n        COALESCE(t.talla, 'D')                                                              AS size,\n        COALESCE( t.numeroTalla, -1)                                                        AS numSize,\n        COALESCE(co.color, 'D')                                                             AS colour,\n        COALESCE(ca.categoria, 'D')                                                         AS category,\n        COALESCE(l.codigoLinea, 'D')                                                        AS codLine,\n        COALESCE(l.Linea, 'D')                                                              AS line, \n        CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN 'OI'+CAST((a.idTemporada) AS nvarchar)\n             WHEN a.idCategoria IN (2,4,6,7,9) THEN 'OI'+CAST(a.idTemporada AS nvarchar)\n             ELSE 'PV'+CAST(a.idTemporada AS nvarchar)\n        END                                                                                 AS season,\n        a.fechaCarga                                                                        AS loadDate,\n        a.fechaDelta                                                                        AS deltaDate\nFROM [default].[dbo].[articulos] AS a\nLEFT JOIN [default].[dbo].[talla] AS t\n    ON t.idTalla = a.idTalla\nLEFT JOIN [default].[dbo].[color] AS co\n   ON co.idColor = a.idColor\nLEFT JOIN [default].[dbo].[categoria] AS ca -- select * from [default].[dbo].[categoria]\n    ON ca.idCategoria = a.idCategoria\nLEFT JOIN [default].[dbo].[Linea] AS l\n    ON l.idLinea = a.idLinea\n--where  a.fechaCarga  < '20241009'\n\n\n-------------------\n-- Dim Warehouse --\n-------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Warehouse] AS\nSELECT  idAlmacenes AS idWarehouse,\n        COALESCE(a.Nombre, 'D') AS warehouse,\n        COALESCE(a.codigoAlmacen, 'D') AS externalCode,\n        COALESCE(p.codigoPais, 'D') AS countryCode, \n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(a.ciudad, 'D') AS city,\n        COALESCE(a.Direccion, 'D') AS address,\n        COALESCE(a.descripcion, 'D') AS description,\n        a.fechaCarga AS loadDate,\n        a.fechaDelta AS deltaDate\nFROM [default].[dbo].[almacenes] AS a\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.idpais = a.idPais\n  -- WHERE a.fechaDelta <> 20241011\n--UNION \n--select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\n--UNION \n--select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\n\n\n---------------------\n-- Dim Postal Code --\n---------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_PostalCodes] AS\nSELECT  idCodigoPostal AS idPostalCodes,\n        COALESCE(codigoPostal, 'D') AS postalCode,\n        COALESCE(region, 'D') AS region,\n        COALESCE(c.codigoPais, 'D') AS countryCode,\n        COALESCE(nombre, 'D') AS country,\n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[codigoPostal] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.codigoPais = c.codigoPais\n\n------------------\n-- Dim Currency --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Currency] AS\nSELECT  idDivisa AS idCurrency,\n        COALESCE(nombre, 'D') AS name,\n        COALESCE(Divisa, 'D') AS currency,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[divisa]\n\n---------------\n-- Dim Hours --\n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Hours] AS\nSELECT  idHoras AS idHours,\n        COALESCE(Hora, -1) AS hour,\n        COALESCE(Minuto, -1) AS minute,\n        COALESCE(HoraCompleta, 'D') as fullHour,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[horas]\n\n\n----------------\n-- Dim Tariff --\n----------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Tariff] AS\nSELECT  idTarifa AS idTariff,\n        COALESCE(Tarifa, 'D') AS tariff,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate       \nFROM [default].[dbo].[tarifa]\n\n\n------------------------\n-- Dim Operation Type --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_OperationType] AS\nSELECT  idTipoOperacion AS idOperationType,\n        COALESCE(Operacion, 'D') AS operation,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[tipoOperacion]\n\n\n------------------------\n-- Dim Client --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Client] AS\nSELECT  idCliente AS idClient,\n        COALESCE(c.nombre, 'D') AS name,\n        COALESCE(apellido1, 'D') AS lastName1,\n        COALESCE(apellido2, 'D') AS lastName2, \n        COALESCE(email, 'D') AS email,\n        COALESCE(telefono, 'D') AS phoneNumber,\n        COALESCE(cumpleanos, '1900-01-01') AS birthDay,       \n        DATEDIFF(YEAR, cumpleanos, GETDATE()) \n        - CASE \n            WHEN MONTH(GETDATE()) < MONTH('1983-12-04') \n                 OR (MONTH(GETDATE()) = MONTH('1983-12-04') AND DAY(GETDATE()) < DAY('1983-12-04'))\n            THEN 1\n            ELSE 0\n        END AS age,\n        CASE WHEN hombre = 1 THEN 'Hombre'\n        ELSE 'Mujer' \n        END AS gender,\n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(p.codigoPais, 'D') AS countryCode,\n        COALESCE(cp.region, 'D') AS region,\n        COALESCE(c.direcion, 'D') AS address,\n        COALESCE(cp.codigoPostal, 'D') AS postalCode,\n        COALESCE(activo, 0) AS active,       \n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[cliente] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    On c.idPais = c.idPais\nINNER JOIN [default].[dbo].[codigoPostal] AS cp\n    ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Dimensions Silver')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Views Silver"
				},
				"content": {
					"query": "---------------\n-- Dim Date --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Date] AS \nSELECT  idFechas AS idDate,\n        ClaveFecha AS dateKey,\n        Fecha AS date,\n        COALESCE(Mes, 'D')  AS month,\n        COALESCE(NumeroMes, -1)  AS monthNumber,\n        COALESCE(Ano, -1)  AS year,\n        COALESCE(NumeroSemana, -1)  AS weekNumber,\n        COALESCE(DiaSemana, 'D') AS dayWeek,\n        COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\n        COALESCE(DiaAno, -1) AS yearDay,\n        CASE WHEN Trimestre = 1 THEN 'Q1'\n             WHEN Trimestre = 2 THEN 'Q2'\n             WHEN Trimestre = 3 THEN 'Q3'\n             ELSE '-1' END AS quarter,\n        CASE WHEN Cuatrimestre = 1 THEN 'Q1'\n             WHEN Cuatrimestre = 2 THEN 'Q2'\n             WHEN Cuatrimestre = 3 THEN 'Q3'\n             WHEN Cuatrimestre = 4 THEN 'Q4'\n             ELSE '-1' \n        END                                       AS quadrimester,\n        CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\n            WHEN Cuatrimestre IN (2,4) THEN 'S2'\n            ELSE '-1' \n        END                                       AS semester,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[fechas]\n\n\n------------------\n-- Dim Articles --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Articles] AS\nSELECT  idArticulos                                                                         AS idArticles,\n        COALESCE(nombre, 'D')                                                               AS name,\n        COALESCE(descripcion, 'D')                                                          AS description,\n        COALESCE(codigoReferencia, 'D')                                                     AS externalCode,\n        COALESCE(t.talla, 'D')                                                              AS size,\n        COALESCE( t.numeroTalla, -1)                                                        AS numSize,\n        COALESCE(co.color, 'D')                                                             AS colour,\n        COALESCE(ca.categoria, 'D')                                                         AS category,\n        COALESCE(l.codigoLinea, 'D')                                                        AS codLine,\n        COALESCE(l.Linea, 'D')                                                              AS line, \n        CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN 'OI'+CAST((a.idTemporada) AS nvarchar)\n             WHEN a.idCategoria IN (2,4,6,7,9) THEN 'OI'+CAST(a.idTemporada AS nvarchar)\n             ELSE 'PV'+CAST(a.idTemporada AS nvarchar)\n        END                                                                                 AS season,\n        a.fechaCarga                                                                        AS loadDate,\n        a.fechaDelta                                                                        AS deltaDate\nFROM [default].[dbo].[articulos] AS a\nLEFT JOIN [default].[dbo].[talla] AS t\n    ON t.idTalla = a.idTalla\nLEFT JOIN [default].[dbo].[color] AS co\n   ON co.idColor = a.idColor\nLEFT JOIN [default].[dbo].[categoria] AS ca -- select * from [default].[dbo].[categoria]\n    ON ca.idCategoria = a.idCategoria\nLEFT JOIN [default].[dbo].[Linea] AS l\n    ON l.idLinea = a.idLinea\n--where  a.fechaCarga  < '20241009'\n\n\n-------------------\n-- Dim Warehouse --\n-------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Warehouse] AS\nSELECT  idAlmacenes AS idWarehouse,\n        COALESCE(a.Nombre, 'D') AS warehouse,\n        COALESCE(a.codigoAlmacen, 'D') AS externalCode,\n        COALESCE(p.codigoPais, 'D') AS countryCode, \n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(a.ciudad, 'D') AS city,\n        COALESCE(a.Direccion, 'D') AS address,\n        COALESCE(a.descripcion, 'D') AS description,\n        a.fechaCarga AS loadDate,\n        a.fechaDelta AS deltaDate\nFROM [default].[dbo].[almacenes] AS a\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.idpais = a.idPais\n  -- WHERE a.fechaDelta <> 20241011\n--UNION \n--select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\n--UNION \n--select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\n\n\n---------------------\n-- Dim Postal Code --\n---------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_PostalCodes] AS\nSELECT  idCodigoPostal AS idPostalCodes,\n        COALESCE(codigoPostal, 'D') AS postalCode,\n        COALESCE(region, 'D') AS region,\n        COALESCE(c.codigoPais, 'D') AS countryCode,\n        COALESCE(nombre, 'D') AS country,\n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[codigoPostal] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.codigoPais = c.codigoPais\n\n------------------\n-- Dim Currency --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Currency] AS\nSELECT  idDivisa AS idCurrency,\n        COALESCE(nombre, 'D') AS name,\n        COALESCE(Divisa, 'D') AS currency,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[divisa]\n\n---------------\n-- Dim Hours --\n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Hours] AS\nSELECT  idHoras AS idHours,\n        COALESCE(Hora, -1) AS hour,\n        COALESCE(Minuto, -1) AS minute,\n        COALESCE(HoraCompleta, 'D') as fullHour,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[horas]\n\n\n----------------\n-- Dim Tariff --\n----------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Tariff] AS\nSELECT  idTarifa AS idTariff,\n        COALESCE(Tarifa, 'D') AS tariff,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate       \nFROM [default].[dbo].[tarifa]\n\n\n------------------------\n-- Dim Operation Type --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_OperationType] AS\nSELECT  idTipoOperacion AS idOperationType,\n        COALESCE(Operacion, 'D') AS operation,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[tipoOperacion]\n\n\n------------------------\n-- Dim Client --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Client] AS\nSELECT  idCliente AS idClient,\n        COALESCE(c.nombre, 'D') AS name,\n        COALESCE(apellido1, 'D') AS lastName1,\n        COALESCE(apellido2, 'D') AS lastName2, \n        COALESCE(email, 'D') AS email,\n        COALESCE(telefono, 'D') AS phoneNumber,\n        COALESCE(cumpleanos, '1900-01-01') AS birthDay,       \n        DATEDIFF(YEAR, cumpleanos, GETDATE()) \n        - CASE \n            WHEN MONTH(GETDATE()) < MONTH('1983-12-04') \n                 OR (MONTH(GETDATE()) = MONTH('1983-12-04') AND DAY(GETDATE()) < DAY('1983-12-04'))\n            THEN 1\n            ELSE 0\n        END AS age,\n        CASE WHEN hombre = 1 THEN 'Hombre'\n        ELSE 'Mujer' \n        END AS gender,\n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(p.codigoPais, 'D') AS countryCode,\n        COALESCE(cp.region, 'D') AS region,\n        COALESCE(c.direcion, 'D') AS address,\n        COALESCE(cp.codigoPostal, 'D') AS postalCode,\n        COALESCE(activo, 0) AS active,       \n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[cliente] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    On c.idPais = c.idPais\nINNER JOIN [default].[dbo].[codigoPostal] AS cp\n    ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Fact Gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Views Silver"
				},
				"content": {
					"query": "---------------\n-- Fact Sales --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_fact_Sales] AS \nSELECT  idDesgloseVenta AS idSales,\n        d.idArticulo AS idArticles,\n        c.idAlmacen AS idWarehouse,\n        c.idCliente AS idClient,\n        c.idCodigoPostal AS idPostalCodes,\n        c.idDivisa AS idCurrency,\n        c.idTarifa AS idTariff,\n        c.idTipoOperacion AS idOperationType,\n        c.idHora AS idHours,\n        c.idFecha AS idDate,\n        f.fecha AS  date,\n        CAST(CAST(f.fecha AS nvarchar(10)) + ' ' + h.horaCompleta AS datetime2)AS dateOp,\n        c.codigoTicket As ticketNumber,\n        d.Cantidad AS quantity,\n        d.PrecioUnitario AS unitPrice,\n        d.CosteUnitario AS unitCost,\n        d.importeBruto AS amtTotalEuros,\n        d.importeNeto AS amtNetEuros,\n        d.importeDescuento AS amtTotalEurosDiscount,\n        d.importeNetoDescuento AS amtNetEurosDiscount,\n        CASE WHEN idTarifa IN (2,3) THEN 1\n             ELSE 0 \n        END AS discountSale,\n        CASE WHEN idTarifa = 0 THEN 0\n            ELSE (d.importeBruto - d.importeDescuento) \n        END AS amtTotalDiscount,\n        CASE WHEN idTarifa = 0 THEN 0 \n             ELSE (d.importeNeto - d.importeNetoDescuento) \n        END AS amtNetDiscount,\n        d.fechaCarga AS loadDate,\n        d.fechaDelta AS deltaDate\nFROM [default].[dbo].[desgloseVenta] AS d\nINNER JOIN [default].[dbo].[cabeceraVenta] AS c\n    ON d.idcabecerasVentas = c.idcabeceraVenta\nLEFT JOIN [default].[dbo].[Fechas]  AS f\n    On f.idFechas = c.idFecha\nLEFT JOIN [default].[dbo].[horas]  AS h\n    On h.idHoras = c.idHora",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Fact Silver')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Views Silver"
				},
				"content": {
					"query": "---------------\n-- Fact Sales --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_fact_Sales] AS \nSELECT  idDesgloseVenta AS idSales,\n        d.idArticulo AS idArticles,\n        c.idAlmacen AS idWarehouse,\n        c.idCliente AS idClient,\n        c.idCodigoPostal AS idPostalCodes,\n        c.idDivisa AS idCurrency,\n        c.idTarifa AS idTariff,\n        c.idTipoOperacion AS idOperationType,\n        c.idHora AS idHours,\n        c.idFecha AS idDate,\n        f.fecha AS  date,\n        CAST(CAST(f.fecha AS nvarchar(10)) + ' ' + h.horaCompleta AS datetime2)AS dateOp,\n        c.codigoTicket As ticketNumber,\n        d.Cantidad AS quantity,\n        d.PrecioUnitario AS unitPrice,\n        d.CosteUnitario AS unitCost,\n        d.importeBruto AS amtTotalEuros,\n        d.importeNeto AS amtNetEuros,\n        d.importeDescuento AS amtTotalEurosDiscount,\n        d.importeNetoDescuento AS amtNetEurosDiscount,\n        CASE WHEN idTarifa IN (2,3) THEN 1\n             ELSE 0 \n        END AS discountSale,\n        CASE WHEN idTarifa = 0 THEN 0\n            ELSE (d.importeBruto - d.importeDescuento) \n        END AS amtTotalDiscount,\n        CASE WHEN idTarifa = 0 THEN 0 \n             ELSE (d.importeNeto - d.importeNetoDescuento) \n        END AS amtNetDiscount,\n        d.fechaCarga AS loadDate,\n        d.fechaDelta AS deltaDate\nFROM [default].[dbo].[desgloseVenta] AS d\nINNER JOIN [default].[dbo].[cabeceraVenta] AS c\n    ON d.idcabecerasVentas = c.idcabeceraVenta\nLEFT JOIN [default].[dbo].[Fechas]  AS f\n    On f.idFechas = c.idFecha\nLEFT JOIN [default].[dbo].[horas]  AS h\n    On h.idHoras = c.idHora",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/First Load Gold CETAS SCD2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Gold Dim CETAs"
				},
				"content": {
					"query": "---------------\n-- Articles  --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Articles]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles') )\n    DROP EXTERNAL TABLE dim.Articles \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idArticles) idSkArticles,\n\tidArticles,\n\tname,\n\texternalcode,\n\tsize,\n\tnumSize,\n\tcolour,\n\tcategory, \n\tcodLine,\n\tline, \n\tdescription,\n\tseason, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent, \n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idArticles AS NVARCHAR)+ name + description COLLATE DATABASE_DEFAULT + externalcode + size + CAST(numSize AS NVARCHAR) + colour + category + codLine + line + season)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Articles]\n\n\n--------------\n-- Client --\n--------------\n-- drop EXTERNAL TABLE [dim].[Client]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Client \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Client]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkClient, \n\t-1  \t\t AS idClient, \n\t'D' \t\t AS name,\n\t'D' \t\t AS lastName1,\n\t'D' \t\t AS lastName2,\n\t'D' AS email,\n\t'D' AS phoneNumber,\n\tCAST('1900-01-01' AS date) AS birthDay,       \n\t-1 AS age,\n\t'D' AS gender,\n\t'D' AS country,\n\t'D' AS countryCode,\n\t'D'AS region,\n\t'D' AS address,\n\t'D' AS postalCode,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' +'D' + CAST('1900-01-01' AS nvarchar) + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idClient) AS idSkClient,\n\tidClient,\n\tname,\n\tlastName1,\n\tlastName2, \n\temail,\n\tphoneNumber,\n\tbirthDay,       \n\tage,\n\tgender,\n\tcountry,\n\tcountryCode,\n\tregion,\n\taddress,\n\tpostalCode,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idClient AS NVARCHAR) + name COLLATE DATABASE_DEFAULT + lastName1 COLLATE DATABASE_DEFAULT + lastName2 COLLATE DATABASE_DEFAULT\n\t+ email COLLATE DATABASE_DEFAULT + phoneNumber COLLATE DATABASE_DEFAULT + CAST(birthDay AS nvarchar) + CAST(age AS nvarchar) + gender + country + countryCode\n\t+ region + address + postalCode)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Client]\n\n\n--------------\n-- Currency --\n--------------\n-- drop EXTERNAL TABLE [dim].[Currency]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Currency \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idCurrency) AS idSkCurrency,\n\tidCurrency,\n\tname,\n\tcurrency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idCurrency AS NVARCHAR) + name + currency)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Currency]\n\n\n----------\n-- Date --\n----------\n-- drop EXTERNAL TABLE [dim].[Date]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Date') )\n    DROP EXTERNAL TABLE dim.Date \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n    HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n    + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Date]\n\n\n-----------\n-- Hours --\n-----------\n-- drop EXTERNAL TABLE [dim].[Hours]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Hours') )\n    DROP EXTERNAL TABLE dim.Hours \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Hours]\n\n\n-------------------\n-- OperationType --\n-------------------\n-- drop EXTERNAL TABLE [dim].[OperationType]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.OperationType') )\n    DROP EXTERNAL TABLE dim.OperationType \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_OperationType]\n\n\n-----------------\n-- PostalCodes --\n-----------------\n-- drop EXTERNAL TABLE [dim].[PostalCodes]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.PostalCodes') )\n    DROP EXTERNAL TABLE dim.PostalCodes \n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idPostalCodes, \n\t'D' \t\t AS postalCode,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_PostalCodes]\n\n\n------------\n-- Tariff --\n------------\n-- drop EXTERNAL TABLE [dim].[Tariff]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Tariff') )\n    DROP EXTERNAL TABLE dim.Tariff \n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Tariff]\n\n\n---------------\n-- Warehouse --\n---------------\n-- drop EXTERNAL TABLE [dim].[Warehouse]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Warehouse') )\n    DROP EXTERNAL TABLE dim.Warehouse \n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idWarehouse) AS idSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Warehouse]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load SCD2 Silver CETAs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Silver SCD + Fact"
				},
				"content": {
					"query": "---------------\n-- Articles --\n---------------\n-- DROP EXTERNAL TABLE  scd.Articles\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Articles') )\n    DROP EXTERNAL TABLE scd.Articles;\n\n\nCREATE EXTERNAL TABLE  scd.Articles \nWITH\n(\n\tLOCATION = 'silver/SCD/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n    \n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Articles]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idArticles AS NVARCHAR)+ s.name + s.description COLLATE DATABASE_DEFAULT + s.externalcode + s.size + CAST(s.numSize AS NVARCHAR) + s.colour + s.category + s.codLine + s.line + s.season)) AS [$hash]\n    FROM etl.vw_dim_Articles s\n    LEFT JOIN current_data c\n        ON s.idArticles = c.idArticles\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkArticles), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Articles]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idArticles) + max_key AS new_idSkArticles, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idArticles = c.idArticles\n    CROSS JOIN max_surrogate_key\n    WHERE c.idArticles IS NULL  OR (c.[$hash] != chg.[$hash] )\n), --select * from new_or_updated_data order by idarticles\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkarticles AS new_idSkArticles,\n        c.idArticles,\n        c.name,\n        c.externalCode,\n        c.size,\n        c.numSize,\n        c.colour,\n        c.category,\n        c.codLine,\n        c.line,\n        c.description,\n        c.season,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idArticles = chg.idArticles\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkArticles, -- Mantener la clave subrogada original\n    c.idArticles,\n    c.name,\n    c.externalCode,\n    c.size,\n    c.numSize,\n    c.colour,\n    c.category,\n    c.codLine,\n    c.line,\n    c.description,\n    c.season,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idArticles = chg.idArticles\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkArticles;\n\n\n---------------\n-- Currency --\n---------------\n-- DROP EXTERNAL TABLE  scd.Currency\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Currency') )\n    DROP EXTERNAL TABLE scd.Currency;\n\n\nCREATE EXTERNAL TABLE  scd.Currency \nWITH\n(\n\tLOCATION = 'silver/SCD/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Currency]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idCurrency AS NVARCHAR) + s.name + s.currency)) AS [$hash]\n    FROM etl.vw_dim_Currency s\n    LEFT JOIN current_data c\n        ON s.idCurrency = c.idCurrency\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkCurrency), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Currency]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idCurrency) + max_key AS new_idSkCurrency, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idCurrency = c.idCurrency\n    CROSS JOIN max_surrogate_key\n    WHERE c.idCurrency IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkCurrency AS new_idSkCurrency,\n        c.idCurrency,\n        c.name,\n        c.currency,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idCurrency = chg.idCurrency\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkCurrency, -- Mantener la clave subrogada original\n    c.idCurrency,\n    c.name,\n    c.currency,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idCurrency = chg.idCurrency\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkCurrency;\n\n\n------------\n-- Client --\n------------\n--DROP EXTERNAL TABLE  scd.Client\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Client') )\n   DROP EXTERNAL TABLE scd.Client   \n\n\nCREATE EXTERNAL TABLE  scd.Client \nWITH\n(\n\tLOCATION = 'silver/SCD/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Client]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idClient AS NVARCHAR) + s.name COLLATE DATABASE_DEFAULT + s.lastName1 COLLATE DATABASE_DEFAULT + s.lastName2 COLLATE DATABASE_DEFAULT\n\t    + s.email COLLATE DATABASE_DEFAULT + s.phoneNumber COLLATE DATABASE_DEFAULT + CAST(s.birthDay AS nvarchar) + CAST(s.age AS nvarchar) + s.gender + s.country + s.countryCode\n\t    + s.region + s.address + s.postalCode)) AS [$hash]\n    FROM etl.vw_dim_Client s\n    LEFT JOIN current_data c\n        ON s.idClient = c.idClient\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkClient), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Client]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idClient) + max_key AS new_idSkClient, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idClient = c.idClient\n    CROSS JOIN max_surrogate_key\n    WHERE c.idClient IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkClient AS new_idSkClient,\n        c.idClient,\n        c.name,\n        c.lastName1,\n        c.lastName2, \n        c.email,\n        c.phoneNumber,\n        c.birthDay,       \n        c.age,\n        c.gender,\n        c.country,\n        c.countryCode,\n        c.region,\n        c.address,\n        c.postalCode,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idClient = chg.idClient\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkClient, -- Mantener la clave subrogada original\n    c.idClient,\n    c.name,\n    c.lastName1,\n    c.lastName2, \n    c.email,\n    c.phoneNumber,\n    c.birthDay,       \n    c.age,\n    c.gender,\n    c.country,\n    c.countryCode,\n    c.region,\n    c.address,\n    c.postalCode,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idClient = chg.idClient\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkClient;\n\n\n----------\n-- Date --\n----------\n--DROP EXTERNAL TABLE  scd.Date \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Date') )\n   DROP EXTERNAL TABLE scd.Date;  \n\n\nCREATE EXTERNAL TABLE  scd.Date \nWITH\n(\n\tLOCATION = 'silver/SCD/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Date]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idDate AS NVARCHAR) + CAST(s.dateKey AS NVARCHAR) + CAST(s.date AS NVARCHAR) + s.month COLLATE DATABASE_DEFAULT + CAST(s.monthNumber AS NVARCHAR) + CAST(s.year AS NVARCHAR) \n        + CAST(s.weekNumber AS NVARCHAR) + s.dayWeek COLLATE DATABASE_DEFAULT + CAST(s.dayWeekNumber AS NVARCHAR) + CAST(s.yearDay AS NVARCHAR) + s.quarter + s.quadrimester + s.semester )) AS [$hash]\n    FROM etl.vw_dim_Date s\n    LEFT JOIN current_data c\n        ON s.idDate = c.idDate\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkDate), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Date]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idDate) + max_key AS new_idSkDate, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idDate = c.idDate\n    CROSS JOIN max_surrogate_key\n    WHERE c.idDate IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkDate AS new_idSkDate,\n        c.idDate,\n        c.dateKey,\n        c.date,\n        c.month,\n        c.monthNumber,\n        c.year,\n        c.weekNumber,\n        c.dayWeek,\n        c.dayWeekNumber,\n        c.yearDay,\n        c.quarter,\n        c.quadrimester,\n        c.semester,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idDate = chg.idDate\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkDate, -- Mantener la clave subrogada original\n    c.idDate,\n\tc.dateKey,\n\tc.date,\n\tc.month,\n\tc.monthNumber,\n\tc.year,\n\tc.weekNumber,\n\tc.dayWeek,\n\tc.dayWeekNumber,\n\tc.yearDay,\n\tc.quarter,\n\tc.quadrimester,\n\tc.semester,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idDate = chg.idDate\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkDate;\n\n\n---------------\n-- Hours --\n---------------\n-- DROP EXTERNAL TABLE  scd.Hours\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Hours') )\n   DROP EXTERNAL TABLE scd.Hours;   \n\n\nCREATE EXTERNAL TABLE  scd.Hours \nWITH\n(\n\tLOCATION = 'silver/SCD/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Hours]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idHours AS NVARCHAR) + CAST(s.hour AS NVARCHAR) + CAST(s.minute AS NVARCHAR) + s.fullHour)) AS [$hash]\n    FROM etl.vw_dim_Hours s\n    LEFT JOIN current_data c\n        ON s.idHours = c.idHours\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkHours), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Hours]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idHours) + max_key AS new_idSkHours, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idHours = c.idHours\n    CROSS JOIN max_surrogate_key\n    WHERE c.idHours IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkHours AS new_idSkHours,\n        c.idHours,\n        c.hour,\n        c.minute,\n        c.fullHour,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idHours = chg.idHours\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkHours, -- Mantener la clave subrogada original\n\tc.idHours,\n\tc.hour,\n\tc.minute,\n\tc.fullHour,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idHours = chg.idHours\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkHours;\n\n\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE  scd.OperationType \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.OperationType') )\n   DROP EXTERNAL TABLE scd.OperationType;   \n\n\nCREATE EXTERNAL TABLE  scd.OperationType \nWITH\n(\n\tLOCATION = 'silver/SCD/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[OperationType]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idOperationType AS NVARCHAR) + s.operation)) AS [$hash]\n    FROM etl.vw_dim_OperationType s\n    LEFT JOIN current_data c\n        ON s.idOperationType = c.idOperationType\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkOperationType), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[OperationType]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idOperationType) + max_key AS new_idSkOperationType, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idOperationType = c.idOperationType\n    CROSS JOIN max_surrogate_key\n    WHERE c.idOperationType IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkOperationType AS new_idSkOperationType,\n        c.idOperationType,\n        c.operation,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idOperationType = chg.idOperationType\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkOperationType, -- Mantener la clave subrogada original\n    c.idOperationType,\n    c.operation,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idOperationType = chg.idOperationType\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkOperationType,\n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkOperationType, \n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkOperationType;\n\n\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE  scd.PostalCodes \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.PostalCodes') )\n    DROP EXTERNAL TABLE scd.PostalCodes; \n\nCREATE EXTERNAL TABLE  scd.PostalCodes \nWITH\n(\n\tLOCATION = 'silver/SCD/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idPostalCodes AS NVARCHAR) + s.postalCode + s.region + s.countryCode + s.country)) AS [$hash]\n    FROM etl.vw_dim_PostalCodes s\n    LEFT JOIN current_data c\n        ON s.idPostalCodes = c.idPostalCodes\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkPostalCodes), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idPostalCodes) + max_key AS new_idSkPostalCodes, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idPostalCodes = c.idPostalCodes\n    CROSS JOIN max_surrogate_key\n    WHERE c.idPostalCodes IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkPostalCodes AS new_idSkPostalCodes,\n        c.idPostalCodes,\n        c.postalCode,\n        c.region,\n        c.countryCode,\n        c.country,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idPostalCodes = chg.idPostalCodes\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkPostalCodes, -- Mantener la clave subrogada original\n    c.idPostalCodes,\n    c.postalCode,\n    c.region,\n    c.countryCode,\n    c.country,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idPostalCodes = chg.idPostalCodes\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkPostalCodes,\n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkPostalCodes, \n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkPostalCodes;\n\n\n\n------------\n-- Tariff --\n------------\n--DROP EXTERNAL TABLE  scd.Tariff \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Tariff') )\n   DROP EXTERNAL TABLE scd.Tariff;   \n\n\nCREATE EXTERNAL TABLE  scd.Tariff \nWITH\n(\n\tLOCATION = 'silver/SCD/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Tariff]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idTariff AS NVARCHAR) + s.tariff)) AS [$hash]\n    FROM etl.vw_dim_Tariff s\n    LEFT JOIN current_data c\n        ON s.idTariff = c.idTariff\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkTariff), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Tariff]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idTariff) + max_key AS new_idSkTariff, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idTariff = c.idTariff\n    CROSS JOIN max_surrogate_key\n    WHERE c.idTariff IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkTariff AS new_idSkTariff,\n        c.idTariff,\n        c.tariff,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idTariff = chg.idTariff\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkTariff, -- Mantener la clave subrogada original\n    c.idTariff,\n    c.tariff,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idTariff = chg.idTariff\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkTariff;\n\n\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE  scd.Warehouse \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Warehouse') )\n   DROP EXTERNAL TABLE scd.Warehouse;   \n\n\nCREATE EXTERNAL TABLE  scd.Warehouse \nWITH\n(\n\tLOCATION = 'silver/SCD/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.idWarehouse,\n        s.warehouse,\n        s.externalCode,\n        s.countryCode,\n        s.country,\n        s.city,\n        s.address,\n        s.description,\n        s.loadDate,\n        s.deltaDate,\n        HASHBYTES('SHA2_256',(CAST(s.idWarehouse AS NVARCHAR) + s.warehouse + s.externalcode + s.countryCode + s.country + s.city + s.address + s.description)) AS [$hash]\n    FROM etl.vw_dim_Warehouse s\n    LEFT JOIN current_data c\n        ON s.idWarehouse = c.idWarehouse\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkWarehouse), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idWarehouse) + max_key AS new_idSkWarehouse, -- Asignar nueva clave subrogada\n        chg.idWarehouse,\n        chg.warehouse,\n        chg.externalCode,\n        chg.countryCode,\n        chg.country,\n        chg.city,\n        chg.address,\n        chg.description,\n        chg.loadDate,\n        chg.deltaDate,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate,\n        chg.[$hash]\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idWarehouse = c.idWarehouse\n    CROSS JOIN max_surrogate_key\n    WHERE c.idWarehouse IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkWarehouse AS new_idSkWarehouse,\n        c.idWarehouse,\n        c.warehouse,\n        c.externalCode,\n        c.countryCode,\n        c.country,\n        c.city,\n        c.address,\n        c.description,\n        c.loadDate,\n        c.deltaDate,\n        c.isCurrent,\n        c.fromDate,\n        c.toDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idWarehouse = chg.idWarehouse\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\nfinal AS (\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nSELECT\n    c.idSkWarehouse, -- Mantener la clave subrogada original\n    c.idWarehouse,\n    c.warehouse,\n    c.externalCode,\n    c.countryCode,\n    c.country,\n    c.city,\n    c.address,\n    c.description,\n    c.loadDate,\n    c.deltaDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.fromDate,\n    GETDATE() AS toDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idWarehouse = chg.idWarehouse\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM unchanged_data\n\n)\nselect     idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM final\nORDER BY idSkWarehouse;\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load Silver Fact')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Silver SCD + Fact"
				},
				"content": {
					"query": "-- EXEC silver.FactSales\n-- ==============================================================================\n-- Procedimiento almacenado para cargar la tabla de hechos Sales en SilverlessSTG\n-- ==============================================================================\n\n\nCREATE OR ALTER PROCEDURE silver.FactSales\nWITH ENCRYPTION AS \n\n--------------\n-- Sales  --\n---------------\n-- DROP EXTERNAL TABLE [silver].[Sales]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('silver.Sales') )\n    DROP EXTERNAL TABLE silver.Sales \n\nCREATE  EXTERNAL TABLE SilverlessSTG.[silver].[Sales]\nWITH\n(\n\tLOCATION = 'silver/Fact/Sales', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT\n    -1 AS idSkSales,\n    -1 AS idSales,\n    -1 AS idArticles,\n    -1 AS idWarehouse,\n    -1 AS idClient,\n    -1 AS idPostalCodes,\n    -1 AS idCurrency,\n    -1 AS idTariff,\n    -1 AS idOperationType,\n    -1 AS idHours,\n    -1 AS idDate,\n    CAST('1990-01-01' AS datetime2) AS date,\n    CAST('1990-01-01' AS datetime2) dateOp,\n    'D' AS ticketNumber,\n    -1 AS quantity,\n    -1 AS unitPrice,\n    -1 AS unitCost,\n    -1 AS amtTotalEuros,\n    -1 AS amtNetEuros,\n    -1 AS amtTotalEurosDiscount,\n    -1 AS amtNetEurosDiscount,\n    -1 AS discountSale,\n    -1 AS amtTotalDiscount,\n    -1 AS amtNetDiscount,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1' + '-1' +'-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '1990-01-01' + '1990-01-01' + 'D' \n    + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1')) AS [$hash]\nUNION\nSELECT  ROW_NUMBER() OVER (ORDER BY idSales) AS idSkSales,\n        *, \n        HASHBYTES('SHA2_256',( CAST(idSales AS nvarchar) + CAST(idArticles AS nvarchar) + CAST(idWarehouse AS nvarchar) + CAST(idClient AS nvarchar) \n        + CAST(idPostalCodes AS nvarchar) + CAST(idCurrency AS nvarchar) + CAST(idTariff AS nvarchar) + CAST(idOperationType AS nvarchar) \n        + CAST(idHours AS nvarchar) + CAST(idDate AS nvarchar) + CAST(date AS nvarchar) + CAST(dateOp AS nvarchar) + ticketNumber \n        + CAST(quantity AS nvarchar) + CAST(unitPrice AS nvarchar) + CAST(unitCost AS nvarchar) + CAST(amtTotalEuros AS nvarchar) \n        + CAST(amtNetEuros AS nvarchar) + CAST(amtTotalEurosDiscount AS nvarchar) + CAST(amtNetEurosDiscount AS nvarchar) + CAST(discountSale AS nvarchar) \n        + CAST(amtTotalDiscount AS nvarchar) + CAST(amtNetDiscount AS nvarchar) )) AS [$hash]\nFROM [SilverlessSTG].[etl].[vw_fact_Sales]\n) a ORDER BY idSkSales",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Re-Load Fact')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Gold Fact CETAs"
				},
				"content": {
					"query": "-- EXEC gold.FactSales\n-- ==============================================================================\n-- Procedimiento almacenado para cargar la tabla de hechos Sales en GoldenlessSTG\n-- ==============================================================================\n\n\nCREATE OR ALTER PROCEDURE gold.FactSales\nWITH ENCRYPTION AS \n\n--------------\n-- Sales  --\n---------------\n-- DROP EXTERNAL TABLE [fact].[Sales]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('fact.Sales') )\n    DROP EXTERNAL TABLE fact.Sales \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[fact].[Sales]\nWITH\n(\n\tLOCATION = 'gold/Fact/Sales', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT\n    -1 AS idSkSales,\n    -1 AS idSales,\n    -1 AS idArticles,\n    -1 AS idskArticles,\n    -1 AS idWarehouse,\n    -1 AS idskWarehouse,\n    -1 AS idClient,\n    -1 AS idskClient,\n    -1 AS idPostalCodes,\n    -1 AS idskPostalCodes,\n    -1 AS idCurrency,\n    -1 AS idskCurrency,\n    -1 AS idTariff,\n    -1 AS idskTariff,\n    -1 AS idOperationType,\n    -1 AS idskOperationType,\n    -1 AS idHours,\n    -1 AS idskHours,\n    -1 AS idDate,\n    -1 AS idskDate,\n    CAST('1990-01-01' AS datetime2) AS date,\n    CAST('1990-01-01' AS datetime2) dateOp,\n    'D' AS ticketNumber,\n    -1 AS quantity,\n    -1 AS unitPrice,\n    -1 AS unitCost,\n    -1 AS amtTotalEuros,\n    -1 AS amtNetEuros,\n    -1 AS amtTotalEurosDiscount,\n    -1 AS amtNetEurosDiscount,\n    -1 AS discountSale,\n    -1 AS amtTotalDiscount,\n    -1 AS amtNetDiscount,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate\n\t--HASHBYTES('SHA2_256',('-1' + '-1' +'-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '1990-01-01' + '1990-01-01' + 'D' \n    --+ '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1')) AS [$hash]\nUNION\n\nSELECT \n    s.idSkSales,\n    s.idSales,\n    s.idArticles,\n    COALESCE(a.idSkArticles, -1) AS idSkArticles,\n    s.idWarehouse,\n    COALESCE(w.idSkWarehouse, -1) AS idSkWarehouse,\n    s.idClient,\n    COALESCE(cl.idSkClient, -1) AS idSkClient, \n    s.idPostalCodes,\n    COALESCE(p.idSkPostalCodes, -1) AS idSkPostalCodes, \n    s.idCurrency,\n    COALESCE(cu.idSkCurrency, -1) AS idSkCurrency,\n    s.idTariff,\n    COALESCE(t.idSkTariff, -1) AS idSkTariff, \n    s.idOperationType,\n    COALESCE(o.idSkOperationType, -1) AS idSkOperationType,\n    s.idHours,\n    COALESCE(h.idSkHours, -1) AS idSkHours,\n    s.idDate,\n    COALESCE(d.idSkDate, -1) AS idSkDate,\n    s.date,\n    s.dateOp,\n    s.ticketNumber,\n    s.quantity,\n    s.unitPrice,\n    s.unitCost,\n    s.amtTotalEuros,\n    s.amtNetEuros,\n    s.amtTotalEurosDiscount,\n    s.amtNetEurosDiscount,\n    s.discountSale,\n    s.amtTotalDiscount,\n    s.amtNetDiscount,\n    s.loadDate,\n    s.deltaDate\nFROM [SilverlessSTG].[silver].[Sales] AS s\nLEFT JOIN [GoldenlessDWH].[dim].[Articles] AS a\n    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Client] AS cl\n    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Currency] AS cu\n    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Date] AS d\n    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Hours] AS h\n    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[OperationType] AS o\n    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[PostalCodes] AS p\n    ON s.idPostalCodes = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Tariff] AS t\n    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Warehouse] AS w\n    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\n\n\n\n\n\n) a ORDER BY idSkSales",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Re-Load Gold CETAs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Gold Dim CETAs"
				},
				"content": {
					"query": "---------------\n-- Articles  --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Articles]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles') )\n    DROP EXTERNAL TABLE dim.Articles \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash]\nUNION\nSELECT *\nFROM [SilverlessSTG].[scd].[Articles]\n) a ORDER BY idSkArticles\n\n\n--------------\n-- Client--\n--------------\n-- DROP EXTERNAL TABLE [dim].[Client]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Client') )\n    DROP EXTERNAL TABLE dim.Client \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Client]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkClient, \n\t-1  \t\t AS idClient, \n\t'D' \t\t AS name,\n\t'D' \t\t AS lastName1,\n\t'D' \t\t AS lastName2,\n\t'D' AS email,\n\t'D' AS phoneNumber,\n\tCAST('1900-01-01' AS date) AS birthDay,       \n\t-1 AS age,\n\t'D' AS gender,\n\t'D' AS country,\n\t'D' AS countryCode,\n\t'D'AS region,\n\t'D' AS address,\n\t'D' AS postalCode,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' +'D' + CAST('1900-01-01' AS nvarchar) + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Client]\n) C ORDER BY idSkClient\n\n\n--------------\n-- Currency --\n--------------\n-- DROP EXTERNAL TABLE [dim].[Currency]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Currency \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2)  AS toDate,\t\t\n\t1 \t\t\t AS isCurrent, \t\t \t\t\t \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Currency]\n) C ORDER BY idSkCurrency\n\n\n----------\n-- Date --\n----------\n-- DROP EXTERNAL TABLE [dim].[Date]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Date') )\n    DROP EXTERNAL TABLE dim.Date \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n        HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n        + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Date]\n) d ORDER BY idSkDate\n\n-----------\n-- Hours --\n-----------\n-- DROP EXTERNAL TABLE [dim].[Hours]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Hours') )\n    DROP EXTERNAL TABLE dim.Hours \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Hours]\n) h ORDER BY idSkHours\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE [dim].[OperationType]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.OperationType') )\n    DROP EXTERNAL TABLE dim.OperationType \nGO\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_OperationType]\n) o ORDER BY idSkOperationType\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE [dim].[PostalCodes]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.PostalCodes') )\n    DROP EXTERNAL TABLE dim.PostalCodes \nGO\n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idpostalCode, \n\t'D' \t\t AS postalCodes,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_PostalCodes]\n) p ORDER BY idSkPostalCodes\n\n------------\n-- Tariff --\n------------\n-- DROP EXTERNAL TABLE [dim].[Tariff]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Tariff') )\n    DROP EXTERNAL TABLE dim.Tariff \n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Tariff]\n) T ORDER BY idSkTariff\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Warehouse]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Warehouse') )\n    DROP EXTERNAL TABLE dim.Warehouse \nGO\n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM ( \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tidSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\tisCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Warehouse]\n) W ORDER BY idSkWarehouse",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SP Backup')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- EXEC bk.LoadDim\n-- ==============================================================================\n-- Procedimiento almacenado para cargar las dimensiones en la BBDD Backup \n-- ==============================================================================\n\nCREATE OR ALTER PROCEDURE bk.LoadDim\nWITH ENCRYPTION AS \n\n\n---------------\n-- Articles --\n---------------\n-- DROP EXTERNAL TABLE  bk.dim_Articles\n\n-- IF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('bk.dim_Articles') ) AND ( SELECT * FROM GoldenlessDWH.sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles') )\n--     DROP EXTERNAL TABLE bk.dim_Articles;\n\n-- Darle otra vuelta para hace check de DWH antes del DROP en otra tarea \n\nIF EXISTS (\n    SELECT *  FROM sys.external_tables  WHERE object_id = OBJECT_ID('bk.dim_Articles') ) \nAND \n    EXISTS (\n    SELECT * FROM GoldenlessDWH.sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles') )\nBEGIN\n    DROP EXTERNAL TABLE bk.dim_Articles;\nEND;\n\n\n\nCREATE EXTERNAL TABLE  bk.dim_Articles \nWITH\n(\n\tLOCATION = 'gold/Backup/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n    \n) AS \n\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Articles]\n    ORDER BY idSkArticles\n\n---------------\n-- Currency --\n---------------\n-- DROP EXTERNAL TABLE  bk.dim_Currency\n\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('bk.dim_Currency') )\n    DROP EXTERNAL TABLE bk.dim_Currency;\n\nCREATE EXTERNAL TABLE  bk.dim_Currency \nWITH\n(\n\tLOCATION = 'gold/Backup/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Currency]\n    ORDER BY idSkCurrency\n\n------------\n-- Client --\n------------\n--DROP EXTERNAL TABLE  bk.dim_Client\n\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('bk.dim_Client') )\n   DROP EXTERNAL TABLE bk.dim_Client   \n\nCREATE EXTERNAL TABLE  bk.dim_Client \nWITH\n(\n\tLOCATION = 'gold/Backup/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Client]\n    ORDER BY idSkClient\n\n----------\n-- Date --\n----------\n--DROP EXTERNAL TABLE  bk.dim_Date \n\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('bk.dim_Date') )\n   DROP EXTERNAL TABLE bk.dim_Date;  \n\nCREATE EXTERNAL TABLE  bk.dim_Date \nWITH\n(\n\tLOCATION = 'gold/Backup/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Date]\n    ORDER BY idSkDate\n\n---------------\n-- Hours --\n---------------\n-- DROP EXTERNAL TABLE  bk.dim_Hours\n\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('bk.dim_Hours') )\n   DROP EXTERNAL TABLE bk.dim_Hours;   \n\nCREATE EXTERNAL TABLE  bk.dim_Hours \nWITH\n(\n\tLOCATION = 'gold/Backup/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Hours]\n    ORDER BY idSkHours\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE  bk.dim_OperationType \n\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('bk.dim_OperationType') )\n   DROP EXTERNAL TABLE bk.dim_OperationType;   \n\nCREATE EXTERNAL TABLE  bk.dim_OperationType \nWITH\n(\n\tLOCATION = 'gold/Backup/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[OperationType]\n    ORDER BY idSkOperationType\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE  bk.dim_PostalCodes \n\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('bk.dim_PostalCodes') )\n    DROP EXTERNAL TABLE bk.dim_PostalCodes; \n\nCREATE EXTERNAL TABLE  bk.dim_PostalCodes \nWITH\n(\n\tLOCATION = 'gold/Backup/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n    ORDER BY idSkPostalCodes\n\n\n------------\n-- Tariff --\n------------\n--DROP EXTERNAL TABLE  bk.dim_Tariff \n\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('bk.dim_Tariff') )\n   DROP EXTERNAL TABLE bk.dim_Tariff;   \n\nCREATE EXTERNAL TABLE  bk.dim_Tariff \nWITH\n(\n\tLOCATION = 'gold/Backup/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Tariff]\n    ORDER BY idSkTariff\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE  bk.dim_Warehouse \n\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('bk.dim_Warehouse') )\n   DROP EXTERNAL TABLE bk.dim_Warehouse;   \n\nCREATE EXTERNAL TABLE  bk.dim_Warehouse \nWITH\n(\n\tLOCATION = 'gold/Backup/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n    ORDER BY idSkWarehouse\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "BackuplessDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SP Reload Dim Gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- EXEC gold.ReloadDimSCD\n\nCREATE OR ALTER PROCEDURE gold.ReloadDimSCD\nWITH ENCRYPTION AS \n\n---------------\n-- Articles  --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Articles]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles'))\n    DROP EXTERNAL TABLE dim.Articles \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash],\n\txxhash64()\nUNION\nSELECT *\nFROM [SilverlessSTG].[scd].[Articles]\n) a ORDER BY idSkArticles\n\n\n--------------\n-- Client--\n--------------\n-- DROP EXTERNAL TABLE [dim].[Client]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Client') )\n    DROP EXTERNAL TABLE dim.Client \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Client]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkClient, \n\t-1  \t\t AS idClient, \n\t'D' \t\t AS name,\n\t'D' \t\t AS lastName1,\n\t'D' \t\t AS lastName2,\n\t'D' AS email,\n\t'D' AS phoneNumber,\n\tCAST('1900-01-01' AS date) AS birthDay,       \n\t-1 AS age,\n\t'D' AS gender,\n\t'D' AS country,\n\t'D' AS countryCode,\n\t'D'AS region,\n\t'D' AS address,\n\t'D' AS postalCode,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' +'D' + CAST('1900-01-01' AS nvarchar) + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Client]\n) C ORDER BY idSkClient\n\n\n--------------\n-- Currency --\n--------------\n-- DROP EXTERNAL TABLE [dim].[Currency]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Currency \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2)  AS toDate,\t\t\n\t1 \t\t\t AS isCurrent, \t\t \t\t\t \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Currency]\n) C ORDER BY idSkCurrency\n\n\n----------\n-- Date --\n----------\n-- DROP EXTERNAL TABLE [dim].[Date]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Date') )\n    DROP EXTERNAL TABLE dim.Date \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n        HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n        + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Date]\n) d ORDER BY idSkDate\n\n-----------\n-- Hours --\n-----------\n-- DROP EXTERNAL TABLE [dim].[Hours]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Hours') )\n    DROP EXTERNAL TABLE dim.Hours \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Hours]\n) h ORDER BY idSkHours\n\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE [dim].[OperationType]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.OperationType') )\n    DROP EXTERNAL TABLE dim.OperationType \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[OperationType]\n) o ORDER BY idSkOperationType\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE [dim].[PostalCodes]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.PostalCodes') )\n    DROP EXTERNAL TABLE dim.PostalCodes \n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idpostalCode, \n\t'D' \t\t AS postalCodes,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[PostalCodes]\n) p ORDER BY idSkPostalCodes\n\n------------\n-- Tariff --\n------------\n-- DROP EXTERNAL TABLE [dim].[Tariff]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Tariff') )\n    DROP EXTERNAL TABLE dim.Tariff \n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Tariff]\n) T ORDER BY idSkTariff\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Warehouse]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Warehouse') )\n    DROP EXTERNAL TABLE dim.Warehouse \n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM ( \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tidSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\tisCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Warehouse]\n) W ORDER BY idSkWarehouse",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SP Silver Dim SCD2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Silver SCD + Fact"
				},
				"content": {
					"query": "CREATE OR ALTER PROCEDURE silver.DimSCD\nWITH ENCRYPTION AS \n\n---------------\n-- Articles --\n---------------\n-- DROP EXTERNAL TABLE  scd.Articles\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Articles') )\n    DROP EXTERNAL TABLE scd.Articles;\n\n\nCREATE EXTERNAL TABLE  scd.Articles \nWITH\n(\n\tLOCATION = 'silver/SCD/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n    \n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Articles]\n    WHERE isCurrent = 1\n),\n\n-- CTE* creacion\n\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idArticles AS NVARCHAR)+ s.name + s.description COLLATE DATABASE_DEFAULT + s.externalcode + s.size + CAST(s.numSize AS NVARCHAR) + s.colour + s.category + s.codLine + s.line + s.season)) AS [$hash]\n    FROM etl.vw_dim_Articles s  -- METER AQUI DEFINICION LAS VISTAS CTE*\n    LEFT JOIN current_data c\n        ON s.idArticles = c.idArticles\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkArticles), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Articles]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idArticles) + max_key AS new_idSkArticles, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idArticles = c.idArticles\n    CROSS JOIN max_surrogate_key\n    WHERE c.idArticles IS NULL  OR (c.[$hash] != chg.[$hash] )\n), --select * from new_or_updated_data order by idarticles\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkarticles AS new_idSkArticles,\n        c.idArticles,\n        c.name,\n        c.externalCode,\n        c.size,\n        c.numSize,\n        c.colour,\n        c.category,\n        c.codLine,\n        c.line,\n        c.description,\n        c.season,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idArticles = chg.idArticles\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkArticles, -- Mantener la clave subrogada original\n    c.idArticles,\n    c.name,\n    c.externalCode,\n    c.size,\n    c.numSize,\n    c.colour,\n    c.category,\n    c.codLine,\n    c.line,\n    c.description,\n    c.season,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idArticles = chg.idArticles\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkArticles;\n\n\n---------------\n-- Currency --\n---------------\n-- DROP EXTERNAL TABLE  scd.Currency\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Currency') )\n    DROP EXTERNAL TABLE scd.Currency;\n\n\nCREATE EXTERNAL TABLE  scd.Currency \nWITH\n(\n\tLOCATION = 'silver/SCD/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Currency]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idCurrency AS NVARCHAR) + s.name + s.currency)) AS [$hash]\n    FROM etl.vw_dim_Currency s\n    LEFT JOIN current_data c\n        ON s.idCurrency = c.idCurrency\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkCurrency), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Currency]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idCurrency) + max_key AS new_idSkCurrency, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idCurrency = c.idCurrency\n    CROSS JOIN max_surrogate_key\n    WHERE c.idCurrency IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkCurrency AS new_idSkCurrency,\n        c.idCurrency,\n        c.name,\n        c.currency,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idCurrency = chg.idCurrency\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkCurrency, -- Mantener la clave subrogada original\n    c.idCurrency,\n    c.name,\n    c.currency,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idCurrency = chg.idCurrency\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkCurrency;\n\n\n------------\n-- Client --\n------------\n--DROP EXTERNAL TABLE  scd.Client\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Client') )\n   DROP EXTERNAL TABLE scd.Client   \n\n\nCREATE EXTERNAL TABLE  scd.Client \nWITH\n(\n\tLOCATION = 'silver/SCD/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Client]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idClient AS NVARCHAR) + s.name COLLATE DATABASE_DEFAULT + s.lastName1 COLLATE DATABASE_DEFAULT + s.lastName2 COLLATE DATABASE_DEFAULT\n\t    + s.email COLLATE DATABASE_DEFAULT + s.phoneNumber COLLATE DATABASE_DEFAULT + CAST(s.birthDay AS nvarchar) + CAST(s.age AS nvarchar) + s.gender + s.country + s.countryCode\n\t    + s.region + s.address + s.postalCode)) AS [$hash]\n    FROM etl.vw_dim_Client s\n    LEFT JOIN current_data c\n        ON s.idClient = c.idClient\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkClient), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Client]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idClient) + max_key AS new_idSkClient, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idClient = c.idClient\n    CROSS JOIN max_surrogate_key\n    WHERE c.idClient IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkClient AS new_idSkClient,\n        c.idClient,\n        c.name,\n        c.lastName1,\n        c.lastName2, \n        c.email,\n        c.phoneNumber,\n        c.birthDay,       \n        c.age,\n        c.gender,\n        c.country,\n        c.countryCode,\n        c.region,\n        c.address,\n        c.postalCode,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idClient = chg.idClient\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkClient, -- Mantener la clave subrogada original\n    c.idClient,\n    c.name,\n    c.lastName1,\n    c.lastName2, \n    c.email,\n    c.phoneNumber,\n    c.birthDay,       \n    c.age,\n    c.gender,\n    c.country,\n    c.countryCode,\n    c.region,\n    c.address,\n    c.postalCode,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idClient = chg.idClient\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkClient;\n\n\n----------\n-- Date --\n----------\n--DROP EXTERNAL TABLE  scd.Date \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Date') )\n   DROP EXTERNAL TABLE scd.Date;  \n\n\nCREATE EXTERNAL TABLE  scd.Date \nWITH\n(\n\tLOCATION = 'silver/SCD/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Date]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idDate AS NVARCHAR) + CAST(s.dateKey AS NVARCHAR) + CAST(s.date AS NVARCHAR) + s.month COLLATE DATABASE_DEFAULT + CAST(s.monthNumber AS NVARCHAR) + CAST(s.year AS NVARCHAR) \n        + CAST(s.weekNumber AS NVARCHAR) + s.dayWeek COLLATE DATABASE_DEFAULT + CAST(s.dayWeekNumber AS NVARCHAR) + CAST(s.yearDay AS NVARCHAR) + s.quarter + s.quadrimester + s.semester )) AS [$hash]\n    FROM etl.vw_dim_Date s\n    LEFT JOIN current_data c\n        ON s.idDate = c.idDate\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkDate), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Date]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idDate) + max_key AS new_idSkDate, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idDate = c.idDate\n    CROSS JOIN max_surrogate_key\n    WHERE c.idDate IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkDate AS new_idSkDate,\n        c.idDate,\n        c.dateKey,\n        c.date,\n        c.month,\n        c.monthNumber,\n        c.year,\n        c.weekNumber,\n        c.dayWeek,\n        c.dayWeekNumber,\n        c.yearDay,\n        c.quarter,\n        c.quadrimester,\n        c.semester,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idDate = chg.idDate\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkDate, -- Mantener la clave subrogada original\n    c.idDate,\n\tc.dateKey,\n\tc.date,\n\tc.month,\n\tc.monthNumber,\n\tc.year,\n\tc.weekNumber,\n\tc.dayWeek,\n\tc.dayWeekNumber,\n\tc.yearDay,\n\tc.quarter,\n\tc.quadrimester,\n\tc.semester,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idDate = chg.idDate\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkDate;\n\n\n---------------\n-- Hours --\n---------------\n-- DROP EXTERNAL TABLE  scd.Hours\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Hours') )\n   DROP EXTERNAL TABLE scd.Hours;   \n\n\nCREATE EXTERNAL TABLE  scd.Hours \nWITH\n(\n\tLOCATION = 'silver/SCD/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Hours]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idHours AS NVARCHAR) + CAST(s.hour AS NVARCHAR) + CAST(s.minute AS NVARCHAR) + s.fullHour)) AS [$hash]\n    FROM etl.vw_dim_Hours s\n    LEFT JOIN current_data c\n        ON s.idHours = c.idHours\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkHours), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Hours]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idHours) + max_key AS new_idSkHours, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idHours = c.idHours\n    CROSS JOIN max_surrogate_key\n    WHERE c.idHours IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkHours AS new_idSkHours,\n        c.idHours,\n        c.hour,\n        c.minute,\n        c.fullHour,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idHours = chg.idHours\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkHours, -- Mantener la clave subrogada original\n\tc.idHours,\n\tc.hour,\n\tc.minute,\n\tc.fullHour,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idHours = chg.idHours\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkHours;\n\n\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE  scd.OperationType \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.OperationType') )\n   DROP EXTERNAL TABLE scd.OperationType;   \n\n\nCREATE EXTERNAL TABLE  scd.OperationType \nWITH\n(\n\tLOCATION = 'silver/SCD/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[OperationType]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idOperationType AS NVARCHAR) + s.operation)) AS [$hash]\n    FROM etl.vw_dim_OperationType s\n    LEFT JOIN current_data c\n        ON s.idOperationType = c.idOperationType\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkOperationType), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[OperationType]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idOperationType) + max_key AS new_idSkOperationType, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idOperationType = c.idOperationType\n    CROSS JOIN max_surrogate_key\n    WHERE c.idOperationType IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkOperationType AS new_idSkOperationType,\n        c.idOperationType,\n        c.operation,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idOperationType = chg.idOperationType\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkOperationType, -- Mantener la clave subrogada original\n    c.idOperationType,\n    c.operation,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idOperationType = chg.idOperationType\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkOperationType,\n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkOperationType, \n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkOperationType;\n\n\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE  scd.PostalCodes \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.PostalCodes') )\n    DROP EXTERNAL TABLE scd.PostalCodes; \n\nCREATE EXTERNAL TABLE  scd.PostalCodes \nWITH\n(\n\tLOCATION = 'silver/SCD/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idPostalCodes AS NVARCHAR) + s.postalCode + s.region + s.countryCode + s.country)) AS [$hash]\n    FROM etl.vw_dim_PostalCodes s\n    LEFT JOIN current_data c\n        ON s.idPostalCodes = c.idPostalCodes\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkPostalCodes), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idPostalCodes) + max_key AS new_idSkPostalCodes, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idPostalCodes = c.idPostalCodes\n    CROSS JOIN max_surrogate_key\n    WHERE c.idPostalCodes IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkPostalCodes AS new_idSkPostalCodes,\n        c.idPostalCodes,\n        c.postalCode,\n        c.region,\n        c.countryCode,\n        c.country,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idPostalCodes = chg.idPostalCodes\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkPostalCodes, -- Mantener la clave subrogada original\n    c.idPostalCodes,\n    c.postalCode,\n    c.region,\n    c.countryCode,\n    c.country,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idPostalCodes = chg.idPostalCodes\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkPostalCodes,\n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkPostalCodes, \n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkPostalCodes;\n\n\n\n------------\n-- Tariff --\n------------\n--DROP EXTERNAL TABLE  scd.Tariff \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Tariff') )\n   DROP EXTERNAL TABLE scd.Tariff;   \n\n\nCREATE EXTERNAL TABLE  scd.Tariff \nWITH\n(\n\tLOCATION = 'silver/SCD/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Tariff]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idTariff AS NVARCHAR) + s.tariff)) AS [$hash]\n    FROM etl.vw_dim_Tariff s\n    LEFT JOIN current_data c\n        ON s.idTariff = c.idTariff\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkTariff), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Tariff]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idTariff) + max_key AS new_idSkTariff, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idTariff = c.idTariff\n    CROSS JOIN max_surrogate_key\n    WHERE c.idTariff IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkTariff AS new_idSkTariff,\n        c.idTariff,\n        c.tariff,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idTariff = chg.idTariff\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkTariff, -- Mantener la clave subrogada original\n    c.idTariff,\n    c.tariff,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idTariff = chg.idTariff\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkTariff;\n\n\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE  scd.Warehouse \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Warehouse') )\n   DROP EXTERNAL TABLE scd.Warehouse;   \n\n\nCREATE EXTERNAL TABLE  scd.Warehouse \nWITH\n(\n\tLOCATION = 'silver/SCD/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.idWarehouse,\n        s.warehouse,\n        s.externalCode,\n        s.countryCode,\n        s.country,\n        s.city,\n        s.address,\n        s.description,\n        s.loadDate,\n        s.deltaDate,\n        HASHBYTES('SHA2_256',(CAST(s.idWarehouse AS NVARCHAR) + s.warehouse + s.externalcode + s.countryCode + s.country + s.city + s.address + s.description)) AS [$hash]\n    FROM etl.vw_dim_Warehouse s\n    LEFT JOIN current_data c\n        ON s.idWarehouse = c.idWarehouse\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkWarehouse), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idWarehouse) + max_key AS new_idSkWarehouse, -- Asignar nueva clave subrogada\n        chg.idWarehouse,\n        chg.warehouse,\n        chg.externalCode,\n        chg.countryCode,\n        chg.country,\n        chg.city,\n        chg.address,\n        chg.description,\n        chg.loadDate,\n        chg.deltaDate,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate,\n        chg.[$hash]\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idWarehouse = c.idWarehouse\n    CROSS JOIN max_surrogate_key\n    WHERE c.idWarehouse IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkWarehouse AS new_idSkWarehouse,\n        c.idWarehouse,\n        c.warehouse,\n        c.externalCode,\n        c.countryCode,\n        c.country,\n        c.city,\n        c.address,\n        c.description,\n        c.loadDate,\n        c.deltaDate,\n        c.isCurrent,\n        c.fromDate,\n        c.toDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idWarehouse = chg.idWarehouse\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\nfinal AS (\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nSELECT\n    c.idSkWarehouse, -- Mantener la clave subrogada original\n    c.idWarehouse,\n    c.warehouse,\n    c.externalCode,\n    c.countryCode,\n    c.country,\n    c.city,\n    c.address,\n    c.description,\n    c.loadDate,\n    c.deltaDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.fromDate,\n    GETDATE() AS toDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idWarehouse = chg.idWarehouse\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM unchanged_data\n\n)\nselect     idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM final\nORDER BY idSkWarehouse;\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "test",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) * FROM [gold].[dbo].[client]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "gold",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoadSCD2AndFact')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "edaca146-8240-45f9-a302-7bf2ff5732e0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Import modules\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format,concat_ws,xxhash64,current_timestamp\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql import functions as F\r\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark.sql('select * from gold.warehouse')\r\n",
							"\r\n",
							"df.show(20,False)"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'Tariff'\r\n",
							"delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Dimensions/{table_name}\""
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#spark.sql(\"DROP DATABASE IF EXISTS gold CASCADE\")\r\n",
							"\r\n",
							"\r\n",
							"spark.sql(f'CREATE TABLE IF NOT EXISTS gold.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
							"spark.sql(f\"ALTER TABLE gold.{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"spark.sql(f\"ALTER TABLE gold.{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dimensionsList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Articles\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idArticulos                                                                         AS idArticles,\r\n",
							"                            COALESCE(nombre, 'D')                                                               AS name,\r\n",
							"                            COALESCE(descripcion, 'D')                                                          AS description,\r\n",
							"                            COALESCE(codigoReferencia, 'D')                                                     AS externalCode,\r\n",
							"                            COALESCE(t.talla, 'D')                                                              AS size,\r\n",
							"                            COALESCE( t.numeroTalla, -1)                                                        AS numSize,\r\n",
							"                            COALESCE(co.color, 'D')                                                             AS colour,\r\n",
							"                            COALESCE(ca.categoria, 'D')                                                         AS category,\r\n",
							"                            COALESCE(l.codigoLinea, 'D')                                                        AS codLine,\r\n",
							"                            COALESCE(l.Linea, 'D')                                                              AS line, \r\n",
							"                            CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN 'OI'+CAST((a.idTemporada) AS string)\r\n",
							"                                WHEN a.idCategoria IN (2,4,6,7,9) THEN 'OI'+CAST(a.idTemporada AS string)\r\n",
							"                                ELSE 'PV'+CAST(a.idTemporada AS string)\r\n",
							"                            END                                                                                 AS season,\r\n",
							"                            a.fechaCarga                                                                        AS loadDate,\r\n",
							"                            a.fechaDelta                                                                        AS deltaDate\r\n",
							"                    FROM default.articulos AS a\r\n",
							"                    LEFT JOIN default.talla AS t\r\n",
							"                        ON t.idTalla = a.idTalla\r\n",
							"                    LEFT JOIN default.color AS co\r\n",
							"                    ON co.idColor = a.idColor\r\n",
							"                    LEFT JOIN default.categoria AS ca -- select * from default.categoria\r\n",
							"                        ON ca.idCategoria = a.idCategoria\r\n",
							"                    LEFT JOIN default.Linea AS l\r\n",
							"                        ON l.idLinea = a.idLinea\r\n",
							"                    where  a.fechaCarga  < '20241009'\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Client\",\r\n",
							"            \"query\": \"\"\"              \r\n",
							"                    SELECT  idCliente AS idClient,\r\n",
							"                            COALESCE(c.nombre, 'D') AS name,\r\n",
							"                            COALESCE(apellido1, 'D') AS lastName1,\r\n",
							"                            COALESCE(apellido2, 'D') AS lastName2, \r\n",
							"                            COALESCE(email, 'D') AS email,\r\n",
							"                            COALESCE(telefono, 'D') AS phoneNumber,\r\n",
							"                            COALESCE(CAST(cumpleanos AS STRING), '1900-01-01') AS birthDay,       \r\n",
							"                            YEAR(current_date()) - YEAR(CAST(cumpleanos AS DATE)) \r\n",
							"                                - CASE \r\n",
							"                                    WHEN MONTH(current_date()) < MONTH(CAST(cumpleanos AS DATE)) \r\n",
							"                                        OR (MONTH(current_date()) = MONTH(CAST(cumpleanos AS DATE)) AND DAY(current_date()) < DAY(CAST(cumpleanos AS DATE)))\r\n",
							"                                    THEN 1\r\n",
							"                                    ELSE 0\r\n",
							"                                END AS age,\r\n",
							"                            CASE WHEN hombre = 1 THEN 'Hombre' ELSE 'Mujer' END AS gender,\r\n",
							"                            COALESCE(p.nombre, 'D') AS country,\r\n",
							"                            COALESCE(p.codigoPais, 'D') AS countryCode,\r\n",
							"                            COALESCE(cp.region, 'D') AS region,\r\n",
							"                            COALESCE(c.Direcion, 'D') AS address,  \r\n",
							"                            COALESCE(codigoPostal, 'D') AS postalCode,\r\n",
							"                            COALESCE(activo, false) AS active,  -- Cambiado 0 a false aquÃ­\r\n",
							"                            c.fechaCarga AS loadDate,\r\n",
							"                            c.fechaDelta AS deltaDate\r\n",
							"                        FROM default.cliente AS c\r\n",
							"                        LEFT JOIN default.pais AS p ON c.idPais = p.idPais\r\n",
							"                        INNER JOIN default.codigoPostal AS cp ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\r\n",
							"\r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Currency\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idDivisa AS idCurrency,\r\n",
							"                            COALESCE(nombre, 'D') AS name,\r\n",
							"                            COALESCE(Divisa, 'D') AS currency,\r\n",
							"                            fechaCarga AS loadDate,\r\n",
							"                            fechaDelta AS deltaDate\r\n",
							"                    FROM default.divisa\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Date\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idFechas AS idDate,\r\n",
							"                            ClaveFecha AS dateKey,\r\n",
							"                            Fecha AS date,\r\n",
							"                            COALESCE(Mes, 'D')  AS month,\r\n",
							"                            COALESCE(NumeroMes, -1)  AS monthNumber,\r\n",
							"                            COALESCE(Ano, -1)  AS year,\r\n",
							"                            COALESCE(NumeroSemana, -1)  AS weekNumber,\r\n",
							"                            COALESCE(DiaSemana, 'D') AS dayWeek,\r\n",
							"                            COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\r\n",
							"                            COALESCE(DiaAno, -1) AS yearDay,\r\n",
							"                            CASE WHEN Trimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Trimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Trimestre = 3 THEN 'Q3'\r\n",
							"                                ELSE '-1' END AS quarter,\r\n",
							"                            CASE WHEN Cuatrimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Cuatrimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Cuatrimestre = 3 THEN 'Q3'\r\n",
							"                                WHEN Cuatrimestre = 4 THEN 'Q4'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS quadrimester,\r\n",
							"                            CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\r\n",
							"                                WHEN Cuatrimestre IN (2,4) THEN 'S2'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS semester,\r\n",
							"                            fechaCarga AS loadDate,\r\n",
							"                            fechaDelta AS deltaDate\r\n",
							"                    FROM default.fechas;\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Hours\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idHoras AS idHours,\r\n",
							"                            COALESCE(Hora, -1) AS hour,\r\n",
							"                            COALESCE(Minuto, -1) AS minute,\r\n",
							"                            COALESCE(HoraCompleta, 'D') as fullHour,\r\n",
							"                            fechaCarga AS loadDate,\r\n",
							"                            fechaDelta AS deltaDate\r\n",
							"                    FROM default.horas\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"OperationType\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTipoOperacion AS idOperationType,\r\n",
							"                            COALESCE(Operacion, 'D') AS operation,\r\n",
							"                            fechaCarga AS loadDate,\r\n",
							"                            fechaDelta AS deltaDate\r\n",
							"                    FROM default.tipoOperacion\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"PostalCode\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idCodigoPostal AS idPostalCode,\r\n",
							"                            COALESCE(codigoPostal, 'D') AS postalCode,\r\n",
							"                            COALESCE(region, 'D') AS region,\r\n",
							"                            COALESCE(c.codigoPais, 'D') AS countryCode,\r\n",
							"                            COALESCE(nombre, 'D') AS country,\r\n",
							"                            c.fechaCarga AS loadDate,\r\n",
							"                            c.fechaDelta AS deltaDate\r\n",
							"                    FROM default.codigoPostal AS c\r\n",
							"                    LEFT JOIN default.pais AS p\r\n",
							"                        ON p.codigoPais = c.codigoPais\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Tariff\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTarifa AS idTariff,\r\n",
							"                            COALESCE(Tarifa, 'D') AS tariff,\r\n",
							"                            fechaCarga AS loadDate,\r\n",
							"                            fechaDelta AS deltaDate       \r\n",
							"                    FROM default.tarifa\r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Warehouse\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idAlmacenes AS idWarehouse,\r\n",
							"                            COALESCE(a.Nombre, 'D') AS warehouse,\r\n",
							"                            COALESCE(a.codigoAlmacen, 'D') AS externalCode,\r\n",
							"                            COALESCE(p.codigoPais, 'D') AS countryCode, \r\n",
							"                            COALESCE(p.nombre, 'D') AS country,\r\n",
							"                            COALESCE(a.ciudad, 'D') AS city,\r\n",
							"                            COALESCE(a.Direccion, 'D') AS address,\r\n",
							"                            COALESCE(a.descripcion, 'D') AS description,\r\n",
							"                            a.fechaCarga AS loadDate,\r\n",
							"                            a.fechaDelta AS deltaDate\r\n",
							"                    FROM default.almacenes AS a\r\n",
							"                    LEFT JOIN default.pais AS p\r\n",
							"                        ON p.idpais = a.idPais\r\n",
							"                    WHERE a.fechaDelta <> 20241011\r\n",
							"                    UNION \r\n",
							"                    select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\r\n",
							"                    --UNION \r\n",
							"                    --select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]\r\n",
							"    "
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dimensionsList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Warehouse\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idAlmacenes AS idWarehouse,\r\n",
							"                            COALESCE(a.Nombre, 'D') AS warehouse,\r\n",
							"                            COALESCE(a.codigoAlmacen, 'D') AS externalCode,\r\n",
							"                            COALESCE(p.codigoPais, 'D') AS countryCode, \r\n",
							"                            COALESCE(p.nombre, 'D') AS country,\r\n",
							"                            COALESCE(a.ciudad, 'D') AS city,\r\n",
							"                            COALESCE(a.Direccion, 'D') AS address,\r\n",
							"                            COALESCE(a.descripcion, 'D') AS description,\r\n",
							"                            a.fechaCarga AS loadDate,\r\n",
							"                            a.fechaDelta AS deltaDate\r\n",
							"                    FROM default.almacenes AS a\r\n",
							"                    LEFT JOIN default.pais AS p\r\n",
							"                        ON p.idpais = a.idPais\r\n",
							"                    --WHERE a.fechaDelta <> 20241011\r\n",
							"                    --UNION \r\n",
							"                    --select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\r\n",
							"                    --UNION \r\n",
							"                    --select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Supongamos que ya has definido 'sdf' y que contiene tus datos iniciales.\r\n",
							"\r\n",
							"# AsegÃºrate de que la surrogate key se genere correctamente\r\n",
							"sdf = sdf.withColumn(\"surrogate_key\", F.monotonically_increasing_id())  # Genera surrogate_key\r\n",
							"\r\n",
							"# Comprobar si la tabla Delta existe\r\n",
							"if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"    delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"    \r\n",
							"    # Ahora, asegurarte de que surrogate_key estÃ¡ presente\r\n",
							"    existing_columns = delta_table.toDF().columns\r\n",
							"\r\n",
							"    # Obtener la Ãºltima surrogate key para crear nuevas\r\n",
							"    last_surrogate_key = delta_table.toDF().agg(F.max(\"surrogate_key\")).collect()[0][0] or 0\r\n",
							"    next_surrogate_key = last_surrogate_key + 1\r\n",
							"    \r\n",
							"    # Merge new data into existing table con lÃ³gica SCD Tipo 2\r\n",
							"    delta_table.alias(\"existing\").merge(\r\n",
							"        source=sdf.alias(\"updates\"),\r\n",
							"        condition=\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])\r\n",
							"    ).whenMatchedUpdate(\r\n",
							"        condition=\"existing.isCurrent = 1 AND existing.hash != updates.hash\",\r\n",
							"        set={\r\n",
							"            \"toDate\": F.current_date(),\r\n",
							"            \"isCurrent\": F.lit(0)\r\n",
							"        }\r\n",
							"    ).whenNotMatchedInsert(\r\n",
							"        values={\r\n",
							"            \"surrogate_key\": F.expr(f\"{next_surrogate_key} + row_number() over (order by (select null)) - 1\"),  # Aumenta la surrogate key en base a la fila\r\n",
							"            **{f\"updates.{col}\": f\"updates.{col}\" for col in existing_columns}\r\n",
							"        }\r\n",
							"    ).execute()\r\n",
							"\r\n",
							"else:\r\n",
							"    # Crear nueva tabla Delta\r\n",
							"    last_surrogate_key = 0  # Iniciar con 0 si la tabla no existe\r\n",
							"    sdf = sdf.withColumn(\"surrogate_key\", F.monotonically_increasing_id() + last_surrogate_key + 1)  # Asigna un valor Ãºnico a cada fila.\r\n",
							"    \r\n",
							"    # AÃ±adir las columnas necesarias\r\n",
							"    sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"    sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"    sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"    # Crear la tabla Delta\r\n",
							"    sdf.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path)\r\n",
							"\r\n",
							"    spark.sql(f'CREATE TABLE IF NOT EXISTS gold.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
							"    spark.sql(f\"ALTER TABLE gold.{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"    spark.sql(f\"ALTER TABLE gold.{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')\")\r\n",
							"\r\n",
							"    # Almacenar archivo procesado correctamente\r\n",
							"    file_result['status'] = 'Correcto'\r\n",
							"    file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"    file_result['updated_rows'] = 0\r\n",
							"    correct_files.append(file['name'])\r\n",
							""
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import functions as F\r\n",
							"from delta.tables import DeltaTable\r\n",
							"spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\r\n",
							"# Variable para la fecha de carga\r\n",
							"fecha_delta = '20241012'\r\n",
							"\r\n",
							"# Inicializar listas para almacenar resultados\r\n",
							"results = []\r\n",
							"correct_files = []\r\n",
							"incorrect_files = []\r\n",
							"exception = []\r\n",
							"\r\n",
							"for file in dimensionsList:\r\n",
							"    file_result = {\r\n",
							"        'file_name': file['name'],\r\n",
							"        'status': 'Incorrecto',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A'\r\n",
							"    }\r\n",
							"\r\n",
							"    try:\r\n",
							"        table_name = file[\"name\"].split('_')[0]\r\n",
							"        delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Dimensions/{table_name}\"\r\n",
							"\r\n",
							"        # Leer archivo(s) en DataFrame de Spark\r\n",
							"        df = spark.sql(file[\"query\"])\r\n",
							"        sdf = df.cache()\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['exception'] = \"Fichero vacio\"\r\n",
							"            results.append(file_result)\r\n",
							"            correct_files.append(file['name'])\r\n",
							"            continue\r\n",
							"\r\n",
							"        # AÃ±adir las columnas necesarias\r\n",
							"        sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"        sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"        sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"        columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fromDate', 'toDate', 'isCurrent')]\r\n",
							"        sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"        # Comprobar si la tabla Delta existe\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"            existing_columns = delta_table.toDF().columns\r\n",
							"            \r\n",
							"            # Obtener la Ãºltima surrogate key para crear nuevas\r\n",
							"            last_surrogate_key = delta_table.toDF().agg(F.max(\"surrogate_key\")).collect()[0][0] or 0\r\n",
							"            next_surrogate_key = last_surrogate_key + 1\r\n",
							"            \r\n",
							"            # Merge new data into existing table con lÃ³gica SCD Tipo 2\r\n",
							"            delta_table.alias(\"existing\").merge(\r\n",
							"                source=sdf.alias(\"updates\"),\r\n",
							"                condition=\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])\r\n",
							"            ).whenMatchedUpdate(\r\n",
							"                condition=\"existing.isCurrent = 1 AND existing.hash != updates.hash\",\r\n",
							"                set={\r\n",
							"                    \"toDate\": F.current_date(),\r\n",
							"                    \"isCurrent\": F.lit(0)\r\n",
							"                }\r\n",
							"            ).whenNotMatchedInsert(\r\n",
							"                values={\r\n",
							"                    \"surrogate_key\": F.expr(f\"{next_surrogate_key} + row_number() over (order by (select null)) - 1\"),  # Aumenta la surrogate key en base a la fila\r\n",
							"                    **{f\"updates.{col}\": f\"updates.{col}\" for col in existing_columns}\r\n",
							"                }\r\n",
							"            ).execute()\r\n",
							"\r\n",
							"            # Almacenar conteos de filas\r\n",
							"            file_result['status'] = 'Correcto'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            file_result['inserted_rows'] = last_result[0].operationMetrics[\"numTargetRowsInserted\"]\r\n",
							"            file_result['updated_rows'] = last_result[0].operationMetrics[\"numTargetRowsUpdated\"]\r\n",
							"\r\n",
							"            correct_files.append(file['name'])\r\n",
							"\r\n",
							"        else:\r\n",
							"\r\n",
							"            spark.sql(f'DROP TABLE IF EXISTS gold.{table_name}')\r\n",
							"\r\n",
							"            # Crear nueva tabla Delta\r\n",
							"            last_surrogate_key = 0  # Iniciar con 0 si la tabla no existe\r\n",
							"            sdf = sdf.withColumn(\"surrogate_key\", F.monotonically_increasing_id() + last_surrogate_key + 1)  # Asigna un valor Ãºnico a cada fila.\r\n",
							"            #sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"\r\n",
							"            #sdf = sdf.withColumn(\"toDate\", lit(fecha_delta))  # Establecer fechaDelta\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"\r\n",
							"            #sdf = sdf.withColumn(\"isCurrent\", lit(fecha_delta))  # Establecer fechaDelta\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('surrogate_key,fechaCarga', 'fechaDelta','fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"            sdf.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')\")\r\n",
							"\r\n",
							"            # Almacenar archivo procesado correctamente\r\n",
							"            file_result['status'] = 'Correcto'\r\n",
							"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"            file_result['updated_rows'] = 0\r\n",
							"            correct_files.append(file['name'])\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        incorrect_files.append(file['name'])\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)\r\n",
							"\r\n",
							"# Convertir resultados a DataFrame\r\n",
							"results_df = spark.createDataFrame(results)\r\n",
							"\r\n",
							"# Ruta donde quieres guardar el archivo\r\n",
							"output_path = \"/gold/Logs/log_prueba_20241020.txt\"\r\n",
							"\r\n",
							"print(\"======================\")\r\n",
							"# Imprimir resultados en consola\r\n",
							"for result in results:\r\n",
							"    print(f\"Archivo: {result['file_name']}, Estado: {result['status']}, Filas insertadas: {result['inserted_rows']}, Filas actualizadas: {result['updated_rows']}, ExcepciÃ³n: {result['exception']}\")\r\n",
							"\r\n",
							"# ConfirmaciÃ³n\r\n",
							"print(f\"Resultados guardados en: {output_path}\")\r\n",
							"\r\n",
							"# Guardar nombres de archivos correctos e incorrectos en variables de la pipeline\r\n",
							"correct_files_variable = correct_files\r\n",
							"incorrect_files_variable = incorrect_files\r\n",
							"\r\n",
							"# Si necesitas ver las variables\r\n",
							"print(\"Archivos correctos:\", correct_files_variable)\r\n",
							"print(\"Archivos incorrectos:\", incorrect_files_variable)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"DROP TABLE gold.warehouse\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# BK\r\n",
							"#spark.sql(f'DROP TABLE Silver.{table_name}')\r\n",
							"\r\n",
							"#fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
							"\r\n",
							"\r\n",
							"fecha_delta = '20241012'\r\n",
							"\r\n",
							"# Inicializar listas para almacenar resultados\r\n",
							"results = []\r\n",
							"correct_files = []\r\n",
							"incorrect_files = []\r\n",
							"exception = []\r\n",
							"\r\n",
							"for file in dimensionsList:\r\n",
							"    file_result = {\r\n",
							"        'file_name': file['name'],\r\n",
							"        'status': 'Incorrecto',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A'\r\n",
							"    }\r\n",
							"\r\n",
							"    try:\r\n",
							"        #table_name = file[\"name\"][:file[\"name\"].index('_')]  # Extrae la subcadena\r\n",
							"        table_name = file[\"name\"].split('_')[0]\r\n",
							"        #table_name = \"almacenes\"\r\n",
							"        source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
							"        key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
							"\r\n",
							"        #key_columns = key_columns_str\r\n",
							"        key_columns = key_columns_str.split(',')\r\n",
							"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"        #conditions_list = key_columns\r\n",
							"\r\n",
							"        # # Convert array with keys to where-clause for merge statement\r\n",
							"        #conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"        # Determinar ruta de archivos fuente\r\n",
							"        #source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"        #delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"        \r\n",
							"        delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Dimensions/{table_name}\"\r\n",
							"        \r\n",
							"        # Leer archivo(s) en DataFrame de Spark\r\n",
							"        #sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"        df = spark.sql(file[\"query\"])\r\n",
							"        sdf = df\r\n",
							"        sdf.cache()\r\n",
							"        #sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")  # Eliminar columnas no deseadas\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:  # Valorar si meter filtrado antes y enviar a errores o no y no eviar ni a procesados ni a errores\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['exception'] = \"Fichero vacio\"\r\n",
							"            results.append(file_result)  # Agregar resultado sin procesar\r\n",
							"            correct_files.append(file['name'])  # Agregar a la lista de archivos incorrectos\r\n",
							"            continue  # Saltar a la siguiente iteraciÃ³n si no hay datos\r\n",
							"\r\n",
							"        # Eliminar columna no deseada\r\n",
							"        #sdf = sdf.drop(\"fechaActualizacion\")\r\n",
							"        #sdf = sdf.withColumn(\"fromDate\", F.current_date()) \r\n",
							"        sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss')) \r\n",
							"                                      # Nueva fecha de inicio\r\n",
							"        sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"        sdf = sdf.withColumn(\"isCurrent\", F.lit(1))  \r\n",
							"        columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta', 'fromDate', 'toDate', 'isCurrent')]\r\n",
							"        sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"        # Comprobar si la tabla Delta existe\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"            existing_columns = delta_table.toDF().columns\r\n",
							"            \r\n",
							"            # AÃ±adir columna fechaDelta\r\n",
							"            #sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))\r\n",
							"\r\n",
							"            # columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta', 'fromDate', 'toDate', 'isCurrent')]\r\n",
							"            # sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # Crear conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
							"            #update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"            # Merge new data into existing table con lÃ³gica SCD Tipo 2\r\n",
							"            delta_table.alias(\"existing\").merge(\r\n",
							"                source=sdf.alias(\"updates\"),\r\n",
							"                condition=\" AND \".join(conditions_list)\r\n",
							"            ).whenMatchedUpdate(\r\n",
							"                condition=\"existing.isCurrent = 1 AND existing.hash != updates.hash\",\r\n",
							"                set={\r\n",
							"                    \"toDate\": F.current_date(),  # Marcar la fecha de fin como hoy\r\n",
							"                    \"isCurrent\": F.lit(0)         # Marcar como no actual\r\n",
							"                }\r\n",
							"            ).whenNotMatchedInsert(\r\n",
							"                values={\r\n",
							"                    # # \"fechaCarga\": \"updates.fechaCarga\",\r\n",
							"                    # # \"fechaDelta\": fecha_delta,\r\n",
							"                    # \"fromDate\": F.current_date(),  # Nueva fecha de inicio\r\n",
							"                    # \"toDate\": F.lit('9999-12-31'),  # Fecha de fin indefinida\r\n",
							"                    # \"isCurrent\": F.lit(1),           # Este es el registro actual\r\n",
							"                    # **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fromDate', 'toDate', 'isCurrent')}\r\n",
							"                    #\"hash\": fecha_delta,\r\n",
							"                    **{f\"{col}\": f\"updates.{col}\" for col in existing_columns}\r\n",
							"                }\r\n",
							"            ).execute()\r\n",
							"\r\n",
							"            #post_merge_count = delta_table.toDF().count()\r\n",
							"            # Almacenar conteos de filas\r\n",
							"            file_result['status'] = 'Correcto'\r\n",
							"            \r\n",
							"            #file_result['inserted_rows'] = merge_result.get('numTargetRowsInserted', 0)  # Filas insertadas  numTargetRowsInserted\r\n",
							"            #file_result['inserted_rows'] = merge_result.numTargetRowsInserted  # Filas insertadas  numTargetRowsInserted\r\n",
							"\r\n",
							"            #file_result['inserted_rows'] = 0  #merge_result.get('numInserted', 0)  # Filas insertadas\r\n",
							"            #file_result['updated_rows'] = merge_result.get('numTargetRowsUpdated', 0)  # Filas actualizadas\r\n",
							"            #file_result['updated_rows'] = merge_result.numTargetRowsUpdated # Filas actualizadas\r\n",
							"\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.{table_name}\")\r\n",
							"\r\n",
							"            # Filtrar solo las operaciones de tipo MERGE y seleccionar las mÃ©tricas\r\n",
							"            merge_operations = history_df.filter(history_df.operation == 'MERGE').select(\r\n",
							"                \"operation\",\r\n",
							"                \"timestamp\",\r\n",
							"                history_df.operationMetrics[\"numTargetRowsInserted\"].alias(\"numTargetRowsInserted\"),\r\n",
							"                history_df.operationMetrics[\"numTargetRowsUpdated\"].alias(\"numTargetRowsUpdated\")\r\n",
							"            )\r\n",
							"\r\n",
							"            # Obtener el Ãºltimo registro ordenando por timestamp en orden descendente\r\n",
							"            last_merge_operation = merge_operations.orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"\r\n",
							"            # Collect the result\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            # print(f\"Operation: {last_result.operation}, Timestamp: {last_result.timestamp}, Inserted: {last_result.numTargetRowsInserted}, Updated: {last_result.numTargetRowsUpdated}\")\r\n",
							"            #row = last_result[0]\r\n",
							"\r\n",
							"            # Obtener los valores de numTargetRowsInserted y numTargetRowsUpdated\r\n",
							"            file_result['inserted_rows'] = last_result[0].numTargetRowsInserted\r\n",
							"            file_result['updated_rows'] = last_result[0].numTargetRowsUpdated\r\n",
							"\r\n",
							"\r\n",
							"            #post_merge_count = delta_table.toDF().count()\r\n",
							"\r\n",
							"            # Calcular el nÃºmero de filas insertadas\r\n",
							"            #file_result['inserted_rows'] = max(0, post_merge_count - pre_merge_count)\r\n",
							"            #file_result['updated_rows'] = merge_result.get('numUpdated', 0) if merge_result is not None else 0\r\n",
							"\r\n",
							"            correct_files.append(file['name'])  # Agregar a la lista de archivos correctos\r\n",
							"\r\n",
							"        else:\r\n",
							"            # Crear nueva tabla Delta\r\n",
							"            #sdf = sdf.withColumn(\"fromDate\", sdf.fechaCarga)  # Mantener fechaCarga\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"\r\n",
							"            #sdf = sdf.withColumn(\"toDate\", lit(fecha_delta))  # Establecer fechaDelta\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"\r\n",
							"            #sdf = sdf.withColumn(\"isCurrent\", lit(fecha_delta))  # Establecer fechaDelta\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta','fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"            sdf.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"            # Almacenar archivo procesado correctamente\r\n",
							"            file_result['status'] = 'Correcto'\r\n",
							"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
							"            file_result['updated_rows'] = 0  # Ninguna fila actualizada en la creaciÃ³n\r\n",
							"            correct_files.append(file['name'])  # Agregar a la lista de archivos correctos\r\n",
							"\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        # Estado ya configurado como 'Incorrecto'\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)  # Agregar resultado de fallo al final\r\n",
							"        incorrect_files.append(file['name'])  # Agregar a la lista de archivos incorrectos\r\n",
							"        continue  # Continuar con la siguiente iteraciÃ³n\r\n",
							"\r\n",
							"    results.append(file_result)  # Agregar resultado al final del bucle\r\n",
							"\r\n",
							"# Convertir resultados a DataFrame\r\n",
							"results_df = spark.createDataFrame(results)\r\n",
							"\r\n",
							"# Ruta donde quieres guardar el archivo\r\n",
							"output_path = \"/gold/Logs/log_prueba_20241020.txt\"  # Cambia esta ruta segÃºn tu configuraciÃ³n\r\n",
							"#output_path= \"/Users/pablo2799@hotmail.es/\"\r\n",
							"# Escribir los resultados en un archivo TXT\r\n",
							"#results_df.coalesce(1).write.mode('overwrite').text(output_path)\r\n",
							"\r\n",
							"\r\n",
							"# Concatenar las columnas en una sola columna con un delimitador (por ejemplo, \"|\")\r\n",
							"#results_df = results_df.select(concat_ws(\"|\", *results_df.columns).alias(\"result\"))\r\n",
							"\r\n",
							"# Escribir el DataFrame en un archivo de texto\r\n",
							"#results_df.coalesce(1).write.mode('overwrite').text(output_path)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"print(\"======================\")\r\n",
							"#print(f\"Resultados guardados en: {output_path}\")\r\n",
							"\r\n",
							"\r\n",
							"# Imprimir resultados en consola\r\n",
							"for result in results:\r\n",
							"    print(f\"Archivo: {result['file_name']}, Estado: {result['status']}, Filas insertadas: {result['inserted_rows']}, Filas actualizadas: {result['updated_rows']}, ExcepciÃ³n: {result['exception']}\")\r\n",
							"\r\n",
							"# with open(output_path, 'w') as file:\r\n",
							"#     for row in results:\r\n",
							"#         # Crear la cadena en el formato deseado\r\n",
							"#         line = (f\"Archivo: {row['file_name']}, Estado: {row['status']}, \"\r\n",
							"#                 f\"Filas insertadas: {row['inserted_rows']}, Filas actualizadas: {row['updated_rows']}, \"\r\n",
							"#                 f\"ExcepciÃ³n: {row['exception']}\\n\")\r\n",
							"        \r\n",
							"#         # Agregar la lÃ­nea al archivo\r\n",
							"#         file.write(line)\r\n",
							"\r\n",
							"# ConfirmaciÃ³n\r\n",
							"print(f\"Resultados guardados en: {output_path}\")\r\n",
							"\r\n",
							"# Guardar nombres de archivos correctos e incorrectos en variables de la pipeline\r\n",
							"correct_files_variable = correct_files\r\n",
							"incorrect_files_variable = incorrect_files\r\n",
							"\r\n",
							"# Si necesitas ver las variables\r\n",
							"print(\"Archivos correctos:\", correct_files_variable)\r\n",
							"print(\"Archivos incorrectos:\", incorrect_files_variable)"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 19
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MergeLandingFilesToSilver')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e3af29eb-8b5c-41b5-8e7d-e62af53fd9f6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# # path of the data lake container (bronze and silver for this example)\r\n",
							"# data_lake_container = 'abfss://mdw@datalake1pgc.dfs.core.windows.net'\r\n",
							"# # The ingestion folder where your parquet file are located\r\n",
							"# bronze_folder = 'bronze/Landing'\r\n",
							"# # The silver folder where your Delta Tables will be stored\r\n",
							"# silver_folder = 'silver'\r\n",
							"# # The name of the table\r\n",
							"# table_name = 'color'\r\n",
							"# # The wildcard filter used within the bronze folder to find files\r\n",
							"# source_wildcard = 'color*.parquet'\r\n",
							"# # A comma separated string of one or more key columns (for the merge)\r\n",
							"# key_columns_str = 'idColor'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Carga de ficheros parquet Landing sobre tablas Delta capa Silver**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Import modules\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format\r\n",
							"from pyspark.sql import SparkSession"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Prueba::\r\n",
							"\r\n",
							"# Supongamos que tienes las variables necesarias\r\n",
							"# Fecha actual en formato yyyymmdd\r\n",
							"fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
							"#fecha_delta = '20241011'\r\n",
							"# Convert comma separated string with keys to array\r\n",
							"key_columns = key_columns_str.split(',')\r\n",
							"\r\n",
							"# Convert array with keys to where-clause for merge statement\r\n",
							"conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"# Determine path of source files from ingest layer\r\n",
							"source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"\r\n",
							"# Determine path of Delta Lake Table \r\n",
							"delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"\r\n",
							"# Read file(s) in spark data frame\r\n",
							"sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"sdf.cache()\r\n",
							"\r\n",
							"# Check countRows > 0 - Stop Notebook \r\n",
							"# if sdf.count() == 0:\r\n",
							"#     print(\"no procesar\")\r\n",
							"\r\n",
							"if sdf.limit(1).count() == 0:\r\n",
							"    print(\"no procesar\")\r\n",
							"\r\n",
							"else:\r\n",
							"    # Eliminar las columnas que no se desean\r\n",
							"    sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")\r\n",
							"\r\n",
							"    # Check if the Delta Table exists\r\n",
							"    if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"        # Read the existing Delta Table\r\n",
							"        delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"\r\n",
							"        #ALTER TABLE sales SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true');\r\n",
							"        spark.sql(f\"ALTER TABLE default.{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"\r\n",
							"        # Get the schema of the existing Delta table\r\n",
							"        existing_columns = delta_table.toDF().columns\r\n",
							"        \r\n",
							"        # AÃ±adir columna fechaDelta al DataFrame\r\n",
							"        sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))\r\n",
							"\r\n",
							"        # Crear el conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
							"        update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"        # Merge new data into existing table\r\n",
							"        delta_table.alias(\"existing\").merge(\r\n",
							"            source=sdf.alias(\"updates\"),\r\n",
							"            condition=\" AND \".join(conditions_list)\r\n",
							"        ).whenMatchedUpdate(\r\n",
							"            condition=\" OR \".join([f\"existing.{col} != updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')]),\r\n",
							"            set={\r\n",
							"                \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"                \"fechaDelta\": \"updates.fechaDelta\",  # Usar fechaDelta del DataFrame\r\n",
							"                **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"            }\r\n",
							"        ).whenNotMatchedInsert(\r\n",
							"            values={\r\n",
							"                \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"                \"fechaDelta\": fecha_delta,  # Establecer fechaDelta al insertar\r\n",
							"                **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"            }\r\n",
							"        ).execute()\r\n",
							"    else:\r\n",
							"        # Crear nueva tabla Delta con nuevos datos, incluyendo la columna fechaDelta\r\n",
							"        sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga del DataFrame\r\n",
							"        sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))  # Establecer fechaDelta al crear la tabla\r\n",
							"        sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Mover ficheros de Pendign a /Error o /Processed"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Crear manualmente Delta Tables**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# # path of the data lake container (bronze and silver for this example)\r\n",
							"# data_lake_container = 'abfss://mdw@datalake1pgc.dfs.core.windows.net'\r\n",
							"# # The ingestion folder where your parquet file are located\r\n",
							"# bronze_folder = 'bronze/Landing/'\r\n",
							"# # The silver folder where your Delta Tables will be stored\r\n",
							"# silver_folder = 'silver/DeltaSTG'\r\n",
							"# # The name of the table\r\n",
							"#table_name = 'lineaventa'\r\n",
							"# # The wildcard filter used within the bronze folder to find files\r\n",
							"# #source_wildcard = 'color*.parquet'\r\n",
							"# # A comma separated string of one or more key columns (for the merge)\r\n",
							"# #key_columns_str = 'idColor'\r\n",
							"\r\n",
							"# #source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"\r\n",
							"# # Determine path of Delta Lake Table \r\n",
							"# delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"#spark.sql(f'DROP TABLE default.{table_name}')\r\n",
							"# spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ### Copia:\r\n",
							"\r\n",
							"# # Supongamos que tienes las variables necesarias\r\n",
							"# # Fecha actual en formato yyyymmdd\r\n",
							"# fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
							"# #fecha_delta = '20241011'\r\n",
							"# # Convert comma separated string with keys to array\r\n",
							"# key_columns = key_columns_str.split(',')\r\n",
							"\r\n",
							"# # Convert array with keys to where-clause for merge statement\r\n",
							"# conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"# # Determine path of source files from ingest layer\r\n",
							"# source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
							"\r\n",
							"# # Determine path of Delta Lake Table \r\n",
							"# delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"\r\n",
							"# # Read file(s) in spark data frame\r\n",
							"# sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"\r\n",
							"# # Check countRows > 0 - Stop Notebook \r\n",
							"# # CREAR DELTA TABLE DE COLOR\r\n",
							"\r\n",
							"# # Eliminar las columnas que no se desean\r\n",
							"# sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")\r\n",
							"\r\n",
							"# # Check if the Delta Table exists\r\n",
							"# if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"#     # Read the existing Delta Table\r\n",
							"#     delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"\r\n",
							"#     # Get the schema of the existing Delta table\r\n",
							"#     existing_columns = delta_table.toDF().columns\r\n",
							"    \r\n",
							"#     # AÃ±adir columna fechaDelta al DataFrame\r\n",
							"#     sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))\r\n",
							"\r\n",
							"#     # Crear el conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
							"#     update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"#     # Merge new data into existing table\r\n",
							"#     delta_table.alias(\"existing\").merge(\r\n",
							"#         source=sdf.alias(\"updates\"),\r\n",
							"#         condition=\" AND \".join(conditions_list)\r\n",
							"#     ).whenMatchedUpdate(\r\n",
							"#         condition=\" OR \".join([f\"existing.{col} != updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')]),\r\n",
							"#         set={\r\n",
							"#             \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"#             \"fechaDelta\": \"updates.fechaDelta\",  # Usar fechaDelta del DataFrame\r\n",
							"#             **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"#         }\r\n",
							"#     ).whenNotMatchedInsert(\r\n",
							"#         values={\r\n",
							"#             \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
							"#             \"fechaDelta\": fecha_delta,  # Establecer fechaDelta al insertar\r\n",
							"#             **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
							"#         }\r\n",
							"#     ).execute()\r\n",
							"# else:\r\n",
							"#     # Crear nueva tabla Delta con nuevos datos, incluyendo la columna fechaDelta\r\n",
							"#     sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga del DataFrame\r\n",
							"#     sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))  # Establecer fechaDelta al crear la tabla\r\n",
							"#     sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"# spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Actualizar CSV Configuracion**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cuenta =  'datalake1pgc'\r\n",
							"contendor = 'mdw'\r\n",
							"archivo = '/bronze/Configuration'\r\n",
							"#archivo = \r\n",
							"#archivo2 = '/bronze/Configuration/ConfiguracionOrigenes\r\n",
							"table_name = 'color'\r\n",
							"#ruta = 'abfss://%s@%s.dfs.core.windows.net/%s' % (contendor, cuenta, archivo)\r\n",
							"ruta = f'abfss://{contendor}@{cuenta}.dfs.core.windows.net/{archivo}'\r\n",
							"#ruta2 = f'abfss://{contendor}@{cuenta}.dfs.core.windows.net/{archivo}'\r\n",
							"# Leer CSV\r\n",
							"#print(ruta)\r\n",
							"df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(ruta)\r\n",
							"#Generamos fecha en formato 'yyyyMMdd'\r\n",
							"fecha_actual = date_format(current_date(), 'yyyy-MM-dd')\r\n",
							"fecha_actual = '2022-01-18'\r\n",
							"# Modificar CSV\r\n",
							"df_modificado = df.withColumn(\r\n",
							"    \"UpdateDate\",\r\n",
							"    when(df[\"Filename\"] == table_name, fecha_actual).otherwise(df[\"UpdateDate\"])\r\n",
							")\r\n",
							"# Sobreescrir CSV\r\n",
							"#df_modificado.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ruta)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df_modificado.show()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_modificado.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ruta)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/silver')]",
			"type": "Microsoft.Synapse/workspaces/databases",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"Ddls": [
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "silver",
							"EntityType": "DATABASE",
							"Origin": {
								"Type": "SPARK"
							},
							"Properties": {
								"IsSyMSCDMDatabase": true
							},
							"Source": {
								"Provider": "ADLS",
								"Location": "abfss://mdw@datalake1pgc.dfs.core.windows.net/silver/DeltaTables",
								"Properties": {
									"FormatType": "parquet",
									"LinkedServiceName": "ls_ADLG2_Bronze_Landing"
								}
							}
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "almacenes",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "silver"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "idAlmacenes",
										"OriginDataTypeName": {
											"TypeName": "integer",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"HIVE_TYPE_STRING": "integer"
											}
										}
									},
									{
										"Name": "Nombre",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "codigoAlmacen",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "idPais",
										"OriginDataTypeName": {
											"TypeName": "integer",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"HIVE_TYPE_STRING": "integer"
											}
										}
									},
									{
										"Name": "Ciudad",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "Direccion",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "Descripcion",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "fechaCarga",
										"OriginDataTypeName": {
											"TypeName": "integer",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"HIVE_TYPE_STRING": "integer"
											}
										}
									},
									{
										"Name": "fechaDelta",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat",
									"FormatType": "parquet",
									"SerializeLib": "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe",
									"Properties": {
										"path": "abfss://mdw@datalake1pgc.dfs.core.windows.net/silver/DeltaSTG/almacenes",
										"FormatTypeSetToDatabaseDefault": false
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://mdw@datalake1pgc.dfs.core.windows.net/silver/DeltaSTG/almacenes",
									"Properties": {
										"LinkedServiceName": "ls_ADLG2_Bronze_Landing",
										"LocationSetToDatabaseDefault": false
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "{\"type\":\"None\",\"level\":\"optimal\"}",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "",
								"spark.sql.sources.provider": "parquet"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "articulos",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "silver"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "idAlmacenes",
										"OriginDataTypeName": {
											"TypeName": "integer",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"HIVE_TYPE_STRING": "integer"
											}
										}
									},
									{
										"Name": "Nombre",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "codigoAlmacen",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "idPais",
										"OriginDataTypeName": {
											"TypeName": "integer",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"HIVE_TYPE_STRING": "integer"
											}
										}
									},
									{
										"Name": "Ciudad",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "Direccion",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "Descripcion",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "fechaCarga",
										"OriginDataTypeName": {
											"TypeName": "integer",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"HIVE_TYPE_STRING": "integer"
											}
										}
									},
									{
										"Name": "fechaDelta",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat",
									"FormatType": "parquet",
									"SerializeLib": "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe",
									"Properties": {
										"path": "abfss://mdw@datalake1pgc.dfs.core.windows.net/silver/DeltaSTG/almacenes",
										"FormatTypeSetToDatabaseDefault": false
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://mdw@datalake1pgc.dfs.core.windows.net/silver/DeltaSTG/almacenes",
									"Properties": {
										"LinkedServiceName": "ls_ADLG2_Bronze_Landing",
										"LocationSetToDatabaseDefault": false
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "{\"type\":\"None\",\"level\":\"optimal\"}",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "",
								"spark.sql.sources.provider": "parquet"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkTFM')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "francecentral"
		}
	]
}