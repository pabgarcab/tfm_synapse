{
	"name": "LoadSCD2AndFact",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkTFM",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "6f434cf2-a064-454d-860b-cab4ea86e55a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
				"name": "sparkTFM",
				"type": "Spark",
				"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Import modules\r\n",
					"from delta.tables import DeltaTable\r\n",
					"from delta.tables import *\r\n",
					"from pyspark.sql.functions import when, lit, current_date, date_format,concat_ws,xxhash64,current_timestamp\r\n",
					"from pyspark.sql import SparkSession,Window\r\n",
					"from pyspark.sql import functions as F"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# %%sql\r\n",
					"# USE default;\r\n",
					"# DROP DATABASE  gold CASCADE ;\r\n",
					"# USE default;\r\n",
					"# CREATE DATABASE IF NOT EXISTS gold;\r\n",
					"# USE gold;"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# %%sql\r\n",
					"# USE gold;\r\n",
					"# DROP TABLE IF EXISTS fact_Sales;"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"factList = [ \r\n",
					"        {\r\n",
					"            \"name\": \"Sales\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idDesgloseVenta AS idSales,\r\n",
					"                        d.idArticulo AS idArticles,\r\n",
					"                        c.idAlmacen AS idWarehouse,\r\n",
					"                        c.idCliente AS idClient,\r\n",
					"                        c.idCodigoPostal AS idPostalCode,\r\n",
					"                        c.idDivisa AS idCurrency,\r\n",
					"                        c.idTarifa AS idTariff,\r\n",
					"                        c.idTipoOperacion AS idOperationType,\r\n",
					"                        c.idHora AS idHours,\r\n",
					"                        c.idFecha AS idDate,\r\n",
					"                        f.fecha AS  date,\r\n",
					"                        CAST(CAST(f.fecha AS string) + ' ' + h.horaCompleta AS TIMESTAMP)AS dateOp,\r\n",
					"                        -- CAST(CONCAT(CAST(f.fecha AS STRING), ' ', CAST(h.horaCompleta AS STRING)) AS TIMESTAMP) AS dateOp\r\n",
					"                        COALESCE(c.codigoTicket, 'D') AS ticketNumber,\r\n",
					"                        COALESCE(d.Cantidad, 0) AS quantity,\r\n",
					"                        COALESCE(d.PrecioUnitario, 0)  AS unitPrice,\r\n",
					"                        COALESCE(d.CosteUnitario, 0)  AS unitCost,\r\n",
					"                        COALESCE(d.importeBruto, 0)  AS amtTotalEuros,\r\n",
					"                        COALESCE(d.importeNeto, 0)  AS amtNetEuros,\r\n",
					"                        COALESCE(d.importeDescuento, 0)  AS amtTotalEurosDiscount,\r\n",
					"                        COALESCE(d.importeNetoDescuento, 0) AS amtNetEurosDiscount,\r\n",
					"                        CASE WHEN idTarifa IN (2,3) THEN 1\r\n",
					"                            ELSE 0 \r\n",
					"                        END AS discountSale,\r\n",
					"                        CASE WHEN idTarifa = 0 THEN 0\r\n",
					"                            ELSE (d.importeBruto - d.importeDescuento) \r\n",
					"                        END AS amtTotalDiscount,\r\n",
					"                        CASE WHEN idTarifa = 0 THEN 0 \r\n",
					"                            ELSE (d.importeNeto - d.importeNetoDescuento) \r\n",
					"                        END AS amtNetDiscount,\r\n",
					"                        d.fechaCarga AS loadDate,\r\n",
					"                        d.fechaDelta AS deltaDate\r\n",
					"                    FROM default.desgloseVenta AS d\r\n",
					"                    INNER JOIN default.cabeceraVenta AS c\r\n",
					"                        ON d.idcabecerasVentas = c.idcabeceraVenta\r\n",
					"                    LEFT JOIN default.Fechas  AS f\r\n",
					"                        On f.idFechas = c.idFecha\r\n",
					"                    LEFT JOIN default.horas  AS h\r\n",
					"                        On h.idHoras = c.idHora\r\n",
					"                    ORDER BY idDesgloseVenta\r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"    ]"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dimensionsList = [ \r\n",
					"        {\r\n",
					"            \"name\": \"Articles\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idArticulos                                                                         AS idArticles,\r\n",
					"                            COALESCE(nombre, 'D')                                                               AS name,\r\n",
					"                            COALESCE(descripcion, 'D')                                                          AS description,\r\n",
					"                            COALESCE(codigoReferencia, 'D')                                                     AS externalCode,\r\n",
					"                            COALESCE(t.talla, 'D')                                                              AS size,\r\n",
					"                            COALESCE( t.numeroTalla, -1)                                                        AS numSize,\r\n",
					"                            COALESCE(co.color, 'D')                                                             AS colour,\r\n",
					"                            COALESCE(ca.categoria, 'D')                                                         AS category,\r\n",
					"                            COALESCE(l.codigoLinea, 'D')                                                        AS codLine,\r\n",
					"                            COALESCE(l.Linea, 'D')                                                              AS line, \r\n",
					"                            CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN 'OI'+CAST((a.idTemporada) AS string)\r\n",
					"                                WHEN a.idCategoria IN (2,4,6,7,9) THEN 'OI'+CAST(a.idTemporada AS string)\r\n",
					"                                ELSE 'PV'+CAST(a.idTemporada AS string)\r\n",
					"                            END                                                                                 AS season,\r\n",
					"                            a.fechaCarga                                                                        AS loadDate,\r\n",
					"                            a.fechaDelta                                                                        AS deltaDate\r\n",
					"                    FROM default.articulos AS a\r\n",
					"                    LEFT JOIN default.talla AS t\r\n",
					"                        ON t.idTalla = a.idTalla\r\n",
					"                    LEFT JOIN default.color AS co\r\n",
					"                    ON co.idColor = a.idColor\r\n",
					"                    LEFT JOIN default.categoria AS ca -- select * from default.categoria\r\n",
					"                        ON ca.idCategoria = a.idCategoria\r\n",
					"                    LEFT JOIN default.Linea AS l\r\n",
					"                        ON l.idLinea = a.idLinea\r\n",
					"                    -- where  a.fechaCarga  < '20241009'\r\n",
					"                    ORDER BY idArticulos\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Client\",\r\n",
					"            \"query\": \"\"\"              \r\n",
					"                    SELECT  idCliente AS idClient,\r\n",
					"                            COALESCE(c.nombre, 'D') AS name,\r\n",
					"                            COALESCE(apellido1, 'D') AS lastName1,\r\n",
					"                            COALESCE(apellido2, 'D') AS lastName2, \r\n",
					"                            COALESCE(email, 'D') AS email,\r\n",
					"                            COALESCE(telefono, 'D') AS phoneNumber,\r\n",
					"                            COALESCE(CAST(cumpleanos AS STRING), '1900-01-01') AS birthDay,       \r\n",
					"                            YEAR(current_date()) - YEAR(CAST(cumpleanos AS DATE)) \r\n",
					"                                - CASE \r\n",
					"                                    WHEN MONTH(current_date()) < MONTH(CAST(cumpleanos AS DATE)) \r\n",
					"                                        OR (MONTH(current_date()) = MONTH(CAST(cumpleanos AS DATE)) AND DAY(current_date()) < DAY(CAST(cumpleanos AS DATE)))\r\n",
					"                                    THEN 1\r\n",
					"                                    ELSE 0\r\n",
					"                                END AS age,\r\n",
					"                            CASE WHEN hombre = 1 THEN 'Hombre' ELSE 'Mujer' END AS gender,\r\n",
					"                            COALESCE(p.nombre, 'D') AS country,\r\n",
					"                            COALESCE(p.codigoPais, 'D') AS countryCode,\r\n",
					"                            COALESCE(cp.region, 'D') AS region,\r\n",
					"                            COALESCE(c.Direcion, 'D') AS address,  \r\n",
					"                            COALESCE(codigoPostal, 'D') AS postalCode,\r\n",
					"                            COALESCE(activo, false) AS active,  -- Cambiado 0 a false aquí\r\n",
					"                            c.fechaCarga AS loadDate,\r\n",
					"                            c.fechaDelta AS deltaDate\r\n",
					"                        FROM default.cliente AS c\r\n",
					"                        LEFT JOIN default.pais AS p ON c.idPais = p.idPais\r\n",
					"                        INNER JOIN default.codigoPostal AS cp ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\r\n",
					"                        ORDER BY idCliente\r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"        ,\r\n",
					"        {\r\n",
					"            \"name\": \"Currency\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idDivisa AS idCurrency,\r\n",
					"                            COALESCE(nombre, 'D') AS name,\r\n",
					"                            COALESCE(Divisa, 'D') AS currency,\r\n",
					"                            fechaCarga AS loadDate,\r\n",
					"                            fechaDelta AS deltaDate\r\n",
					"                    FROM default.divisa\r\n",
					"                    ORDER BY idDivisa\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Date\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idFechas AS idDate,\r\n",
					"                            ClaveFecha AS dateKey,\r\n",
					"                            Fecha AS date,\r\n",
					"                            COALESCE(Mes, 'D')  AS month,\r\n",
					"                            COALESCE(NumeroMes, -1)  AS monthNumber,\r\n",
					"                            COALESCE(Ano, -1)  AS year,\r\n",
					"                            COALESCE(NumeroSemana, -1)  AS weekNumber,\r\n",
					"                            COALESCE(DiaSemana, 'D') AS dayWeek,\r\n",
					"                            COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\r\n",
					"                            COALESCE(DiaAno, -1) AS yearDay,\r\n",
					"                            CASE WHEN Trimestre = 1 THEN 'Q1'\r\n",
					"                                WHEN Trimestre = 2 THEN 'Q2'\r\n",
					"                                WHEN Trimestre = 3 THEN 'Q3'\r\n",
					"                                ELSE '-1' END AS quarter,\r\n",
					"                            CASE WHEN Cuatrimestre = 1 THEN 'Q1'\r\n",
					"                                WHEN Cuatrimestre = 2 THEN 'Q2'\r\n",
					"                                WHEN Cuatrimestre = 3 THEN 'Q3'\r\n",
					"                                WHEN Cuatrimestre = 4 THEN 'Q4'\r\n",
					"                                ELSE '-1' \r\n",
					"                            END                                       AS quadrimester,\r\n",
					"                            CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\r\n",
					"                                WHEN Cuatrimestre IN (2,4) THEN 'S2'\r\n",
					"                                ELSE '-1' \r\n",
					"                            END                                       AS semester,\r\n",
					"                            fechaCarga AS loadDate,\r\n",
					"                            fechaDelta AS deltaDate\r\n",
					"                    FROM default.fechas\r\n",
					"                    ORDER BY idFechas\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Hours\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idHoras AS idHours,\r\n",
					"                            COALESCE(Hora, -1) AS hour,\r\n",
					"                            COALESCE(Minuto, -1) AS minute,\r\n",
					"                            COALESCE(HoraCompleta, 'D') as fullHour,\r\n",
					"                            fechaCarga AS loadDate,\r\n",
					"                            fechaDelta AS deltaDate\r\n",
					"                    FROM default.horas\r\n",
					"                    ORDER BY idHoras\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"OperationType\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idTipoOperacion AS idOperationType,\r\n",
					"                            COALESCE(Operacion, 'D') AS operation,\r\n",
					"                            fechaCarga AS loadDate,\r\n",
					"                            fechaDelta AS deltaDate\r\n",
					"                    FROM default.tipoOperacion\r\n",
					"                    ORDER BY idTipoOperacion\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"PostalCode\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idCodigoPostal AS idPostalCode,\r\n",
					"                            COALESCE(codigoPostal, 'D') AS postalCode,\r\n",
					"                            COALESCE(region, 'D') AS region,\r\n",
					"                            COALESCE(c.codigoPais, 'D') AS countryCode,\r\n",
					"                            COALESCE(nombre, 'D') AS country,\r\n",
					"                            c.fechaCarga AS loadDate,\r\n",
					"                            c.fechaDelta AS deltaDate\r\n",
					"                    FROM default.codigoPostal AS c\r\n",
					"                    LEFT JOIN default.pais AS p\r\n",
					"                        ON p.codigoPais = c.codigoPais\r\n",
					"                    ORDER BY idCodigoPostal\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Tariff\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idTarifa AS idTariff,\r\n",
					"                            COALESCE(Tarifa, 'D') AS tariff,\r\n",
					"                            fechaCarga AS loadDate,\r\n",
					"                            fechaDelta AS deltaDate       \r\n",
					"                    FROM default.tarifa\r\n",
					"                    ORDER BY idTarifa\r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"        ,\r\n",
					"        {\r\n",
					"            \"name\": \"Warehouse\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idAlmacenes AS idWarehouse,\r\n",
					"                            COALESCE(a.Nombre, 'D') AS warehouse,\r\n",
					"                            COALESCE(a.codigoAlmacen, 'D') AS externalCode,\r\n",
					"                            COALESCE(p.codigoPais, 'D') AS countryCode, \r\n",
					"                            COALESCE(p.nombre, 'D') AS country,\r\n",
					"                            COALESCE(a.ciudad, 'D') AS city,\r\n",
					"                            COALESCE(a.Direccion, 'D') AS address,\r\n",
					"                            COALESCE(a.descripcion, 'D') AS description,\r\n",
					"                            a.fechaCarga AS loadDate,\r\n",
					"                            a.fechaDelta AS deltaDate\r\n",
					"                    FROM default.almacenes AS a\r\n",
					"                    LEFT JOIN default.pais AS p\r\n",
					"                        ON p.idpais = a.idPais\r\n",
					"                    WHERE a.fechaDelta <> 20241011\r\n",
					"                    UNION \r\n",
					"                    select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','Almacén especializado en logística',20241010,20241010\r\n",
					"                    --UNION \r\n",
					"                    --select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','Almacén especializado',20241011,20241011\r\n",
					"                    ORDER BY idWarehouse\r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"    ]\r\n",
					"    "
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dimensionsList = [ \r\n",
					"        {\r\n",
					"            \"name\": \"Warehouse\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idAlmacenes AS idWarehouse,\r\n",
					"                            COALESCE(a.Nombre, 'D') AS warehouse,\r\n",
					"                            COALESCE(a.codigoAlmacen, 'D') AS externalCode,\r\n",
					"                            COALESCE(p.codigoPais, 'D') AS countryCode, \r\n",
					"                            COALESCE(p.nombre, 'D') AS country,\r\n",
					"                            COALESCE(a.ciudad, 'D') AS city,\r\n",
					"                            COALESCE(a.Direccion, 'D') AS address,\r\n",
					"                            COALESCE(a.descripcion, 'D') AS description,\r\n",
					"                            a.fechaCarga AS loadDate,\r\n",
					"                            a.fechaDelta AS deltaDate\r\n",
					"                    FROM default.almacenes AS a\r\n",
					"                    LEFT JOIN default.pais AS p\r\n",
					"                        ON p.idpais = a.idPais\r\n",
					"                    --WHERE a.fechaDelta <> 20241011 and \r\n",
					"                    where idAlmacenes = 4\r\n",
					"                    --UNION \r\n",
					"                    --select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','Almacén especializado en logística',20241010,20241010\r\n",
					"                    UNION \r\n",
					"                    select 14,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','Almacén especializado',20241011,20241011\r\n",
					"                    ORDER BY idWarehouse\r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"    ]"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Load SCD2 Dimensions**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Carga o Merge Dimensiones\r\n",
					"results = []\r\n",
					"correct_files = []\r\n",
					"incorrect_files = []\r\n",
					"exception = []\r\n",
					"\r\n",
					"for file in dimensionsList:\r\n",
					"    file_result = {\r\n",
					"        'file_name': file['name'],\r\n",
					"        'status': 'Incorrecto',\r\n",
					"        'inserted_rows': 0,\r\n",
					"        'updated_rows': 0,\r\n",
					"        'exception': 'N/A'\r\n",
					"    }\r\n",
					"\r\n",
					"    try:\r\n",
					"\r\n",
					"        if sdf.limit(1).count() == 0:\r\n",
					"            print(f\"No se procesará el archivo {file['name']} porque no contiene datos.\")\r\n",
					"            file_result['exception'] = \"Fichero vacio\"\r\n",
					"            results.append(file_result)\r\n",
					"            correct_files.append(file['name'])\r\n",
					"            continue\r\n",
					"\r\n",
					"        table_name = file[\"name\"].split('_')[0]\r\n",
					"        source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
					"        key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
					"\r\n",
					"        key_columns = key_columns_str.split(',')\r\n",
					"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
					"\r\n",
					"        delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Dimensions/{table_name}\"\r\n",
					"\r\n",
					"        # Leer archivo(s) en DataFrame de Spark\r\n",
					"        df = spark.sql(file[\"query\"])\r\n",
					"        sdf = df.cache()\r\n",
					"\r\n",
					"        \r\n",
					"        if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
					"            # Añadir las columnas necesarias\r\n",
					"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
					"\r\n",
					"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fromDate', 'toDate', 'isCurrent')]\r\n",
					"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
					"\r\n",
					"            todas_las_columnas = sdf.columns\r\n",
					"            columnas_al_principio = [f\"id{table_name}\"]\r\n",
					"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
					"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
					"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
					"            sdf = sdf.select(*nuevo_orden_columnas)\r\n",
					"\r\n",
					"\r\n",
					"            sdf.createOrReplaceTempView(\"temp_view\")\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                MERGE INTO gold.dim_{table_name}  \r\n",
					"                AS existing\r\n",
					"                USING temp_view AS updates\r\n",
					"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
					"                WHEN MATCHED AND existing.isCurrent = 1 AND existing.hash != updates.hash THEN\r\n",
					"                UPDATE SET\r\n",
					"                    existing.toDate = current_date(),\r\n",
					"                    existing.isCurrent = 0\r\n",
					"            \"\"\")\r\n",
					"\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                INSERT INTO gold.dim_{table_name}    \r\n",
					"                SELECT \r\n",
					"                    {next_surrogate_key} + row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS sk{table_name}, \r\n",
					"                    updates.*\r\n",
					"                FROM temp_view AS updates\r\n",
					"                LEFT JOIN gold.dim_{table_name}  \r\n",
					"                AS existing\r\n",
					"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
					"                WHERE existing.{key_columns_str} IS NULL OR existing.isCurrent = 0 OR existing.hash != updates.hash\r\n",
					"            \"\"\")\r\n",
					"\r\n",
					"            # Almacenar conteos de filas\r\n",
					"            file_result['status'] = 'Correcto'\r\n",
					"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.dim_{table_name}\")\r\n",
					"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
					"            last_result = last_merge_operation.collect()\r\n",
					"\r\n",
					"            file_result['inserted_rows'] = last_result[0].operationMetrics[\"numTargetRowsInserted\"]\r\n",
					"            file_result['updated_rows'] = last_result[0].operationMetrics[\"numTargetRowsUpdated\"]\r\n",
					"\r\n",
					"            correct_files.append(file['name'])\r\n",
					"\r\n",
					"        else: # Crear nueva tabla Delta\r\n",
					"      \r\n",
					"            last_surrogate_key = 0  # Iniciar con 0 si la tabla no existe\r\n",
					"            #sdf = sdf.withColumn(f\"sk{table_name}\", F.monotonically_increasing_id() + last_surrogate_key + 1)  # Asigna un valor único a cada fila.\r\n",
					"            sdf = sdf.withColumn(f\"sk{table_name}\", F.col(f\"id{table_name}\"))\r\n",
					"            # Añadir las columnas necesarias\r\n",
					"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
					"\r\n",
					"            # Hash\r\n",
					"            columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\" ,'fechaCarga', 'fechaDelta','fromDate', 'toDate', 'isCurrent')]\r\n",
					"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
					"\r\n",
					"            # Reorganizar la estructura de la tabla\r\n",
					"            todas_las_columnas = sdf.columns\r\n",
					"            columnas_al_principio = [f\"sk{table_name}\", f\"id{table_name}\"]\r\n",
					"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
					"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
					"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
					"            sdf_reordenado = sdf.select(*nuevo_orden_columnas)\r\n",
					"            \r\n",
					"            # Crear la tabla Delta\r\n",
					"            sdf_reordenado.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path)\r\n",
					"\r\n",
					"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.dim_{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
					"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
					"            #spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
					"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
					"\r\n",
					"            #spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')\")\r\n",
					"\r\n",
					"            # Almacenar archivo procesado correctamente\r\n",
					"            file_result['status'] = 'Correcto'\r\n",
					"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
					"            file_result['updated_rows'] = 0\r\n",
					"            correct_files.append(file['name'])\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
					"        file_result['exception'] = f\"{e}\"\r\n",
					"        results.append(file_result)\r\n",
					"        incorrect_files.append(file['name'])\r\n",
					"        continue\r\n",
					"\r\n",
					"    results.append(file_result)\r\n",
					"\r\n",
					"# Convertir resultados a DataFrame\r\n",
					"results_df = spark.createDataFrame(results)\r\n",
					"\r\n",
					"# Ruta donde quieres guardar el archivo\r\n",
					"output_path = \"/gold/Logs/log_prueba_20241020.txt\"\r\n",
					"\r\n",
					"print(\"======================\")\r\n",
					"# Imprimir resultados en consola\r\n",
					"for result in results:\r\n",
					"    print(f\"Archivo: {result['file_name']}, Estado: {result['status']}, Filas insertadas: {result['inserted_rows']}, Filas actualizadas: {result['updated_rows']}, Excepción: {result['exception']}\")\r\n",
					"\r\n",
					"# Confirmación\r\n",
					"print(f\"Resultados guardados en: {output_path}\")\r\n",
					"\r\n",
					"# Guardar nombres de archivos correctos e incorrectos en variables de la pipeline\r\n",
					"correct_files_variable = correct_files\r\n",
					"incorrect_files_variable = incorrect_files\r\n",
					"\r\n",
					"# Si necesitas ver las variables\r\n",
					"print(\"Archivos correctos:\", correct_files_variable)\r\n",
					"print(\"Archivos incorrectos:\", incorrect_files_variable)\r\n",
					""
				],
				"execution_count": 32
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Load Fact Table**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Carga FULL tabla/s de hechos\r\n",
					"\r\n",
					"# Inicializar listas para almacenar resultados\r\n",
					"results = []\r\n",
					"correct_files = []\r\n",
					"incorrect_files = []\r\n",
					"exception = []\r\n",
					"\r\n",
					"\r\n",
					"for file in factList:\r\n",
					"    file_result = {\r\n",
					"        'file_name': file['name'],\r\n",
					"        'status': 'Incorrecto',\r\n",
					"        'inserted_rows': 0,\r\n",
					"        'updated_rows': 0,\r\n",
					"        'exception': 'N/A'\r\n",
					"    }\r\n",
					"    table_name = file[\"name\"].split('_')[0]\r\n",
					"\r\n",
					"    # Definir la ruta de la tabla Delta\r\n",
					"    delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Fact/{table_name}\"\r\n",
					"    df = spark.sql(file[\"query\"])\r\n",
					"    sdf = df.cache()\r\n",
					"\r\n",
					"    try:\r\n",
					"        # Leer el archivo CSV en un DataFrame de Spark\r\n",
					"        #sdf = spark.read.csv(f\"gs://your-bucket/{file['name']}\", header=True, inferSchema=True)\r\n",
					"\r\n",
					"        # Verificar si el DataFrame está vacío\r\n",
					"        if sdf.count() == 0:\r\n",
					"            print(f\"No se procesará el archivo {file['name']} porque no contiene datos.\")\r\n",
					"            file_result['exception'] = \"Fichero vacio\"\r\n",
					"            results.append(file_result)\r\n",
					"            correct_files.append(file['name'])\r\n",
					"            continue\r\n",
					"\r\n",
					"        # Añadir columnas necesarias\r\n",
					"        # sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss')) \\\r\n",
					"        #          .withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss')) \\\r\n",
					"        #          .withColumn(\"isCurrent\", F.lit(1))\r\n",
					"\r\n",
					"        # Truncar la tabla Delta si existe\r\n",
					"        if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
					"            delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
					"            delta_table.delete()  # Truncar la tabla Delta\r\n",
					"\r\n",
					"            sdf.createOrReplaceTempView(\"temp_view\")\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                    INSERT INTO gold.fact_{table_name}\r\n",
					"                     SELECT\r\n",
					"                        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n",
					"                        -1, \r\n",
					"                            CAST('1990-01-01' AS timestamp), CAST('1990-01-01' AS timestamp),\r\n",
					"                        'D', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n",
					"                        19900101,19900101 \r\n",
					"            \"\"\")\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                INSERT INTO gold.fact_{table_name}  \r\n",
					"                SELECT ROW_NUMBER() OVER(ORDER BY dateOP), t.* from temp_view as t\r\n",
					"                \"\"\")\r\n",
					"        else:\r\n",
					"\r\n",
					"            window_spec = Window.orderBy(\"dateOp\", \"idArticles\")\r\n",
					"            sdf = sdf.withColumn(f\"sk{table_name}\", F.row_number().over(window_spec))\r\n",
					"            todas_las_columnas = sdf.columns\r\n",
					"            columnas_al_principio = [f\"sk{table_name}\"]  # Suponiendo que la tabla tiene una columna idWarehouse\r\n",
					"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio]\r\n",
					"            nuevo_orden_columnas = columnas_al_principio + otras_columnas\r\n",
					"            sdf = sdf.select(*nuevo_orden_columnas)\r\n",
					"\r\n",
					"        # Cargar datos en la tabla Delta\r\n",
					"            sdf.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\r\n",
					"        #sdf.write.format(\"delta\").mode(\"append\").save(delta_table_path)  # valorar carga incremental más adelante\r\n",
					"\r\n",
					"            # Crear la tabla Delta en caso de que no exista\r\n",
					"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.fact_{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
					"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
					"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                    INSERT INTO gold.fact_{table_name}\r\n",
					"                     SELECT\r\n",
					"                        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n",
					"                        -1,\r\n",
					"                        CAST('1990-01-01' AS timestamp), CAST('1990-01-01' AS timestamp),\r\n",
					"                        'D', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n",
					"                        19900101,19900101 \r\n",
					"            \"\"\")\r\n",
					"        # Almacenar conteos de filas\r\n",
					"        file_result['status'] = 'Correcto'\r\n",
					"        file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
					"        correct_files.append(file['name'])\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
					"        file_result['exception'] = f\"{e}\"\r\n",
					"        results.append(file_result)\r\n",
					"        incorrect_files.append(file['name'])\r\n",
					"        continue\r\n",
					"\r\n",
					"    results.append(file_result)\r\n",
					"\r\n",
					"# Convertir resultados a DataFrame\r\n",
					"results_df = spark.createDataFrame(results)\r\n",
					"\r\n",
					"# Ruta donde quieres guardar el archivo\r\n",
					"output_path = \"/gold/Logs/log_prueba_truncado_carga.txt\"\r\n",
					"\r\n",
					"print(\"======================\")\r\n",
					"# Imprimir resultados en consola\r\n",
					"for result in results:\r\n",
					"    print(f\"Archivo: {result['file_name']}, Estado: {result['status']}, Filas insertadas: {result['inserted_rows']}, Filas actualizadas: {result['updated_rows']}, Excepción: {result['exception']}\")\r\n",
					"\r\n",
					"# Confirmación\r\n",
					"print(f\"Resultados guardados en: {output_path}\")\r\n",
					"\r\n",
					"# Guardar nombres de archivos correctos e incorrectos en variables de la pipeline\r\n",
					"correct_files_variable = correct_files\r\n",
					"incorrect_files_variable = incorrect_files\r\n",
					"\r\n",
					"# Si necesitas ver las variables\r\n",
					"print(\"Archivos correctos:\", correct_files_variable)\r\n",
					"print(\"Archivos incorrectos:\", incorrect_files_variable)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"        # # Comprobar si la tabla Delta existe\r\n",
					"        # if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
					"        #     delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
					"        #     existing_columns = delta_table.toDF().columns\r\n",
					"            \r\n",
					"        #     # Obtener la última surrogate key para crear nuevas\r\n",
					"        #     last_surrogate_key = delta_table.toDF().agg(F.max(f\"sk{table_name}\")).collect()[0][0] or 0\r\n",
					"        #     next_surrogate_key = last_surrogate_key + 1\r\n",
					"            \r\n",
					"        #     # Merge new data into existing table con lógica SCD Tipo 2\r\n",
					"        #     delta_table.alias(\"existing\").merge(\r\n",
					"        #         source=sdf.alias(\"updates\"),\r\n",
					"        #         condition=\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])\r\n",
					"        #     ).whenMatchedUpdate(\r\n",
					"        #         condition=\"existing.isCurrent = 1 AND existing.hash != updates.hash\",\r\n",
					"        #         set={\r\n",
					"        #             \"toDate\": F.current_date(),\r\n",
					"        #             \"isCurrent\": F.lit(0)\r\n",
					"        #         }\r\n",
					"        #     ).whenNotMatchedInsert(\r\n",
					"        #         values={\r\n",
					"        #             f\"sk{table_name}\": F.expr(f\"{next_surrogate_key} + row_number() over (order by (select null)) - 1\"),  # Aumenta la surrogate key en base a la fila\r\n",
					"        #             **{f\"updates.{col}\": f\"updates.{col}\" for col in existing_columns}\r\n",
					"        #         }\r\n",
					"        #     ).execute()\r\n",
					""
				],
				"execution_count": null
			}
		]
	}
}