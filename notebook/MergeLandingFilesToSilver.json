{
	"name": "MergeLandingFilesToSilver",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkTFM",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b612374a-198e-444d-9b5e-c422c3bd5c12"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
				"name": "sparkTFM",
				"type": "Spark",
				"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# # path of the data lake container (bronze and silver for this example)\r\n",
					"# data_lake_container = 'abfss://mdw@datalake1pgc.dfs.core.windows.net'\r\n",
					"# # The ingestion folder where your parquet file are located\r\n",
					"# bronze_folder = 'bronze/Landing'\r\n",
					"# # The silver folder where your Delta Tables will be stored\r\n",
					"# silver_folder = 'silver'\r\n",
					"# # The name of the table\r\n",
					"# table_name = 'color'\r\n",
					"# # The wildcard filter used within the bronze folder to find files\r\n",
					"# source_wildcard = 'color*.parquet'\r\n",
					"# # A comma separated string of one or more key columns (for the merge)\r\n",
					"# key_columns_str = 'idColor'"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Carga de ficheros parquet Landing sobre tablas Delta capa Silver**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import modules\r\n",
					"from delta.tables import DeltaTable\r\n",
					"from delta.tables import *\r\n",
					"from pyspark.sql.functions import when, lit, current_date, date_format\r\n",
					"from pyspark.sql import SparkSession"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Prueba::\r\n",
					"\r\n",
					"# Supongamos que tienes las variables necesarias\r\n",
					"# Fecha actual en formato yyyymmdd\r\n",
					"fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
					"#fecha_delta = '20241011'\r\n",
					"# Convert comma separated string with keys to array\r\n",
					"key_columns = key_columns_str.split(',')\r\n",
					"\r\n",
					"# Convert array with keys to where-clause for merge statement\r\n",
					"conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
					"\r\n",
					"# Determine path of source files from ingest layer\r\n",
					"source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
					"\r\n",
					"# Determine path of Delta Lake Table \r\n",
					"delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
					"\r\n",
					"# Read file(s) in spark data frame\r\n",
					"sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
					"\r\n",
					"# Check countRows > 0 - Stop Notebook \r\n",
					"if sdf.count() == 0:\r\n",
					"    print(\"no procesar\")\r\n",
					"else:\r\n",
					"    # Eliminar las columnas que no se desean\r\n",
					"    sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")\r\n",
					"\r\n",
					"    # Check if the Delta Table exists\r\n",
					"    if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
					"        # Read the existing Delta Table\r\n",
					"        delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
					"\r\n",
					"        # Get the schema of the existing Delta table\r\n",
					"        existing_columns = delta_table.toDF().columns\r\n",
					"        \r\n",
					"        # Añadir columna fechaDelta al DataFrame\r\n",
					"        sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))\r\n",
					"\r\n",
					"        # Crear el conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
					"        update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
					"\r\n",
					"        # Merge new data into existing table\r\n",
					"        delta_table.alias(\"existing\").merge(\r\n",
					"            source=sdf.alias(\"updates\"),\r\n",
					"            condition=\" AND \".join(conditions_list)\r\n",
					"        ).whenMatchedUpdate(\r\n",
					"            condition=\" OR \".join([f\"existing.{col} != updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')]),\r\n",
					"            set={\r\n",
					"                \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
					"                \"fechaDelta\": \"updates.fechaDelta\",  # Usar fechaDelta del DataFrame\r\n",
					"                **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
					"            }\r\n",
					"        ).whenNotMatchedInsert(\r\n",
					"            values={\r\n",
					"                \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
					"                \"fechaDelta\": fecha_delta,  # Establecer fechaDelta al insertar\r\n",
					"                **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
					"            }\r\n",
					"        ).execute()\r\n",
					"    else:\r\n",
					"        # Crear nueva tabla Delta con nuevos datos, incluyendo la columna fechaDelta\r\n",
					"        sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga del DataFrame\r\n",
					"        sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))  # Establecer fechaDelta al crear la tabla\r\n",
					"        sdf.write.format('delta').save(delta_table_path)\r\n",
					"\r\n",
					"spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Crear manualmente Delta Tables**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # path of the data lake container (bronze and silver for this example)\r\n",
					"# data_lake_container = 'abfss://mdw@datalake1pgc.dfs.core.windows.net'\r\n",
					"# # The ingestion folder where your parquet file are located\r\n",
					"# bronze_folder = 'bronze/Landing/'\r\n",
					"# # The silver folder where your Delta Tables will be stored\r\n",
					"# silver_folder = 'silver/DeltaSTG'\r\n",
					"# # The name of the table\r\n",
					"#table_name = 'lineaventa'\r\n",
					"# # The wildcard filter used within the bronze folder to find files\r\n",
					"# #source_wildcard = 'color*.parquet'\r\n",
					"# # A comma separated string of one or more key columns (for the merge)\r\n",
					"# #key_columns_str = 'idColor'\r\n",
					"\r\n",
					"# #source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
					"\r\n",
					"# # Determine path of Delta Lake Table \r\n",
					"# delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
					"#spark.sql(f'DROP TABLE default.{table_name}')\r\n",
					"# spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ### Copia:\r\n",
					"\r\n",
					"# # Supongamos que tienes las variables necesarias\r\n",
					"# # Fecha actual en formato yyyymmdd\r\n",
					"# fecha_delta = date_format(current_date(), 'yyyyMMdd')\r\n",
					"# #fecha_delta = '20241011'\r\n",
					"# # Convert comma separated string with keys to array\r\n",
					"# key_columns = key_columns_str.split(',')\r\n",
					"\r\n",
					"# # Convert array with keys to where-clause for merge statement\r\n",
					"# conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
					"\r\n",
					"# # Determine path of source files from ingest layer\r\n",
					"# source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard\r\n",
					"\r\n",
					"# # Determine path of Delta Lake Table \r\n",
					"# delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
					"\r\n",
					"# # Read file(s) in spark data frame\r\n",
					"# sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
					"\r\n",
					"# # Check countRows > 0 - Stop Notebook \r\n",
					"# # CREAR DELTA TABLE DE COLOR\r\n",
					"\r\n",
					"# # Eliminar las columnas que no se desean\r\n",
					"# sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")\r\n",
					"\r\n",
					"# # Check if the Delta Table exists\r\n",
					"# if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
					"#     # Read the existing Delta Table\r\n",
					"#     delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
					"\r\n",
					"#     # Get the schema of the existing Delta table\r\n",
					"#     existing_columns = delta_table.toDF().columns\r\n",
					"    \r\n",
					"#     # Añadir columna fechaDelta al DataFrame\r\n",
					"#     sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))\r\n",
					"\r\n",
					"#     # Crear el conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
					"#     update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
					"\r\n",
					"#     # Merge new data into existing table\r\n",
					"#     delta_table.alias(\"existing\").merge(\r\n",
					"#         source=sdf.alias(\"updates\"),\r\n",
					"#         condition=\" AND \".join(conditions_list)\r\n",
					"#     ).whenMatchedUpdate(\r\n",
					"#         condition=\" OR \".join([f\"existing.{col} != updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')]),\r\n",
					"#         set={\r\n",
					"#             \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
					"#             \"fechaDelta\": \"updates.fechaDelta\",  # Usar fechaDelta del DataFrame\r\n",
					"#             **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
					"#         }\r\n",
					"#     ).whenNotMatchedInsert(\r\n",
					"#         values={\r\n",
					"#             \"fechaCarga\": \"updates.fechaCarga\",  # Usar fechaCarga del DataFrame\r\n",
					"#             \"fechaDelta\": fecha_delta,  # Establecer fechaDelta al insertar\r\n",
					"#             **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}  # Incluir otros campos\r\n",
					"#         }\r\n",
					"#     ).execute()\r\n",
					"# else:\r\n",
					"#     # Crear nueva tabla Delta con nuevos datos, incluyendo la columna fechaDelta\r\n",
					"#     sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga del DataFrame\r\n",
					"#     sdf = sdf.withColumn(\"fechaDelta\", lit(fecha_delta))  # Establecer fechaDelta al crear la tabla\r\n",
					"#     sdf.write.format('delta').save(delta_table_path)\r\n",
					"\r\n",
					"# spark.sql(f'CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Actualizar CSV Configuracion**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# cuenta =  'datalake1pgc'\r\n",
					"# contendor = 'mdw'\r\n",
					"# archivo = '/bronze/Configuration/ConfiguracionOrigenes.csv'\r\n",
					"\r\n",
					"# #ruta = 'abfss://%s@%s.dfs.core.windows.net/%s' % (contendor, cuenta, archivo)\r\n",
					"# ruta = f'abfss://{contendor}@{cuenta}.dfs.core.windows.net/{archivo}'\r\n",
					"# # Leer CSV\r\n",
					"# #print(ruta)\r\n",
					"# df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(ruta)\r\n",
					"# #Generamos fecha en formato 'yyyyMMdd'\r\n",
					"# fecha_actual = date_format(current_date(), 'yyyy-MM-dd')\r\n",
					"# fecha_actual = '2022-01-17'\r\n",
					"# # Modificar CSV\r\n",
					"# df_modificado = df.withColumn(\r\n",
					"#     \"UpdateDate\",\r\n",
					"#     when(df[\"Filename\"] == table_name, fecha_actual).otherwise(df[\"UpdateDate\"])\r\n",
					"# )\r\n",
					"# # Sobreescrir CSV\r\n",
					"# #df_modificado.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ruta)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#df_modificado.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ruta)\r\n",
					""
				],
				"execution_count": null
			}
		]
	}
}