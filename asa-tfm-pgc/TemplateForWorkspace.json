{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Nombre del Ã¡rea de trabajo",
			"defaultValue": "asa-tfm-pgc"
		},
		"asa-tfm-pgc-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"connectionString\"de \"asa-tfm-pgc-WorkspaceDefaultSqlServer\"",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:asa-tfm-pgc.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"ls_COLI_ERP_password": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"password\"de \"ls_COLI_ERP\""
		},
		"asa-tfm-pgc-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalake1pgc.dfs.core.windows.net"
		},
		"ls_ADLG2_Bronze_Landing_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalake1pgc.dfs.core.windows.net/"
		},
		"ls_COLI_ERP_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "@{linkedService().NombreServidor}"
		},
		"ls_COLI_ERP_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "@{linkedService().NombreBaseDatos}"
		},
		"ls_COLI_ERP_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "sa"
		},
		"ls_Severless_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "asa-tfm-pgc-ondemand.sql.azuresynapse.net"
		},
		"ls_Severless_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "@{linkedService().DBName}"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/0 - Get ExecutionID')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Set day",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set month",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "day",
							"value": {
								"value": "@formatDateTime(utcnow(), 'dd')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set hour",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set day2",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "hour",
							"value": {
								"value": "@formatDateTime(utcnow(), 'HHmmss')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set year",
						"type": "SetVariable",
						"dependsOn": [],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "pipelineReturnValue",
							"value": [
								{
									"key": "year",
									"value": {
										"type": "Expression",
										"content": "@formatDateTime(utcnow(), 'yyyy')"
									}
								}
							],
							"setSystemVariable": true
						}
					},
					{
						"name": "Set month",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set year",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "pipelineReturnValue",
							"value": [
								{
									"key": "month",
									"value": {
										"type": "Expression",
										"content": "@formatDateTime(utcnow(), 'MM')"
									}
								}
							],
							"setSystemVariable": true
						}
					},
					{
						"name": "Set day2",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set day",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "pipelineReturnValue",
							"value": [
								{
									"key": "day",
									"value": {
										"type": "Expression",
										"content": "@formatDateTime(utcnow(), 'dd')"
									}
								}
							],
							"setSystemVariable": true
						}
					},
					{
						"name": "Set hour2",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set hour",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "pipelineReturnValue",
							"value": [
								{
									"key": "hour",
									"value": {
										"type": "Expression",
										"content": "@formatDateTime(utcnow(), 'HHmmss')"
									}
								}
							],
							"setSystemVariable": true
						}
					},
					{
						"name": "Set executionID",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set hour2",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "pipelineReturnValue",
							"value": [
								{
									"key": "executionID",
									"value": {
										"type": "Expression",
										"content": "@concat(variables('day'), '_', variables('hour'))"
									}
								}
							],
							"setSystemVariable": true
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"variables": {
					"year": {
						"type": "String"
					},
					"month": {
						"type": "String"
					},
					"day": {
						"type": "String"
					},
					"hour": {
						"type": "String"
					},
					"executionID": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/1 - SqlToLanding')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "List of Tables to Load",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "csvConfiguration",
								"type": "DatasetReference",
								"parameters": {
									"NombreCSV": "ConfiguracionOrigenes.csv"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Save Landing Files",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "List of Tables to Load",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('List of tables to load').output.value",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Active For Load",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@not(equals(trim(item().Load), 'N'))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Load Landing",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "SqlServerSource",
														"sqlReaderQuery": {
															"value": "SELECT *, @{formatDateTime(pipeline().TriggerTime,'yyyyMMdd')} AS fechaCarga, '@{pipeline().RunId}' AS pipelineID \nFROM @{item().SchemaName}.@{item().TableName} \nWHERE @{item().IncrementalColumn} >= CAST(CAST('@{item().UpdateDate}' AS nvarchar) AS datetime2);",
															"type": "Expression"
														},
														"queryTimeout": "02:00:00",
														"partitionOption": "None"
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings",
															"copyBehavior": "FlattenHierarchy"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"parallelCopies": 1,
													"dataIntegrationUnits": 4,
													"translator": {
														"type": "TabularTranslator",
														"typeConversion": true,
														"typeConversionSettings": {
															"allowDataTruncation": true,
															"treatBooleanAsNumber": false
														}
													}
												},
												"inputs": [
													{
														"referenceName": "dsSQLGenerico",
														"type": "DatasetReference",
														"parameters": {
															"NombreServidor": "@item().ServerName",
															"NombreBD": "@item().DataBaseName",
															"NombreEsquema": "@item().SchemaName",
															"NombreTabla": "@item().TableName",
															"FechaActualizacion": "@item().UpdateDate",
															"ColumnaIncremental": {
																"value": "@item().IncrementalColumn",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "dsParquetRaw",
														"type": "DatasetReference",
														"parameters": {
															"NombreFichero": "@concat( item().FileName, '_', formatDateTime(utcnow(), 'yyyyMMdd'),'.', item().Format)"
														}
													}
												]
											}
										]
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-09-16T00:20:34Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/csvConfiguration')]",
				"[concat(variables('workspaceId'), '/datasets/dsSQLGenerico')]",
				"[concat(variables('workspaceId'), '/datasets/dsParquetRaw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/2 - BronzeToSilver')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get Metadata Landing",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "connectionString",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "dsRawEntidades",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "ParquetReadSettings"
							}
						}
					},
					{
						"name": "UPSERT Silver",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "landingFiles",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "MergeLandingFilesToSilver",
								"type": "NotebookReference"
							},
							"parameters": {
								"data_lake_container": {
									"value": {
										"value": "@pipeline().parameters.ADLG2",
										"type": "Expression"
									},
									"type": "string"
								},
								"silver_folder": {
									"value": {
										"value": "@pipeline().parameters.SilverFolder",
										"type": "Expression"
									},
									"type": "string"
								},
								"landing_files": {
									"value": {
										"value": "@string(variables('landingFiles'))",
										"type": "Expression"
									},
									"type": "string"
								},
								"connection_string": {
									"value": {
										"value": "@variables('connectionString')",
										"type": "Expression"
									},
									"type": "string"
								},
								"bronze_error": {
									"value": {
										"value": "@pipeline().parameters.BronzeFolderError",
										"type": "Expression"
									},
									"type": "string"
								},
								"bronze_processed": {
									"value": {
										"value": "@pipeline().parameters.BronzeFolderProcessed",
										"type": "Expression"
									},
									"type": "string"
								},
								"bronze_logs": {
									"value": {
										"value": "@pipeline().parameters.BronzeFolderLogs",
										"type": "Expression"
									},
									"type": "string"
								},
								"silver_logs": {
									"value": {
										"value": "@pipeline().parameters.SilverFolderLogs",
										"type": "Expression"
									},
									"type": "string"
								},
								"container_name": {
									"value": {
										"value": "@pipeline().parameters.ContainerName",
										"type": "Expression"
									},
									"type": "string"
								},
								"bronze_pending": {
									"value": {
										"value": "@pipeline().parameters.BronzeFolderPending",
										"type": "Expression"
									},
									"type": "string"
								},
								"metadata_folder": {
									"value": {
										"value": "@pipeline().parameters.MetadataFolder",
										"type": "Expression"
									},
									"type": "string"
								},
								"bronze_landing": {
									"value": {
										"value": "@pipeline().parameters.BronzeFolderLanding",
										"type": "Expression"
									},
									"type": "string"
								},
								"year": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'yyyy')",
										"type": "Expression"
									},
									"type": "string"
								},
								"month": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'MM')",
										"type": "Expression"
									},
									"type": "string"
								},
								"day": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'dd')",
										"type": "Expression"
									},
									"type": "string"
								},
								"hour": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'HHmmss')",
										"type": "Expression"
									},
									"type": "string"
								},
								"executionID": {
									"value": {
										"value": "@pipeline().parameters.executionID",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkTFM",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Small",
							"numExecutors": 2
						}
					},
					{
						"name": "landingFiles",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get Metadata Landing",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "landingFiles",
							"value": {
								"value": "@activity('Get Metadata Landing').output.childItems",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Get ConnectionString",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "dsConnectionString",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "connectionString",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get ConnectionString",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "connectionString",
							"value": {
								"value": "@activity('Get ConnectionString').output.value[0].conn\n",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"ADLG2": {
						"type": "string",
						"defaultValue": "abfss://mdw@datalake1pgc.dfs.core.windows.net"
					},
					"BronzeFolderPending": {
						"type": "string",
						"defaultValue": "mdw/bronze/Landing/Pending"
					},
					"SilverFolder": {
						"type": "string",
						"defaultValue": "silver/DeltaTables"
					},
					"BronzeFolderError": {
						"type": "string",
						"defaultValue": "mdw/bronze/Landing/Error"
					},
					"BronzeFolderProcessed": {
						"type": "string",
						"defaultValue": "mdw/bronze/Processed"
					},
					"BronzeFolderLogs": {
						"type": "string",
						"defaultValue": "mdw/bronze/Logs"
					},
					"SilverFolderLogs": {
						"type": "string",
						"defaultValue": "mdw/silver/Logs"
					},
					"ContainerName": {
						"type": "string",
						"defaultValue": "mdw"
					},
					"MetadataFolder": {
						"type": "string",
						"defaultValue": "mdw/bronze/Configuration"
					},
					"BronzeFolderLanding": {
						"type": "string",
						"defaultValue": "bronze/Landing/Pending"
					},
					"year": {
						"type": "string"
					},
					"month": {
						"type": "string"
					},
					"day": {
						"type": "string"
					},
					"hour": {
						"type": "string"
					},
					"executionID": {
						"type": "string"
					}
				},
				"variables": {
					"landingFiles": {
						"type": "Array",
						"defaultValue": [
							"a"
						]
					},
					"connectionString": {
						"type": "String",
						"defaultValue": "a"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsRawEntidades')]",
				"[concat(variables('workspaceId'), '/notebooks/MergeLandingFilesToSilver')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkTFM')]",
				"[concat(variables('workspaceId'), '/datasets/dsConnectionString')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/3 - Load SCD2 and Fact')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "LoadSCD2DimsAndFact",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "tablesToLoad",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "LoadSCD2AndFact",
								"type": "NotebookReference"
							},
							"parameters": {
								"data_lake_container": {
									"value": {
										"value": "@pipeline().parameters.ADLG2",
										"type": "Expression"
									},
									"type": "string"
								},
								"connection_string": {
									"value": {
										"value": "@variables('connectionString')",
										"type": "Expression"
									},
									"type": "string"
								},
								"gold_folder_logs": {
									"value": {
										"value": "@pipeline().parameters.GoldFolderLogs",
										"type": "Expression"
									},
									"type": "string"
								},
								"container_name": {
									"value": {
										"value": "@pipeline().parameters.ContainerName",
										"type": "Expression"
									},
									"type": "string"
								},
								"metadata_folder": {
									"value": {
										"value": "@pipeline().parameters.MetadataFolder",
										"type": "Expression"
									},
									"type": "string"
								},
								"tables_to_load": {
									"value": {
										"value": "@string(variables('tablesToLoad'))",
										"type": "Expression"
									},
									"type": "string"
								},
								"golden_folder_dimensions": {
									"value": {
										"value": "@pipeline().parameters.GoldenFolderDimensions",
										"type": "Expression"
									},
									"type": "string"
								},
								"golden_folder_fact": {
									"value": {
										"value": "@pipeline().parameters.GoldenFolderfact",
										"type": "Expression"
									},
									"type": "string"
								},
								"year": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'yyyy')",
										"type": "Expression"
									},
									"type": "string"
								},
								"month": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'MM')",
										"type": "Expression"
									},
									"type": "string"
								},
								"day": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'dd')",
										"type": "Expression"
									},
									"type": "string"
								},
								"hour": {
									"value": {
										"value": "@formatDateTime(utcnow(), 'HHmmss')",
										"type": "Expression"
									},
									"type": "string"
								},
								"executionID": {
									"value": {
										"value": "@pipeline().parameters.executionID",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkTFM",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Small",
							"numExecutors": 2
						}
					},
					{
						"name": "Search ActiveForLoad",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "connectionString",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "csvConfiguration",
								"type": "DatasetReference",
								"parameters": {
									"NombreCSV": "ConfiguracionReporting.csv"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "tablesToLoad",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Search ActiveForLoad",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "tablesToLoad",
							"value": {
								"value": "@activity('Search ActiveForLoad').output.value",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Get ConnectionString",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"dataset": {
								"referenceName": "dsConnectionString",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "connectionString",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get ConnectionString",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "connectionString",
							"value": {
								"value": "@activity('Get ConnectionString').output.value[0].conn\n",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"ADLG2": {
						"type": "string",
						"defaultValue": "abfss://mdw@datalake1pgc.dfs.core.windows.net"
					},
					"GoldFolderLogs": {
						"type": "string",
						"defaultValue": "gold/Logs"
					},
					"ContainerName": {
						"type": "string",
						"defaultValue": "mdw"
					},
					"MetadataFolder": {
						"type": "string",
						"defaultValue": "mdw/bronze/Configuration"
					},
					"GoldenFolderDimensions": {
						"type": "string",
						"defaultValue": "gold/Dimensions"
					},
					"GoldenFolderfact": {
						"type": "string",
						"defaultValue": "gold/Fact"
					},
					"year": {
						"type": "string"
					},
					"month": {
						"type": "string"
					},
					"day": {
						"type": "string"
					},
					"hour": {
						"type": "string"
					},
					"executionID": {
						"type": "string"
					}
				},
				"variables": {
					"tablesToLoad": {
						"type": "Array",
						"defaultValue": [
							"a"
						]
					},
					"connectionString": {
						"type": "String",
						"defaultValue": "a"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/LoadSCD2AndFact')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkTFM')]",
				"[concat(variables('workspaceId'), '/datasets/csvConfiguration')]",
				"[concat(variables('workspaceId'), '/datasets/dsConnectionString')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Orchestrator')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Bronze",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Get exeuctionID",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "1 - SqlToLanding",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "Silver",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Bronze",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "2 - BronzeToSilver",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"executionID": {
									"value": "@activity('Get exeuctionID').output.pipelineReturnValue.executionID",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "Gold",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Silver",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "3 - Load SCD2 and Fact",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"executionID": {
									"value": "@activity('Get exeuctionID').output.pipelineReturnValue.executionID",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "Get exeuctionID",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"policy": {
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "0 - Get ExecutionID",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/1 - SqlToLanding')]",
				"[concat(variables('workspaceId'), '/pipelines/2 - BronzeToSilver')]",
				"[concat(variables('workspaceId'), '/pipelines/3 - Load SCD2 and Fact')]",
				"[concat(variables('workspaceId'), '/pipelines/0 - Get ExecutionID')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csvConfiguration')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreCSV": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreCSV",
							"type": "Expression"
						},
						"folderPath": "bronze/Configuration",
						"fileSystem": "mdw"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ServerName;DataBaseName;SchemaName;TableName;PathName;FileName;Load",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsConfiguration')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "bronze/Configuration",
						"fileSystem": "mdw"
					},
					"columnDelimiter": ";",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ServerName;DataBaseName;SchemaName;TableName;PathName;FileName;Load",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsConnectionString')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "conn.csv",
						"folderPath": "bronze/Configuration",
						"fileSystem": "mdw"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "conn",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsGoldDimensions')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "gold/Dimensions",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsGoldFact')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "gold/Fact",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetError')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					},
					"NombreCarpeta": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@concat('bronze/Error/', dataset().NombreCarpeta)",
							"type": "Expression"
						},
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetProcessed')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					},
					"NombreCarpeta": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@concat('bronze/Processed/', dataset().NombreCarpeta)",
							"type": "Expression"
						},
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsParquetRaw')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"NombreFichero": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().NombreFichero",
							"type": "Expression"
						},
						"folderPath": "bronze/Landing/Pending",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsRawEntidades')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "Bronze"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "bronze/Landing/Pending",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSQLGenerico')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_COLI_ERP",
					"type": "LinkedServiceReference",
					"parameters": {
						"NombreServidor": {
							"value": "@dataset().NombreServidor",
							"type": "Expression"
						},
						"NombreBaseDatos": {
							"value": "@dataset().NombreBD",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"NombreServidor": {
						"type": "string"
					},
					"NombreBD": {
						"type": "string"
					},
					"NombreEsquema": {
						"type": "string"
					},
					"NombreTabla": {
						"type": "string"
					},
					"FechaActualizacion": {
						"type": "string"
					},
					"ColumnaIncremental": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServerTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().NombreEsquema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().NombreTabla",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_COLI_ERP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSilverFact')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "silver/Fact",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsSilverSCD')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_ADLG2_Bronze_Landing",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "silver/SCD",
						"fileSystem": "mdw"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_ADLG2_Bronze_Landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-tfm-pgc-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('asa-tfm-pgc-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-tfm-pgc-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('asa-tfm-pgc-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_ADLG2_Bronze_Landing')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ls_ADLG2_Bronze_Landing_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_COLI_ERP')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"NombreServidor": {
						"type": "string"
					},
					"NombreBaseDatos": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "SqlServer",
				"typeProperties": {
					"server": "[parameters('ls_COLI_ERP_properties_typeProperties_server')]",
					"database": "[parameters('ls_COLI_ERP_properties_typeProperties_database')]",
					"encrypt": "mandatory",
					"trustServerCertificate": true,
					"authenticationType": "SQL",
					"userName": "[parameters('ls_COLI_ERP_properties_typeProperties_userName')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('ls_COLI_ERP_password')]"
					}
				},
				"connectVia": {
					"referenceName": "IR-Coli-ERP",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/IR-Coli-ERP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_Severless')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"server": "[parameters('ls_Severless_properties_typeProperties_server')]",
					"database": "[parameters('ls_Severless_properties_typeProperties_database')]",
					"encrypt": "mandatory",
					"trustServerCertificate": false,
					"authenticationType": "SystemAssignedManagedIdentity"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Carga 04_00')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Orchestrator",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2024-10-24T00:44:00",
						"timeZone": "Romance Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								4
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Orchestrator')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IR-Coli-ERP')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential1')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {
					"resourceId": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.ManagedIdentity/userAssignedIdentities/asa-tfm-pgc"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure BackuplessSTG')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Configuration"
				},
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity' \nGO\n\nCREATE USER [asa-tfm-pgc] FROM EXTERNAL PROVIDER;\nGO\nALTER ROLE db_datareader ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_datawriter ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_owner ADD MEMBER [asa-tfm-pgc];\nGO\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA bk AUTHORIZATION dbo\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "BackuplessDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure GoldelessSTG')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Configuration"
				},
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity' \nGO\nCREATE DATABASE SCOPED CREDENTIAL [https://datalake1pgc.dfs.core.windows.net] \nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'SAS Token';\nGO\nCREATE USER [asa-tfm-pgc] FROM EXTERNAL PROVIDER;\nGO\nALTER ROLE db_datareader ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_datawriter ADD MEMBER [asa-tfm-pgc];\nGO\nALTER ROLE db_owner ADD MEMBER [asa-tfm-pgc];\nGO\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA dim AUTHORIZATION dbo\nGO\nCREATE SCHEMA fact AUTHORIZATION dbo\nGO\nCREATE SCHEMA gold AUTHORIZATION dbo\n\nCREATE SCHEMA etl AUTHORIZATION dbo",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure ReportingDB')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'tfmVerne.'\nGO\nCREATE DATABASE SCOPED CREDENTIAL ManagedIdentity WITH IDENTITY = 'Managed Identity'\nGO\nSELECT * FROM sys.database_scoped_credentials;\n\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat WITH (FORMAT_TYPE = PARQUET, DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec')\nGO\nCREATE EXTERNAL DATA SOURCE datalake1pgc\nWITH (\n    \n    LOCATION = 'abfss://mdw@datalake1pgc.dfs.core.windows.net',\n    CREDENTIAL = ManagedIdentity\n);\nGO\nCREATE SCHEMA dim AUTHORIZATION dbo;\nGO\nCREATE SCHEMA fact AUTHORIZATION dbo;\nGO\n\n\n\nCREATE USER [powerBIdata] from EXTERNAL PROVIDER\nALTER ROLE db_datareader ADD MEMBER [powerBI];\n-- 1. Crear un login a nivel de servidor para el usuario\nCREATE LOGIN powerBIuser WITH PASSWORD = 'Laburo99.';\n\n-- 2. Cambiar el contexto a la base de datos donde deseas crear el usuario\n\n\n-- 3. Crear el usuario dentro de la base de datos basada en el login creado\nCREATE USER powerBI FOR LOGIN powerBIuser;\nCREATE USER powerBIdata FOR LOGIN powerBIuser;\n\n-- 4. Asignar permisos especÃ­ficos o roles al usuario\n-- Por ejemplo, otorgar permisos de lectura y escritura\nALTER ROLE db_datareader ADD MEMBER powerBI;\nALTER ROLE db_datawriter ADD MEMBER powerBI;\n\n-- O para darle permisos mÃ¡s amplios como propietario de la base de datos:\n-- ALTER ROLE db_owner ADD MEMBER nuevo_usuario;\n\nSELECT * FROM sys.database_principals WHERE name = 'powerBI';\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ReportingDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Dimensions Gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Views Silver"
				},
				"content": {
					"query": "---------------\n-- Dim Date --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Date] AS \nSELECT  idFechas AS idDate,\n        ClaveFecha AS dateKey,\n        Fecha AS date,\n        COALESCE(Mes, 'D')  AS month,\n        COALESCE(NumeroMes, -1)  AS monthNumber,\n        COALESCE(Ano, -1)  AS year,\n        COALESCE(NumeroSemana, -1)  AS weekNumber,\n        COALESCE(DiaSemana, 'D') AS dayWeek,\n        COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\n        COALESCE(DiaAno, -1) AS yearDay,\n        CASE WHEN Trimestre = 1 THEN 'Q1'\n             WHEN Trimestre = 2 THEN 'Q2'\n             WHEN Trimestre = 3 THEN 'Q3'\n             ELSE '-1' END AS quarter,\n        CASE WHEN Cuatrimestre = 1 THEN 'Q1'\n             WHEN Cuatrimestre = 2 THEN 'Q2'\n             WHEN Cuatrimestre = 3 THEN 'Q3'\n             WHEN Cuatrimestre = 4 THEN 'Q4'\n             ELSE '-1' \n        END                                       AS quadrimester,\n        CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\n            WHEN Cuatrimestre IN (2,4) THEN 'S2'\n            ELSE '-1' \n        END                                       AS semester,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[fechas]\n\n\n------------------\n-- Dim Articles --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Articles] AS\nSELECT  idArticulos                                                                         AS idArticles,\n        COALESCE(nombre, 'D')                                                               AS name,\n        COALESCE(descripcion, 'D')                                                          AS description,\n        COALESCE(codigoReferencia, 'D')                                                     AS externalCode,\n        COALESCE(t.talla, 'D')                                                              AS size,\n        COALESCE( t.numeroTalla, -1)                                                        AS numSize,\n        COALESCE(co.color, 'D')                                                             AS colour,\n        COALESCE(ca.categoria, 'D')                                                         AS category,\n        COALESCE(l.codigoLinea, 'D')                                                        AS codLine,\n        COALESCE(l.Linea, 'D')                                                              AS line, \n        CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN 'OI'+CAST((a.idTemporada) AS nvarchar)\n             WHEN a.idCategoria IN (2,4,6,7,9) THEN 'OI'+CAST(a.idTemporada AS nvarchar)\n             ELSE 'PV'+CAST(a.idTemporada AS nvarchar)\n        END                                                                                 AS season,\n        a.fechaCarga                                                                        AS loadDate,\n        a.fechaDelta                                                                        AS deltaDate\nFROM [default].[dbo].[articulos] AS a\nLEFT JOIN [default].[dbo].[talla] AS t\n    ON t.idTalla = a.idTalla\nLEFT JOIN [default].[dbo].[color] AS co\n   ON co.idColor = a.idColor\nLEFT JOIN [default].[dbo].[categoria] AS ca -- select * from [default].[dbo].[categoria]\n    ON ca.idCategoria = a.idCategoria\nLEFT JOIN [default].[dbo].[Linea] AS l\n    ON l.idLinea = a.idLinea\n--where  a.fechaCarga  < '20241009'\n\n\n-------------------\n-- Dim Warehouse --\n-------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Warehouse] AS\nSELECT  idAlmacenes AS idWarehouse,\n        COALESCE(a.Nombre, 'D') AS warehouse,\n        COALESCE(a.codigoAlmacen, 'D') AS externalCode,\n        COALESCE(p.codigoPais, 'D') AS countryCode, \n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(a.ciudad, 'D') AS city,\n        COALESCE(a.Direccion, 'D') AS address,\n        COALESCE(a.descripcion, 'D') AS description,\n        a.fechaCarga AS loadDate,\n        a.fechaDelta AS deltaDate\nFROM [default].[dbo].[almacenes] AS a\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.idpais = a.idPais\n  -- WHERE a.fechaDelta <> 20241011\n--UNION \n--select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\n--UNION \n--select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\n\n\n---------------------\n-- Dim Postal Code --\n---------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_PostalCodes] AS\nSELECT  idCodigoPostal AS idPostalCodes,\n        COALESCE(codigoPostal, 'D') AS postalCode,\n        COALESCE(region, 'D') AS region,\n        COALESCE(c.codigoPais, 'D') AS countryCode,\n        COALESCE(nombre, 'D') AS country,\n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[codigoPostal] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.codigoPais = c.codigoPais\n\n------------------\n-- Dim Currency --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Currency] AS\nSELECT  idDivisa AS idCurrency,\n        COALESCE(nombre, 'D') AS name,\n        COALESCE(Divisa, 'D') AS currency,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[divisa]\n\n---------------\n-- Dim Hours --\n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Hours] AS\nSELECT  idHoras AS idHours,\n        COALESCE(Hora, -1) AS hour,\n        COALESCE(Minuto, -1) AS minute,\n        COALESCE(HoraCompleta, 'D') as fullHour,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[horas]\n\n\n----------------\n-- Dim Tariff --\n----------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Tariff] AS\nSELECT  idTarifa AS idTariff,\n        COALESCE(Tarifa, 'D') AS tariff,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate       \nFROM [default].[dbo].[tarifa]\n\n\n------------------------\n-- Dim Operation Type --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_OperationType] AS\nSELECT  idTipoOperacion AS idOperationType,\n        COALESCE(Operacion, 'D') AS operation,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[tipoOperacion]\n\n\n------------------------\n-- Dim Client --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Client] AS\nSELECT  idCliente AS idClient,\n        COALESCE(c.nombre, 'D') AS name,\n        COALESCE(apellido1, 'D') AS lastName1,\n        COALESCE(apellido2, 'D') AS lastName2, \n        COALESCE(email, 'D') AS email,\n        COALESCE(telefono, 'D') AS phoneNumber,\n        COALESCE(cumpleanos, '1900-01-01') AS birthDay,       \n        DATEDIFF(YEAR, cumpleanos, GETDATE()) \n        - CASE \n            WHEN MONTH(GETDATE()) < MONTH('1983-12-04') \n                 OR (MONTH(GETDATE()) = MONTH('1983-12-04') AND DAY(GETDATE()) < DAY('1983-12-04'))\n            THEN 1\n            ELSE 0\n        END AS age,\n        CASE WHEN hombre = 1 THEN 'Hombre'\n        ELSE 'Mujer' \n        END AS gender,\n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(p.codigoPais, 'D') AS countryCode,\n        COALESCE(cp.region, 'D') AS region,\n        COALESCE(c.direcion, 'D') AS address,\n        COALESCE(cp.codigoPostal, 'D') AS postalCode,\n        COALESCE(activo, 0) AS active,       \n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[cliente] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    On c.idPais = c.idPais\nINNER JOIN [default].[dbo].[codigoPostal] AS cp\n    ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Dimensions Silver')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Views Silver"
				},
				"content": {
					"query": "---------------\n-- Dim Date --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Date] AS \nSELECT  idFechas AS idDate,\n        ClaveFecha AS dateKey,\n        Fecha AS date,\n        COALESCE(Mes, 'D')  AS month,\n        COALESCE(NumeroMes, -1)  AS monthNumber,\n        COALESCE(Ano, -1)  AS year,\n        COALESCE(NumeroSemana, -1)  AS weekNumber,\n        COALESCE(DiaSemana, 'D') AS dayWeek,\n        COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\n        COALESCE(DiaAno, -1) AS yearDay,\n        CASE WHEN Trimestre = 1 THEN 'Q1'\n             WHEN Trimestre = 2 THEN 'Q2'\n             WHEN Trimestre = 3 THEN 'Q3'\n             ELSE '-1' END AS quarter,\n        CASE WHEN Cuatrimestre = 1 THEN 'Q1'\n             WHEN Cuatrimestre = 2 THEN 'Q2'\n             WHEN Cuatrimestre = 3 THEN 'Q3'\n             WHEN Cuatrimestre = 4 THEN 'Q4'\n             ELSE '-1' \n        END                                       AS quadrimester,\n        CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\n            WHEN Cuatrimestre IN (2,4) THEN 'S2'\n            ELSE '-1' \n        END                                       AS semester,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[fechas]\n\n\n------------------\n-- Dim Articles --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Articles] AS\nSELECT  idArticulos                                                                         AS idArticles,\n        COALESCE(nombre, 'D')                                                               AS name,\n        COALESCE(descripcion, 'D')                                                          AS description,\n        COALESCE(codigoReferencia, 'D')                                                     AS externalCode,\n        COALESCE(t.talla, 'D')                                                              AS size,\n        COALESCE( t.numeroTalla, -1)                                                        AS numSize,\n        COALESCE(co.color, 'D')                                                             AS colour,\n        COALESCE(ca.categoria, 'D')                                                         AS category,\n        COALESCE(l.codigoLinea, 'D')                                                        AS codLine,\n        COALESCE(l.Linea, 'D')                                                              AS line, \n        CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN 'OI'+CAST((a.idTemporada) AS nvarchar)\n             WHEN a.idCategoria IN (2,4,6,7,9) THEN 'OI'+CAST(a.idTemporada AS nvarchar)\n             ELSE 'PV'+CAST(a.idTemporada AS nvarchar)\n        END                                                                                 AS season,\n        a.fechaCarga                                                                        AS loadDate,\n        a.fechaDelta                                                                        AS deltaDate\nFROM [default].[dbo].[articulos] AS a\nLEFT JOIN [default].[dbo].[talla] AS t\n    ON t.idTalla = a.idTalla\nLEFT JOIN [default].[dbo].[color] AS co\n   ON co.idColor = a.idColor\nLEFT JOIN [default].[dbo].[categoria] AS ca -- select * from [default].[dbo].[categoria]\n    ON ca.idCategoria = a.idCategoria\nLEFT JOIN [default].[dbo].[Linea] AS l\n    ON l.idLinea = a.idLinea\n--where  a.fechaCarga  < '20241009'\n\n\n-------------------\n-- Dim Warehouse --\n-------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Warehouse] AS\nSELECT  idAlmacenes AS idWarehouse,\n        COALESCE(a.Nombre, 'D') AS warehouse,\n        COALESCE(a.codigoAlmacen, 'D') AS externalCode,\n        COALESCE(p.codigoPais, 'D') AS countryCode, \n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(a.ciudad, 'D') AS city,\n        COALESCE(a.Direccion, 'D') AS address,\n        COALESCE(a.descripcion, 'D') AS description,\n        a.fechaCarga AS loadDate,\n        a.fechaDelta AS deltaDate\nFROM [default].[dbo].[almacenes] AS a\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.idpais = a.idPais\n  -- WHERE a.fechaDelta <> 20241011\n--UNION \n--select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','AlmacÃ©n especializado en logÃ­stica',20241010,20241010\n--UNION \n--select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','AlmacÃ©n especializado',20241011,20241011\n\n\n---------------------\n-- Dim Postal Code --\n---------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_PostalCodes] AS\nSELECT  idCodigoPostal AS idPostalCodes,\n        COALESCE(codigoPostal, 'D') AS postalCode,\n        COALESCE(region, 'D') AS region,\n        COALESCE(c.codigoPais, 'D') AS countryCode,\n        COALESCE(nombre, 'D') AS country,\n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[codigoPostal] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    ON p.codigoPais = c.codigoPais\n\n------------------\n-- Dim Currency --\n------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Currency] AS\nSELECT  idDivisa AS idCurrency,\n        COALESCE(nombre, 'D') AS name,\n        COALESCE(Divisa, 'D') AS currency,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[divisa]\n\n---------------\n-- Dim Hours --\n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Hours] AS\nSELECT  idHoras AS idHours,\n        COALESCE(Hora, -1) AS hour,\n        COALESCE(Minuto, -1) AS minute,\n        COALESCE(HoraCompleta, 'D') as fullHour,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[horas]\n\n\n----------------\n-- Dim Tariff --\n----------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Tariff] AS\nSELECT  idTarifa AS idTariff,\n        COALESCE(Tarifa, 'D') AS tariff,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate       \nFROM [default].[dbo].[tarifa]\n\n\n------------------------\n-- Dim Operation Type --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_OperationType] AS\nSELECT  idTipoOperacion AS idOperationType,\n        COALESCE(Operacion, 'D') AS operation,\n        fechaCarga AS loadDate,\n        fechaDelta AS deltaDate\nFROM [default].[dbo].[tipoOperacion]\n\n\n------------------------\n-- Dim Client --\n------------------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_dim_Client] AS\nSELECT  idCliente AS idClient,\n        COALESCE(c.nombre, 'D') AS name,\n        COALESCE(apellido1, 'D') AS lastName1,\n        COALESCE(apellido2, 'D') AS lastName2, \n        COALESCE(email, 'D') AS email,\n        COALESCE(telefono, 'D') AS phoneNumber,\n        COALESCE(cumpleanos, '1900-01-01') AS birthDay,       \n        DATEDIFF(YEAR, cumpleanos, GETDATE()) \n        - CASE \n            WHEN MONTH(GETDATE()) < MONTH('1983-12-04') \n                 OR (MONTH(GETDATE()) = MONTH('1983-12-04') AND DAY(GETDATE()) < DAY('1983-12-04'))\n            THEN 1\n            ELSE 0\n        END AS age,\n        CASE WHEN hombre = 1 THEN 'Hombre'\n        ELSE 'Mujer' \n        END AS gender,\n        COALESCE(p.nombre, 'D') AS country,\n        COALESCE(p.codigoPais, 'D') AS countryCode,\n        COALESCE(cp.region, 'D') AS region,\n        COALESCE(c.direcion, 'D') AS address,\n        COALESCE(cp.codigoPostal, 'D') AS postalCode,\n        COALESCE(activo, 0) AS active,       \n        c.fechaCarga AS loadDate,\n        c.fechaDelta AS deltaDate\nFROM [default].[dbo].[cliente] AS c\nLEFT JOIN [default].[dbo].[pais] AS p\n    On c.idPais = c.idPais\nINNER JOIN [default].[dbo].[codigoPostal] AS cp\n    ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Fact Gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Views Silver"
				},
				"content": {
					"query": "---------------\n-- Fact Sales --  \n---------------\nGO\nCREATE OR ALTER VIEW [etl].[vw_fact_Sales] AS \nSELECT  idDesgloseVenta AS idSales,\n        d.idArticulo AS idArticles,\n        c.idAlmacen AS idWarehouse,\n        c.idCliente AS idClient,\n        c.idCodigoPostal AS idPostalCodes,\n        c.idDivisa AS idCurrency,\n        c.idTarifa AS idTariff,\n        c.idTipoOperacion AS idOperationType,\n        c.idHora AS idHours,\n        c.idFecha AS idDate,\n        f.fecha AS  date,\n        CAST(CAST(f.fecha AS nvarchar(10)) + ' ' + h.horaCompleta AS datetime2)AS dateOp,\n        c.codigoTicket As ticketNumber,\n        d.Cantidad AS quantity,\n        d.PrecioUnitario AS unitPrice,\n        d.CosteUnitario AS unitCost,\n        d.importeBruto AS amtTotalEuros,\n        d.importeNeto AS amtNetEuros,\n        d.importeDescuento AS amtTotalEurosDiscount,\n        d.importeNetoDescuento AS amtNetEurosDiscount,\n        CASE WHEN idTarifa IN (2,3) THEN 1\n             ELSE 0 \n        END AS discountSale,\n        CASE WHEN idTarifa = 0 THEN 0\n            ELSE (d.importeBruto - d.importeDescuento) \n        END AS amtTotalDiscount,\n        CASE WHEN idTarifa = 0 THEN 0 \n             ELSE (d.importeNeto - d.importeNetoDescuento) \n        END AS amtNetDiscount,\n        d.fechaCarga AS loadDate,\n        d.fechaDelta AS deltaDate\nFROM [default].[dbo].[desgloseVenta] AS d\nINNER JOIN [default].[dbo].[cabeceraVenta] AS c\n    ON d.idcabecerasVentas = c.idcabeceraVenta\nLEFT JOIN [default].[dbo].[Fechas]  AS f\n    On f.idFechas = c.idFecha\nLEFT JOIN [default].[dbo].[horas]  AS h\n    On h.idHoras = c.idHora",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Fact Silver')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Views Silver"
				},
				"content": {
					"query": "---------------\n-- Fact Sales --  \n---------------\nGO\n--CREATE OR ALTER VIEW [etl].[vw_fact_Sales] AS \nSELECT top 100 idDesgloseVenta AS idSales,\n        d.idArticulo AS idArticles,\n        c.idAlmacen AS idWarehouse,\n        c.idCliente AS idClient,\n        c.idCodigoPostal AS idPostalCodes,\n        c.idDivisa AS idCurrency,\n        c.idTarifa AS idTariff,\n        c.idTipoOperacion AS idOperationType,\n        c.idHora AS idHours,\n        c.idFecha AS idDate,\n        f.fecha AS  date,\n        CAST(CAST(f.fecha AS nvarchar(10)) + ' ' + h.horaCompleta AS datetime2)AS dateOp,\n        COALESCE(c.codigoTicket, 'D') AS ticketNumber,\n        COALESCE(d.Cantidad, 0) AS quantity,\n        COALESCE(d.PrecioUnitario, 0)  AS unitPrice,\n        COALESCE(d.CosteUnitario, 0)  AS unitCost,\n        COALESCE(d.importeBruto, 0)  AS amtTotalEuros,\n        COALESCE(d.importeNeto, 0)  AS amtNetEuros,\n        COALESCE(d.importeDescuento, 0)  AS amtTotalEurosDiscount,\n        COALESCE(d.importeNetoDescuento, 0) AS amtNetEurosDiscount,\n        CASE WHEN idTarifa IN (2,3) THEN 1\n             ELSE 0 \n        END AS discountSale,\n        CASE WHEN idTarifa = 0 THEN 0\n            ELSE (d.importeBruto - d.importeDescuento) \n        END AS amtTotalDiscount,\n        CASE WHEN idTarifa = 0 THEN 0 \n             ELSE (d.importeNeto - d.importeNetoDescuento) \n        END AS amtNetDiscount,\n        d.fechaCarga AS loadDate,\n        d.fechaDelta AS deltaDate\nFROM [default].[dbo].[desgloseVenta] AS d\nINNER JOIN [default].[dbo].[cabeceraVenta] AS c\n    ON d.idcabecerasVentas = c.idcabeceraVenta\nLEFT JOIN [default].[dbo].[Fechas]  AS f\n    On f.idFechas = c.idFecha\nLEFT JOIN [default].[dbo].[horas]  AS h\n    On h.idHoras = c.idHora",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Views Reporting')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- ==============================================================================\n-- Vistas de la capa Gold para el informe\n-- ==============================================================================\n\n-- Arreglar dateop en noteboobk\n\nCREATE OR ALTER VIEW fact.Sales AS \nSELECT  s.skSales,\n        s.idSales,\n        s.idArticles,\n        skArticles,\n        s.idWarehouse,\n        skWarehouse,\n        s.idClient,\n        skClient, \n        s.idPostalCode,\n        skPostalCode, \n        s.idCurrency,\n        skCurrency,\n        s.idTariff,\n        skTariff, \n        s.idOperationType,\n        skOperationType,\n        s.idHours,\n        skHours,\n        s.idDate,\n        skDate,\n        s.date,\n        s.dateOp,\n        s.ticketNumber,\n        s.quantity,\n        s.unitPrice,\n        s.unitCost,\n        s.amtTotalEuros,\n        s.amtNetEuros,\n        s.amtTotalEurosDiscount,\n        s.amtNetEurosDiscount,\n        s.discountSale,\n        s.amtTotalDiscount,\n        s.amtNetDiscount\nFROM gold.dbo.fact_Sales AS s\n\n\nCREATE OR ALTER VIEW dim.Articles AS\nSELECT  -1 AS skArticles, \n        -1 AS idArticles, \n        'D' AS name, \n        'D' AS description, \n        'D' AS externalCode, \n        'D' AS size, \n        -1  AS numSize, \n        'D' AS colour, \n        'D' AS category, \n        'D' AS codLine, \n        'D' AS line, \n        'D' AS season\n\nUNION \n\nSELECT  skarticles, \n        idarticles, \n        name, \n        description, \n        externalCode, \n        size, \n        numSize, \n        colour, \n        category, \n        codLine, \n        line, \n        season\nFROM gold.dbo.dim_articles;\n\nCREATE OR ALTER VIEW dim.Client AS\nSELECT  -1 AS skClient,\n        -1 AS idClient,\n        'D' AS name,\n        'D' AS lastName1,\n        'D' AS lastName2,\n        'D' AS email,\n        'D' AS phoneNumber,\n        'D' AS birthDay,\n        -1 AS age,\n        'D' AS gender,\n        'D' AS country,\n        'D' AS countryCode,\n        'D' AS region,\n        'D' AS address,\n        'D' AS postalCode,\n        0 AS active\nUNION\nSELECT  skClient,\n        idClient,\n        name,\n        lastName1,\n        lastName2,\n        email,\n        phoneNumber,\n        birthDay,\n        age,\n        gender,\n        country,\n        countryCode,\n        region,\n        address,\n        postalCode,\n        active\nFROM gold.dbo.dim_client\n\nCREATE OR ALTER VIEW dim.Currency AS\nSELECT  -1 AS skCurrency,\n        -1 AS idCurrency,\n        'D' AS name,\n        'D' AS currency\nUNION\nSELECT  skCurrency,\n        idCurrency,\n        name,\n        currency\nFROM gold.dbo.dim_currency\n\nCREATE OR ALTER VIEW dim.Date AS\nSELECT  -1 AS skDate,\n        -1 AS idDate,\n        -1 AS datekey,\n        CAST('1990-01-01' AS datetime2(7)) AS date,\n        'D' AS month,\n        -1 AS monthNumber,\n        -1 AS year,\n        -1 AS weekNumber,\n        'D' AS dayWeek,\n        -1 AS dayWeekNumber,\n        -1 AS yearDay,\n        'D' AS quarter,\n        'D' AS quadrimester,\n        'D' AS semester\nUNION\nSELECT  skDate,\n        idDate,\n        datekey,\n        date,\n        month,\n        monthNumber,\n        year,\n        weekNumber,\n        dayWeek,\n        dayWeekNumber,\n        yearDay,\n        quarter,\n        quadrimester,\n        semester\nFROM gold.dbo.dim_date\n\nCREATE OR ALTER VIEW dim.Hours AS\nSELECT  -1 AS skHours,\n        -1 AS idHours,\n        -1 AS hour,\n        -1 AS minute,\n        'D' AS fullHour\nUNION\nSELECT  skHours,\n        idHours,\n        hour,\n        minute,\n        fullHour\nFROM gold.dbo.dim_hours\n\n\nCREATE OR ALTER VIEW dim.OperationType AS\nSELECT  -1 AS skOperationType,\n        -1 AS idOperationType,\n        'D' AS operation\nUNION \nSELECT  skOperationType,\n        idOperationType,\n        operation\nFROM gold.dbo.dim_operationtype\n\n\nCREATE OR ALTER VIEW dim.PostalCode AS\nSELECT  -1 AS skPostalCode,\n        -1 AS idPostalCode,\n        'D' AS postalCode,\n        'D' AS region,\n        'D' AS countryCode,\n        'D' AS country\nUNION\nSELECT  skPostalCode,\n        idPostalCode,\n        postalCode,\n        region,\n        countryCode,\n        country\nFROM gold.dbo.dim_postalcode\n\n\nCREATE OR ALTER VIEW dim.Tariff AS\nSELECT  -1 AS skTariff,\n        -1 AS idTariff,\n        'D' AS tariff\nUNION\nSELECT  skTariff,\n        idTariff,\n        tariff\nFROM gold.dbo.dim_tariff\n\n\nCREATE OR ALTER VIEW dim.Warehouse AS\nSELECT  -1 AS skWarehouse,\n        -1 AS idWarehouse,\n        'D' AS warehouse,\n        'D' AS externalCode,\n        'D' AS countryCode,\n        'D' AS country,\n        'D' AS city,\n        'D' AS address,\n        'D' AS description\nUNION\nSELECT  skWarehouse,\n        idWarehouse,\n        warehouse,\n        externalCode,\n        countryCode,\n        country,\n        city,\n        address,\n        description\nFROM gold.dbo.dim_warehouse",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ReportingDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/First Load Gold CETAS SCD2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Gold Dim CETAs"
				},
				"content": {
					"query": "---------------\n-- Articles  --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Articles]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles') )\n    DROP EXTERNAL TABLE dim.Articles \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idArticles) idSkArticles,\n\tidArticles,\n\tname,\n\texternalcode,\n\tsize,\n\tnumSize,\n\tcolour,\n\tcategory, \n\tcodLine,\n\tline, \n\tdescription,\n\tseason, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent, \n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idArticles AS NVARCHAR)+ name + description COLLATE DATABASE_DEFAULT + externalcode + size + CAST(numSize AS NVARCHAR) + colour + category + codLine + line + season)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Articles]\n\n\n--------------\n-- Client --\n--------------\n-- drop EXTERNAL TABLE [dim].[Client]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Client \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Client]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkClient, \n\t-1  \t\t AS idClient, \n\t'D' \t\t AS name,\n\t'D' \t\t AS lastName1,\n\t'D' \t\t AS lastName2,\n\t'D' AS email,\n\t'D' AS phoneNumber,\n\tCAST('1900-01-01' AS date) AS birthDay,       \n\t-1 AS age,\n\t'D' AS gender,\n\t'D' AS country,\n\t'D' AS countryCode,\n\t'D'AS region,\n\t'D' AS address,\n\t'D' AS postalCode,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' +'D' + CAST('1900-01-01' AS nvarchar) + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idClient) AS idSkClient,\n\tidClient,\n\tname,\n\tlastName1,\n\tlastName2, \n\temail,\n\tphoneNumber,\n\tbirthDay,       \n\tage,\n\tgender,\n\tcountry,\n\tcountryCode,\n\tregion,\n\taddress,\n\tpostalCode,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idClient AS NVARCHAR) + name COLLATE DATABASE_DEFAULT + lastName1 COLLATE DATABASE_DEFAULT + lastName2 COLLATE DATABASE_DEFAULT\n\t+ email COLLATE DATABASE_DEFAULT + phoneNumber COLLATE DATABASE_DEFAULT + CAST(birthDay AS nvarchar) + CAST(age AS nvarchar) + gender + country + countryCode\n\t+ region + address + postalCode)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Client]\n\n\n--------------\n-- Currency --\n--------------\n-- drop EXTERNAL TABLE [dim].[Currency]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Currency \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idCurrency) AS idSkCurrency,\n\tidCurrency,\n\tname,\n\tcurrency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idCurrency AS NVARCHAR) + name + currency)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Currency]\n\n\n----------\n-- Date --\n----------\n-- drop EXTERNAL TABLE [dim].[Date]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Date') )\n    DROP EXTERNAL TABLE dim.Date \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n    HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n    + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Date]\n\n\n-----------\n-- Hours --\n-----------\n-- drop EXTERNAL TABLE [dim].[Hours]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Hours') )\n    DROP EXTERNAL TABLE dim.Hours \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Hours]\n\n\n-------------------\n-- OperationType --\n-------------------\n-- drop EXTERNAL TABLE [dim].[OperationType]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.OperationType') )\n    DROP EXTERNAL TABLE dim.OperationType \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_OperationType]\n\n\n-----------------\n-- PostalCodes --\n-----------------\n-- drop EXTERNAL TABLE [dim].[PostalCodes]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.PostalCodes') )\n    DROP EXTERNAL TABLE dim.PostalCodes \n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idPostalCodes, \n\t'D' \t\t AS postalCode,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_PostalCodes]\n\n\n------------\n-- Tariff --\n------------\n-- drop EXTERNAL TABLE [dim].[Tariff]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Tariff') )\n    DROP EXTERNAL TABLE dim.Tariff \n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Tariff]\n\n\n---------------\n-- Warehouse --\n---------------\n-- drop EXTERNAL TABLE [dim].[Warehouse]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Warehouse') )\n    DROP EXTERNAL TABLE dim.Warehouse \n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idWarehouse) AS idSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Warehouse]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load SCD2 Silver CETAs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Silver SCD + Fact"
				},
				"content": {
					"query": "---------------\n-- Articles --\n---------------\n-- DROP EXTERNAL TABLE  scd.Articles\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Articles') )\n    DROP EXTERNAL TABLE scd.Articles;\n\n\nCREATE EXTERNAL TABLE  scd.Articles \nWITH\n(\n\tLOCATION = 'silver/SCD/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n    \n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Articles]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idArticles AS NVARCHAR)+ s.name + s.description COLLATE DATABASE_DEFAULT + s.externalcode + s.size + CAST(s.numSize AS NVARCHAR) + s.colour + s.category + s.codLine + s.line + s.season)) AS [$hash]\n    FROM etl.vw_dim_Articles s\n    LEFT JOIN current_data c\n        ON s.idArticles = c.idArticles\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkArticles), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Articles]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idArticles) + max_key AS new_idSkArticles, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idArticles = c.idArticles\n    CROSS JOIN max_surrogate_key\n    WHERE c.idArticles IS NULL  OR (c.[$hash] != chg.[$hash] )\n), --select * from new_or_updated_data order by idarticles\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkarticles AS new_idSkArticles,\n        c.idArticles,\n        c.name,\n        c.externalCode,\n        c.size,\n        c.numSize,\n        c.colour,\n        c.category,\n        c.codLine,\n        c.line,\n        c.description,\n        c.season,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idArticles = chg.idArticles\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkArticles, -- Mantener la clave subrogada original\n    c.idArticles,\n    c.name,\n    c.externalCode,\n    c.size,\n    c.numSize,\n    c.colour,\n    c.category,\n    c.codLine,\n    c.line,\n    c.description,\n    c.season,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idArticles = chg.idArticles\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkArticles;\n\n\n---------------\n-- Currency --\n---------------\n-- DROP EXTERNAL TABLE  scd.Currency\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Currency') )\n    DROP EXTERNAL TABLE scd.Currency;\n\n\nCREATE EXTERNAL TABLE  scd.Currency \nWITH\n(\n\tLOCATION = 'silver/SCD/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Currency]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idCurrency AS NVARCHAR) + s.name + s.currency)) AS [$hash]\n    FROM etl.vw_dim_Currency s\n    LEFT JOIN current_data c\n        ON s.idCurrency = c.idCurrency\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkCurrency), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Currency]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idCurrency) + max_key AS new_idSkCurrency, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idCurrency = c.idCurrency\n    CROSS JOIN max_surrogate_key\n    WHERE c.idCurrency IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkCurrency AS new_idSkCurrency,\n        c.idCurrency,\n        c.name,\n        c.currency,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idCurrency = chg.idCurrency\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkCurrency, -- Mantener la clave subrogada original\n    c.idCurrency,\n    c.name,\n    c.currency,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idCurrency = chg.idCurrency\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkCurrency;\n\n\n------------\n-- Client --\n------------\n--DROP EXTERNAL TABLE  scd.Client\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Client') )\n   DROP EXTERNAL TABLE scd.Client   \n\n\nCREATE EXTERNAL TABLE  scd.Client \nWITH\n(\n\tLOCATION = 'silver/SCD/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Client]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idClient AS NVARCHAR) + s.name COLLATE DATABASE_DEFAULT + s.lastName1 COLLATE DATABASE_DEFAULT + s.lastName2 COLLATE DATABASE_DEFAULT\n\t    + s.email COLLATE DATABASE_DEFAULT + s.phoneNumber COLLATE DATABASE_DEFAULT + CAST(s.birthDay AS nvarchar) + CAST(s.age AS nvarchar) + s.gender + s.country + s.countryCode\n\t    + s.region + s.address + s.postalCode)) AS [$hash]\n    FROM etl.vw_dim_Client s\n    LEFT JOIN current_data c\n        ON s.idClient = c.idClient\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkClient), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Client]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idClient) + max_key AS new_idSkClient, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idClient = c.idClient\n    CROSS JOIN max_surrogate_key\n    WHERE c.idClient IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkClient AS new_idSkClient,\n        c.idClient,\n        c.name,\n        c.lastName1,\n        c.lastName2, \n        c.email,\n        c.phoneNumber,\n        c.birthDay,       \n        c.age,\n        c.gender,\n        c.country,\n        c.countryCode,\n        c.region,\n        c.address,\n        c.postalCode,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idClient = chg.idClient\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkClient, -- Mantener la clave subrogada original\n    c.idClient,\n    c.name,\n    c.lastName1,\n    c.lastName2, \n    c.email,\n    c.phoneNumber,\n    c.birthDay,       \n    c.age,\n    c.gender,\n    c.country,\n    c.countryCode,\n    c.region,\n    c.address,\n    c.postalCode,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idClient = chg.idClient\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkClient;\n\n\n----------\n-- Date --\n----------\n--DROP EXTERNAL TABLE  scd.Date \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Date') )\n   DROP EXTERNAL TABLE scd.Date;  \n\n\nCREATE EXTERNAL TABLE  scd.Date \nWITH\n(\n\tLOCATION = 'silver/SCD/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Date]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idDate AS NVARCHAR) + CAST(s.dateKey AS NVARCHAR) + CAST(s.date AS NVARCHAR) + s.month COLLATE DATABASE_DEFAULT + CAST(s.monthNumber AS NVARCHAR) + CAST(s.year AS NVARCHAR) \n        + CAST(s.weekNumber AS NVARCHAR) + s.dayWeek COLLATE DATABASE_DEFAULT + CAST(s.dayWeekNumber AS NVARCHAR) + CAST(s.yearDay AS NVARCHAR) + s.quarter + s.quadrimester + s.semester )) AS [$hash]\n    FROM etl.vw_dim_Date s\n    LEFT JOIN current_data c\n        ON s.idDate = c.idDate\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkDate), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Date]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idDate) + max_key AS new_idSkDate, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idDate = c.idDate\n    CROSS JOIN max_surrogate_key\n    WHERE c.idDate IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkDate AS new_idSkDate,\n        c.idDate,\n        c.dateKey,\n        c.date,\n        c.month,\n        c.monthNumber,\n        c.year,\n        c.weekNumber,\n        c.dayWeek,\n        c.dayWeekNumber,\n        c.yearDay,\n        c.quarter,\n        c.quadrimester,\n        c.semester,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idDate = chg.idDate\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkDate, -- Mantener la clave subrogada original\n    c.idDate,\n\tc.dateKey,\n\tc.date,\n\tc.month,\n\tc.monthNumber,\n\tc.year,\n\tc.weekNumber,\n\tc.dayWeek,\n\tc.dayWeekNumber,\n\tc.yearDay,\n\tc.quarter,\n\tc.quadrimester,\n\tc.semester,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idDate = chg.idDate\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkDate;\n\n\n---------------\n-- Hours --\n---------------\n-- DROP EXTERNAL TABLE  scd.Hours\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Hours') )\n   DROP EXTERNAL TABLE scd.Hours;   \n\n\nCREATE EXTERNAL TABLE  scd.Hours \nWITH\n(\n\tLOCATION = 'silver/SCD/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Hours]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idHours AS NVARCHAR) + CAST(s.hour AS NVARCHAR) + CAST(s.minute AS NVARCHAR) + s.fullHour)) AS [$hash]\n    FROM etl.vw_dim_Hours s\n    LEFT JOIN current_data c\n        ON s.idHours = c.idHours\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkHours), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Hours]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idHours) + max_key AS new_idSkHours, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idHours = c.idHours\n    CROSS JOIN max_surrogate_key\n    WHERE c.idHours IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkHours AS new_idSkHours,\n        c.idHours,\n        c.hour,\n        c.minute,\n        c.fullHour,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idHours = chg.idHours\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkHours, -- Mantener la clave subrogada original\n\tc.idHours,\n\tc.hour,\n\tc.minute,\n\tc.fullHour,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idHours = chg.idHours\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkHours;\n\n\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE  scd.OperationType \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.OperationType') )\n   DROP EXTERNAL TABLE scd.OperationType;   \n\n\nCREATE EXTERNAL TABLE  scd.OperationType \nWITH\n(\n\tLOCATION = 'silver/SCD/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[OperationType]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idOperationType AS NVARCHAR) + s.operation)) AS [$hash]\n    FROM etl.vw_dim_OperationType s\n    LEFT JOIN current_data c\n        ON s.idOperationType = c.idOperationType\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkOperationType), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[OperationType]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idOperationType) + max_key AS new_idSkOperationType, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idOperationType = c.idOperationType\n    CROSS JOIN max_surrogate_key\n    WHERE c.idOperationType IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkOperationType AS new_idSkOperationType,\n        c.idOperationType,\n        c.operation,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idOperationType = chg.idOperationType\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkOperationType, -- Mantener la clave subrogada original\n    c.idOperationType,\n    c.operation,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idOperationType = chg.idOperationType\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkOperationType,\n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkOperationType, \n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkOperationType;\n\n\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE  scd.PostalCodes \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.PostalCodes') )\n    DROP EXTERNAL TABLE scd.PostalCodes; \n\nCREATE EXTERNAL TABLE  scd.PostalCodes \nWITH\n(\n\tLOCATION = 'silver/SCD/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idPostalCodes AS NVARCHAR) + s.postalCode + s.region + s.countryCode + s.country)) AS [$hash]\n    FROM etl.vw_dim_PostalCodes s\n    LEFT JOIN current_data c\n        ON s.idPostalCodes = c.idPostalCodes\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkPostalCodes), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idPostalCodes) + max_key AS new_idSkPostalCodes, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idPostalCodes = c.idPostalCodes\n    CROSS JOIN max_surrogate_key\n    WHERE c.idPostalCodes IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkPostalCodes AS new_idSkPostalCodes,\n        c.idPostalCodes,\n        c.postalCode,\n        c.region,\n        c.countryCode,\n        c.country,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idPostalCodes = chg.idPostalCodes\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkPostalCodes, -- Mantener la clave subrogada original\n    c.idPostalCodes,\n    c.postalCode,\n    c.region,\n    c.countryCode,\n    c.country,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idPostalCodes = chg.idPostalCodes\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkPostalCodes,\n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkPostalCodes, \n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkPostalCodes;\n\n\n\n------------\n-- Tariff --\n------------\n--DROP EXTERNAL TABLE  scd.Tariff \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Tariff') )\n   DROP EXTERNAL TABLE scd.Tariff;   \n\n\nCREATE EXTERNAL TABLE  scd.Tariff \nWITH\n(\n\tLOCATION = 'silver/SCD/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Tariff]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idTariff AS NVARCHAR) + s.tariff)) AS [$hash]\n    FROM etl.vw_dim_Tariff s\n    LEFT JOIN current_data c\n        ON s.idTariff = c.idTariff\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkTariff), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Tariff]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idTariff) + max_key AS new_idSkTariff, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idTariff = c.idTariff\n    CROSS JOIN max_surrogate_key\n    WHERE c.idTariff IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkTariff AS new_idSkTariff,\n        c.idTariff,\n        c.tariff,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idTariff = chg.idTariff\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkTariff, -- Mantener la clave subrogada original\n    c.idTariff,\n    c.tariff,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idTariff = chg.idTariff\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkTariff;\n\n\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE  scd.Warehouse \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Warehouse') )\n   DROP EXTERNAL TABLE scd.Warehouse;   \n\n\nCREATE EXTERNAL TABLE  scd.Warehouse \nWITH\n(\n\tLOCATION = 'silver/SCD/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.idWarehouse,\n        s.warehouse,\n        s.externalCode,\n        s.countryCode,\n        s.country,\n        s.city,\n        s.address,\n        s.description,\n        s.loadDate,\n        s.deltaDate,\n        HASHBYTES('SHA2_256',(CAST(s.idWarehouse AS NVARCHAR) + s.warehouse + s.externalcode + s.countryCode + s.country + s.city + s.address + s.description)) AS [$hash]\n    FROM etl.vw_dim_Warehouse s\n    LEFT JOIN current_data c\n        ON s.idWarehouse = c.idWarehouse\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkWarehouse), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idWarehouse) + max_key AS new_idSkWarehouse, -- Asignar nueva clave subrogada\n        chg.idWarehouse,\n        chg.warehouse,\n        chg.externalCode,\n        chg.countryCode,\n        chg.country,\n        chg.city,\n        chg.address,\n        chg.description,\n        chg.loadDate,\n        chg.deltaDate,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate,\n        chg.[$hash]\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idWarehouse = c.idWarehouse\n    CROSS JOIN max_surrogate_key\n    WHERE c.idWarehouse IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkWarehouse AS new_idSkWarehouse,\n        c.idWarehouse,\n        c.warehouse,\n        c.externalCode,\n        c.countryCode,\n        c.country,\n        c.city,\n        c.address,\n        c.description,\n        c.loadDate,\n        c.deltaDate,\n        c.isCurrent,\n        c.fromDate,\n        c.toDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idWarehouse = chg.idWarehouse\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\nfinal AS (\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nSELECT\n    c.idSkWarehouse, -- Mantener la clave subrogada original\n    c.idWarehouse,\n    c.warehouse,\n    c.externalCode,\n    c.countryCode,\n    c.country,\n    c.city,\n    c.address,\n    c.description,\n    c.loadDate,\n    c.deltaDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.fromDate,\n    GETDATE() AS toDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idWarehouse = chg.idWarehouse\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM unchanged_data\n\n)\nselect     idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM final\nORDER BY idSkWarehouse;\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load Silver Fact')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Silver SCD + Fact"
				},
				"content": {
					"query": "-- EXEC silver.FactSales\n-- ==============================================================================\n-- Procedimiento almacenado para cargar la tabla de hechos Sales en SilverlessSTG\n-- ==============================================================================\n\n\nCREATE OR ALTER PROCEDURE silver.FactSales\nWITH ENCRYPTION AS \n\n--------------\n-- Sales  --\n---------------\n-- DROP EXTERNAL TABLE [silver].[Sales]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('silver.Sales') )\n    DROP EXTERNAL TABLE silver.Sales \n\nCREATE  EXTERNAL TABLE SilverlessSTG.[silver].[Sales]\nWITH\n(\n\tLOCATION = 'silver/Fact/Sales', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT\n    -1 AS idSkSales,\n    -1 AS idSales,\n    -1 AS idArticles,\n    -1 AS idWarehouse,\n    -1 AS idClient,\n    -1 AS idPostalCodes,\n    -1 AS idCurrency,\n    -1 AS idTariff,\n    -1 AS idOperationType,\n    -1 AS idHours,\n    -1 AS idDate,\n    CAST('1990-01-01' AS datetime2) AS date,\n    CAST('1990-01-01' AS datetime2) dateOp,\n    'D' AS ticketNumber,\n    -1 AS quantity,\n    -1 AS unitPrice,\n    -1 AS unitCost,\n    -1 AS amtTotalEuros,\n    -1 AS amtNetEuros,\n    -1 AS amtTotalEurosDiscount,\n    -1 AS amtNetEurosDiscount,\n    -1 AS discountSale,\n    -1 AS amtTotalDiscount,\n    -1 AS amtNetDiscount,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1' + '-1' +'-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '1990-01-01' + '1990-01-01' + 'D' \n    + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1')) AS [$hash]\nUNION\nSELECT  ROW_NUMBER() OVER (ORDER BY idSales) AS idSkSales,\n        *, \n        HASHBYTES('SHA2_256',( CAST(idSales AS nvarchar) + CAST(idArticles AS nvarchar) + CAST(idWarehouse AS nvarchar) + CAST(idClient AS nvarchar) \n        + CAST(idPostalCodes AS nvarchar) + CAST(idCurrency AS nvarchar) + CAST(idTariff AS nvarchar) + CAST(idOperationType AS nvarchar) \n        + CAST(idHours AS nvarchar) + CAST(idDate AS nvarchar) + CAST(date AS nvarchar) + CAST(dateOp AS nvarchar) + ticketNumber \n        + CAST(quantity AS nvarchar) + CAST(unitPrice AS nvarchar) + CAST(unitCost AS nvarchar) + CAST(amtTotalEuros AS nvarchar) \n        + CAST(amtNetEuros AS nvarchar) + CAST(amtTotalEurosDiscount AS nvarchar) + CAST(amtNetEurosDiscount AS nvarchar) + CAST(discountSale AS nvarchar) \n        + CAST(amtTotalDiscount AS nvarchar) + CAST(amtNetDiscount AS nvarchar) )) AS [$hash]\nFROM [SilverlessSTG].[etl].[vw_fact_Sales]\n) a ORDER BY idSkSales",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Re-Load Fact')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Gold Fact CETAs"
				},
				"content": {
					"query": "-- EXEC gold.FactSales\n-- ==============================================================================\n-- Procedimiento almacenado para cargar la tabla de hechos Sales en GoldenlessSTG\n-- ==============================================================================\n\n\nCREATE OR ALTER PROCEDURE gold.FactSales\nWITH ENCRYPTION AS \n\n--------------\n-- Sales  --\n---------------\n-- DROP EXTERNAL TABLE [fact].[Sales]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('fact.Sales') )\n    DROP EXTERNAL TABLE fact.Sales \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[fact].[Sales]\nWITH\n(\n\tLOCATION = 'gold/Fact/Sales', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT\n    -1 AS idSkSales,\n    -1 AS idSales,\n    -1 AS idArticles,\n    -1 AS idskArticles,\n    -1 AS idWarehouse,\n    -1 AS idskWarehouse,\n    -1 AS idClient,\n    -1 AS idskClient,\n    -1 AS idPostalCodes,\n    -1 AS idskPostalCodes,\n    -1 AS idCurrency,\n    -1 AS idskCurrency,\n    -1 AS idTariff,\n    -1 AS idskTariff,\n    -1 AS idOperationType,\n    -1 AS idskOperationType,\n    -1 AS idHours,\n    -1 AS idskHours,\n    -1 AS idDate,\n    -1 AS idskDate,\n    CAST('1990-01-01' AS datetime2) AS date,\n    CAST('1990-01-01' AS datetime2) dateOp,\n    'D' AS ticketNumber,\n    -1 AS quantity,\n    -1 AS unitPrice,\n    -1 AS unitCost,\n    -1 AS amtTotalEuros,\n    -1 AS amtNetEuros,\n    -1 AS amtTotalEurosDiscount,\n    -1 AS amtNetEurosDiscount,\n    -1 AS discountSale,\n    -1 AS amtTotalDiscount,\n    -1 AS amtNetDiscount,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate\n\t--HASHBYTES('SHA2_256',('-1' + '-1' +'-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '1990-01-01' + '1990-01-01' + 'D' \n    --+ '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1' + '-1')) AS [$hash]\nUNION\n\nSELECT \n    s.idSkSales,\n    s.idSales,\n    s.idArticles,\n    COALESCE(a.idSkArticles, -1) AS idSkArticles,\n    s.idWarehouse,\n    COALESCE(w.idSkWarehouse, -1) AS idSkWarehouse,\n    s.idClient,\n    COALESCE(cl.idSkClient, -1) AS idSkClient, \n    s.idPostalCodes,\n    COALESCE(p.idSkPostalCodes, -1) AS idSkPostalCodes, \n    s.idCurrency,\n    COALESCE(cu.idSkCurrency, -1) AS idSkCurrency,\n    s.idTariff,\n    COALESCE(t.idSkTariff, -1) AS idSkTariff, \n    s.idOperationType,\n    COALESCE(o.idSkOperationType, -1) AS idSkOperationType,\n    s.idHours,\n    COALESCE(h.idSkHours, -1) AS idSkHours,\n    s.idDate,\n    COALESCE(d.idSkDate, -1) AS idSkDate,\n    s.date,\n    s.dateOp,\n    s.ticketNumber,\n    s.quantity,\n    s.unitPrice,\n    s.unitCost,\n    s.amtTotalEuros,\n    s.amtNetEuros,\n    s.amtTotalEurosDiscount,\n    s.amtNetEurosDiscount,\n    s.discountSale,\n    s.amtTotalDiscount,\n    s.amtNetDiscount,\n    s.loadDate,\n    s.deltaDate\nFROM [SilverlessSTG].[silver].[Sales] AS s\nLEFT JOIN [GoldenlessDWH].[dim].[Articles] AS a\n    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Client] AS cl\n    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Currency] AS cu\n    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Date] AS d\n    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Hours] AS h\n    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[OperationType] AS o\n    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[PostalCodes] AS p\n    ON s.idPostalCodes = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Tariff] AS t\n    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\nLEFT JOIN [GoldenlessDWH].[dim].[Warehouse] AS w\n    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\n\n\n\n\n\n) a ORDER BY idSkSales",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Re-Load Gold CETAs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Gold Dim CETAs"
				},
				"content": {
					"query": "---------------\n-- Articles  --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Articles]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles') )\n    DROP EXTERNAL TABLE dim.Articles \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash]\nUNION\nSELECT *\nFROM [SilverlessSTG].[scd].[Articles]\n) a ORDER BY idSkArticles\n\n\n--------------\n-- Client--\n--------------\n-- DROP EXTERNAL TABLE [dim].[Client]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Client') )\n    DROP EXTERNAL TABLE dim.Client \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Client]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkClient, \n\t-1  \t\t AS idClient, \n\t'D' \t\t AS name,\n\t'D' \t\t AS lastName1,\n\t'D' \t\t AS lastName2,\n\t'D' AS email,\n\t'D' AS phoneNumber,\n\tCAST('1900-01-01' AS date) AS birthDay,       \n\t-1 AS age,\n\t'D' AS gender,\n\t'D' AS country,\n\t'D' AS countryCode,\n\t'D'AS region,\n\t'D' AS address,\n\t'D' AS postalCode,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' +'D' + CAST('1900-01-01' AS nvarchar) + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Client]\n) C ORDER BY idSkClient\n\n\n--------------\n-- Currency --\n--------------\n-- DROP EXTERNAL TABLE [dim].[Currency]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Currency \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2)  AS toDate,\t\t\n\t1 \t\t\t AS isCurrent, \t\t \t\t\t \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Currency]\n) C ORDER BY idSkCurrency\n\n\n----------\n-- Date --\n----------\n-- DROP EXTERNAL TABLE [dim].[Date]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Date') )\n    DROP EXTERNAL TABLE dim.Date \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n        HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n        + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Date]\n) d ORDER BY idSkDate\n\n-----------\n-- Hours --\n-----------\n-- DROP EXTERNAL TABLE [dim].[Hours]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Hours') )\n    DROP EXTERNAL TABLE dim.Hours \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Hours]\n) h ORDER BY idSkHours\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE [dim].[OperationType]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.OperationType') )\n    DROP EXTERNAL TABLE dim.OperationType \nGO\n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_OperationType]\n) o ORDER BY idSkOperationType\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE [dim].[PostalCodes]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.PostalCodes') )\n    DROP EXTERNAL TABLE dim.PostalCodes \nGO\n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idpostalCode, \n\t'D' \t\t AS postalCodes,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_PostalCodes]\n) p ORDER BY idSkPostalCodes\n\n------------\n-- Tariff --\n------------\n-- DROP EXTERNAL TABLE [dim].[Tariff]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Tariff') )\n    DROP EXTERNAL TABLE dim.Tariff \n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[etl].[vw_dim_Tariff]\n) T ORDER BY idSkTariff\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Warehouse]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Warehouse') )\n    DROP EXTERNAL TABLE dim.Warehouse \nGO\n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM ( \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tidSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\tisCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Warehouse]\n) W ORDER BY idSkWarehouse",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SP Reload Dim Gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Gold Dim CETAs"
				},
				"content": {
					"query": "-- EXEC gold.ReloadDimSCD\n\nCREATE OR ALTER PROCEDURE gold.ReloadDimSCD\nWITH ENCRYPTION AS \n\n---------------\n-- Articles  --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Articles]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Articles'))\n    DROP EXTERNAL TABLE dim.Articles \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Articles]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  AS idSkArticles,\n\t-1  AS idArticles,\n\t'D' AS name,\n\t'D' AS externalcode,\n\t'D' AS size,\n\t-1  AS numSize,\n\t'D' AS colour,\n\t'D' AS category, \n\t'D' AS codLine,\n\t'D' AS line, \n\t'D' AS description, \n\t'D' AS season,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1  AS isCurrent,\n\t19900101 AS loadDate,\n\t19900101 AS deltaDate,\n\tHASHBYTES('SHA2_256',('-1'+'D'+'D'+'D'+'-1'+'D'+'D'+'D'+'D'+'D'+'D')) AS [$hash],\n\txxhash64()\nUNION\nSELECT *\nFROM [SilverlessSTG].[scd].[Articles]\n) a ORDER BY idSkArticles\n\n\n--------------\n-- Client--\n--------------\n-- DROP EXTERNAL TABLE [dim].[Client]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Client') )\n    DROP EXTERNAL TABLE dim.Client \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Client]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkClient, \n\t-1  \t\t AS idClient, \n\t'D' \t\t AS name,\n\t'D' \t\t AS lastName1,\n\t'D' \t\t AS lastName2,\n\t'D' AS email,\n\t'D' AS phoneNumber,\n\tCAST('1900-01-01' AS date) AS birthDay,       \n\t-1 AS age,\n\t'D' AS gender,\n\t'D' AS country,\n\t'D' AS countryCode,\n\t'D'AS region,\n\t'D' AS address,\n\t'D' AS postalCode,\t\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D' + 'D' + 'D' +'D' + CAST('1900-01-01' AS nvarchar) + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Client]\n) C ORDER BY idSkClient\n\n\n--------------\n-- Currency --\n--------------\n-- DROP EXTERNAL TABLE [dim].[Currency]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Currency') )\n    DROP EXTERNAL TABLE dim.Currency \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Currency]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkCurrency, \n\t-1  \t\t AS idCurrency, \n\t'D' \t\t AS name,\n\t'D' \t\t AS currency,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-12-31' AS datetime2)  AS toDate,\t\t\n\t1 \t\t\t AS isCurrent, \t\t \t\t\t \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D' + 'D')) AS [$hash]\nUNION ALL\nSELECT *\nFROM [SilverlessSTG].[scd].[Currency]\n) C ORDER BY idSkCurrency\n\n\n----------\n-- Date --\n----------\n-- DROP EXTERNAL TABLE [dim].[Date]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Date') )\n    DROP EXTERNAL TABLE dim.Date \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Date]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkDate, \n\t-1  \t\t AS idDate, \n\t-1 \t\t \t AS dateKey,\n\t'1990-01-01' AS date,\n\t'D'    \t     AS month, \n\t-1  \t\t AS monthNumber, \n\t-1 \t\t \t AS year,\n\t-1\t\t\t AS weekNumber,\n\t'D'    \t     AS dayWeek, \t\n\t-1\t\t\t AS dayWeekNumber, \n\t-1 \t\t \t AS yearDay,\n\t'D' \t\t AS quarter,\n\t'D'    \t     AS quadrimester, \t\n\t'D' \t\t AS semester, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '1990-01-01' + 'D' + '-1' + '-1' + '-1' + 'D' + '-1' + '-1' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idDate) AS idSkDate,\n\tidDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n        HASHBYTES('SHA2_256',(CAST(idDate AS NVARCHAR) + CAST(dateKey AS NVARCHAR) + CAST(date AS NVARCHAR) + month COLLATE DATABASE_DEFAULT + CAST(monthNumber AS NVARCHAR) + CAST(year AS NVARCHAR) \n        + CAST(weekNumber AS NVARCHAR) + dayWeek COLLATE DATABASE_DEFAULT + CAST(dayWeekNumber AS NVARCHAR) + CAST(yearDay AS NVARCHAR) + quarter + quadrimester + semester )) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Date]\n) d ORDER BY idSkDate\n\n-----------\n-- Hours --\n-----------\n-- DROP EXTERNAL TABLE [dim].[Hours]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Hours') )\n    DROP EXTERNAL TABLE dim.Hours \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[Hours]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkHours, \n\t-1  \t\t AS idHours, \n\t-1 \t\t \t AS hour,\n\t-1\t\t\t AS minute,\n\t'D'    \t     AS fullHour, \t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + '-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idHours) AS idSkHours,\n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idHours AS NVARCHAR) + CAST(hour AS NVARCHAR) + CAST(minute AS NVARCHAR) + fullHour)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Hours]\n) h ORDER BY idSkHours\n\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE [dim].[OperationType]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.OperationType') )\n    DROP EXTERNAL TABLE dim.OperationType \n\nCREATE  EXTERNAL TABLE GoldenlessDWH.[dim].[OperationType]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkOperationType , \n\t-1  \t\t AS idOperationType , \n\t'D' \t\t AS operation,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idOperationType) AS idSkOperationType,\n\tidOperationType,\n\toperation,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idOperationType AS NVARCHAR) + operation)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[OperationType]\n) o ORDER BY idSkOperationType\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE [dim].[PostalCodes]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.PostalCodes') )\n    DROP EXTERNAL TABLE dim.PostalCodes \n\nCREATE  EXTERNAL TABLE [dim].[PostalCodes]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkPostalCodes, \n\t-1  \t\t AS idpostalCode, \n\t'D' \t\t AS postalCodes,\n\t'D' \t\t AS region,\n\t'D' \t\t AS countryCode,\n\t'D' \t\t AS country,\t\t\t \t\t\t \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idPostalCodes) AS idSkPostalCodes,\n\tidPostalCodes,\n\tpostalCode,\n\tregion,\n\tcountryCode,\n\tcountry,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idPostalCodes AS NVARCHAR)+postalCode+region+countryCode+country)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[PostalCodes]\n) p ORDER BY idSkPostalCodes\n\n------------\n-- Tariff --\n------------\n-- DROP EXTERNAL TABLE [dim].[Tariff]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Tariff') )\n    DROP EXTERNAL TABLE dim.Tariff \n\nCREATE  EXTERNAL TABLE [dim].[Tariff]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM (\nSELECT \n\t-1  \t\t AS idSkTariff, \n\t-1  \t\t AS idTariff, \n\t'D' \t\t AS tariff, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tROW_NUMBER () OVER (ORDER BY idTariff) AS idSkTariff,\n\tidTariff,\n\ttariff,\n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 isCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idTariff AS NVARCHAR)+tariff)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Tariff]\n) T ORDER BY idSkTariff\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE [dim].[Warehouse]\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('dim.Warehouse') )\n    DROP EXTERNAL TABLE dim.Warehouse \n\nCREATE  EXTERNAL TABLE [dim].[Warehouse]\nWITH\n(\n\tLOCATION = 'gold/Dimensions/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nSELECT * FROM ( \nSELECT \n\t-1  \t\t AS idSkWarehouse, \n\t-1  \t\t AS idWarehouse, \n\t'D' \t\t AS warehouse, \n\t'D' \t\t AS externalcode, \n\t'D' \t\t AS countryCode, \n\t'D' \t\t AS country, \n\t'D' \t\t AS city, \n\t'D' \t\t AS address, \n\t'D' \t\t AS description, \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\t1 \t\t\t AS isCurrent, \n\t19900101 \t AS loadDate, \n\t19900101 \t AS deltaDate, \n\tHASHBYTES('SHA2_256',('-1' + '-1' + 'D' + 'D' + 'D' + 'D' + 'D' + 'D'+ 'D')) AS [$hash]\n\nUNION\n\nSELECT \n\tidSkWarehouse,\n\tidWarehouse,\n\twarehouse,\n\texternalcode,\n\tcountryCode,\n\tcountry,\n\tcity,\n\taddress,\n\tdescription,  \n\tCAST('1990-01-01' AS datetime2) AS fromDate,\n\tCAST('9999-01-01' AS datetime2) AS toDate,\n\tisCurrent,\n\tloadDate,\n\tdeltaDate,\n\tHASHBYTES('SHA2_256',(CAST(idWarehouse AS NVARCHAR)+warehouse+externalcode+countryCode+country+city+address+description)) AS [$hash]\nFROM\n\t[SilverlessSTG].[scd].[Warehouse]\n) W ORDER BY idSkWarehouse",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "GoldenlessDWH",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SP Silver Dim SCD2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "old CETAs/Silver SCD + Fact"
				},
				"content": {
					"query": "CREATE OR ALTER PROCEDURE silver.DimSCD\nWITH ENCRYPTION AS \n\n---------------\n-- Articles --\n---------------\n-- DROP EXTERNAL TABLE  scd.Articles\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Articles') )\n    DROP EXTERNAL TABLE scd.Articles;\n\n\nCREATE EXTERNAL TABLE  scd.Articles \nWITH\n(\n\tLOCATION = 'silver/SCD/Articles', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n    \n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Articles]\n    WHERE isCurrent = 1\n),\n\n-- CTE* creacion\n\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idArticles AS NVARCHAR)+ s.name + s.description COLLATE DATABASE_DEFAULT + s.externalcode + s.size + CAST(s.numSize AS NVARCHAR) + s.colour + s.category + s.codLine + s.line + s.season)) AS [$hash]\n    FROM etl.vw_dim_Articles s  -- METER AQUI DEFINICION LAS VISTAS CTE*\n    LEFT JOIN current_data c\n        ON s.idArticles = c.idArticles\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkArticles), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Articles]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idArticles) + max_key AS new_idSkArticles, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idArticles = c.idArticles\n    CROSS JOIN max_surrogate_key\n    WHERE c.idArticles IS NULL  OR (c.[$hash] != chg.[$hash] )\n), --select * from new_or_updated_data order by idarticles\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkarticles AS new_idSkArticles,\n        c.idArticles,\n        c.name,\n        c.externalCode,\n        c.size,\n        c.numSize,\n        c.colour,\n        c.category,\n        c.codLine,\n        c.line,\n        c.description,\n        c.season,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idArticles = chg.idArticles\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkArticles, -- Mantener la clave subrogada original\n    c.idArticles,\n    c.name,\n    c.externalCode,\n    c.size,\n    c.numSize,\n    c.colour,\n    c.category,\n    c.codLine,\n    c.line,\n    c.description,\n    c.season,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idArticles = chg.idArticles\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkArticles, \n    idArticles,\n    name,\n    externalCode,\n    size,\n    numSize,\n    colour,\n    category,\n    codLine,\n    line,\n    description,\n    season,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkArticles;\n\n\n---------------\n-- Currency --\n---------------\n-- DROP EXTERNAL TABLE  scd.Currency\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Currency') )\n    DROP EXTERNAL TABLE scd.Currency;\n\n\nCREATE EXTERNAL TABLE  scd.Currency \nWITH\n(\n\tLOCATION = 'silver/SCD/Currency', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Currency]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idCurrency AS NVARCHAR) + s.name + s.currency)) AS [$hash]\n    FROM etl.vw_dim_Currency s\n    LEFT JOIN current_data c\n        ON s.idCurrency = c.idCurrency\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkCurrency), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Currency]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idCurrency) + max_key AS new_idSkCurrency, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idCurrency = c.idCurrency\n    CROSS JOIN max_surrogate_key\n    WHERE c.idCurrency IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkCurrency AS new_idSkCurrency,\n        c.idCurrency,\n        c.name,\n        c.currency,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idCurrency = chg.idCurrency\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkCurrency, -- Mantener la clave subrogada original\n    c.idCurrency,\n    c.name,\n    c.currency,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idCurrency = chg.idCurrency\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkCurrency, \n    idCurrency,\n    name,\n    currency,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkCurrency;\n\n\n------------\n-- Client --\n------------\n--DROP EXTERNAL TABLE  scd.Client\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Client') )\n   DROP EXTERNAL TABLE scd.Client   \n\n\nCREATE EXTERNAL TABLE  scd.Client \nWITH\n(\n\tLOCATION = 'silver/SCD/Client', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Client]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idClient AS NVARCHAR) + s.name COLLATE DATABASE_DEFAULT + s.lastName1 COLLATE DATABASE_DEFAULT + s.lastName2 COLLATE DATABASE_DEFAULT\n\t    + s.email COLLATE DATABASE_DEFAULT + s.phoneNumber COLLATE DATABASE_DEFAULT + CAST(s.birthDay AS nvarchar) + CAST(s.age AS nvarchar) + s.gender + s.country + s.countryCode\n\t    + s.region + s.address + s.postalCode)) AS [$hash]\n    FROM etl.vw_dim_Client s\n    LEFT JOIN current_data c\n        ON s.idClient = c.idClient\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkClient), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Client]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idClient) + max_key AS new_idSkClient, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idClient = c.idClient\n    CROSS JOIN max_surrogate_key\n    WHERE c.idClient IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkClient AS new_idSkClient,\n        c.idClient,\n        c.name,\n        c.lastName1,\n        c.lastName2, \n        c.email,\n        c.phoneNumber,\n        c.birthDay,       \n        c.age,\n        c.gender,\n        c.country,\n        c.countryCode,\n        c.region,\n        c.address,\n        c.postalCode,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idClient = chg.idClient\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkClient, -- Mantener la clave subrogada original\n    c.idClient,\n    c.name,\n    c.lastName1,\n    c.lastName2, \n    c.email,\n    c.phoneNumber,\n    c.birthDay,       \n    c.age,\n    c.gender,\n    c.country,\n    c.countryCode,\n    c.region,\n    c.address,\n    c.postalCode,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idClient = chg.idClient\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkClient, \n    idClient,\n    name,\n    lastName1,\n    lastName2, \n    email,\n    phoneNumber,\n    birthDay,       \n    age,\n    gender,\n    country,\n    countryCode,\n    region,\n    address,\n    postalCode,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkClient;\n\n\n----------\n-- Date --\n----------\n--DROP EXTERNAL TABLE  scd.Date \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Date') )\n   DROP EXTERNAL TABLE scd.Date;  \n\n\nCREATE EXTERNAL TABLE  scd.Date \nWITH\n(\n\tLOCATION = 'silver/SCD/Date', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Date]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idDate AS NVARCHAR) + CAST(s.dateKey AS NVARCHAR) + CAST(s.date AS NVARCHAR) + s.month COLLATE DATABASE_DEFAULT + CAST(s.monthNumber AS NVARCHAR) + CAST(s.year AS NVARCHAR) \n        + CAST(s.weekNumber AS NVARCHAR) + s.dayWeek COLLATE DATABASE_DEFAULT + CAST(s.dayWeekNumber AS NVARCHAR) + CAST(s.yearDay AS NVARCHAR) + s.quarter + s.quadrimester + s.semester )) AS [$hash]\n    FROM etl.vw_dim_Date s\n    LEFT JOIN current_data c\n        ON s.idDate = c.idDate\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkDate), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Date]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idDate) + max_key AS new_idSkDate, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idDate = c.idDate\n    CROSS JOIN max_surrogate_key\n    WHERE c.idDate IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkDate AS new_idSkDate,\n        c.idDate,\n        c.dateKey,\n        c.date,\n        c.month,\n        c.monthNumber,\n        c.year,\n        c.weekNumber,\n        c.dayWeek,\n        c.dayWeekNumber,\n        c.yearDay,\n        c.quarter,\n        c.quadrimester,\n        c.semester,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idDate = chg.idDate\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkDate, -- Mantener la clave subrogada original\n    c.idDate,\n\tc.dateKey,\n\tc.date,\n\tc.month,\n\tc.monthNumber,\n\tc.year,\n\tc.weekNumber,\n\tc.dayWeek,\n\tc.dayWeekNumber,\n\tc.yearDay,\n\tc.quarter,\n\tc.quadrimester,\n\tc.semester,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idDate = chg.idDate\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkDate, \n    idDate,\n\tdateKey,\n\tdate,\n\tmonth,\n\tmonthNumber,\n\tyear,\n\tweekNumber,\n\tdayWeek,\n\tdayWeekNumber,\n\tyearDay,\n\tquarter,\n\tquadrimester,\n\tsemester,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkDate;\n\n\n---------------\n-- Hours --\n---------------\n-- DROP EXTERNAL TABLE  scd.Hours\nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Hours') )\n   DROP EXTERNAL TABLE scd.Hours;   \n\n\nCREATE EXTERNAL TABLE  scd.Hours \nWITH\n(\n\tLOCATION = 'silver/SCD/Hours', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Hours]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idHours AS NVARCHAR) + CAST(s.hour AS NVARCHAR) + CAST(s.minute AS NVARCHAR) + s.fullHour)) AS [$hash]\n    FROM etl.vw_dim_Hours s\n    LEFT JOIN current_data c\n        ON s.idHours = c.idHours\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkHours), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Hours]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idHours) + max_key AS new_idSkHours, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idHours = c.idHours\n    CROSS JOIN max_surrogate_key\n    WHERE c.idHours IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkHours AS new_idSkHours,\n        c.idHours,\n        c.hour,\n        c.minute,\n        c.fullHour,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idHours = chg.idHours\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkHours, -- Mantener la clave subrogada original\n\tc.idHours,\n\tc.hour,\n\tc.minute,\n\tc.fullHour,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idHours = chg.idHours\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkHours, \n\tidHours,\n\thour,\n\tminute,\n\tfullHour,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkHours;\n\n\n\n-------------------\n-- OperationType --\n-------------------\n-- DROP EXTERNAL TABLE  scd.OperationType \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.OperationType') )\n   DROP EXTERNAL TABLE scd.OperationType;   \n\n\nCREATE EXTERNAL TABLE  scd.OperationType \nWITH\n(\n\tLOCATION = 'silver/SCD/OperationType', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[OperationType]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n\t    HASHBYTES('SHA2_256',(CAST(s.idOperationType AS NVARCHAR) + s.operation)) AS [$hash]\n    FROM etl.vw_dim_OperationType s\n    LEFT JOIN current_data c\n        ON s.idOperationType = c.idOperationType\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkOperationType), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[OperationType]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idOperationType) + max_key AS new_idSkOperationType, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idOperationType = c.idOperationType\n    CROSS JOIN max_surrogate_key\n    WHERE c.idOperationType IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkOperationType AS new_idSkOperationType,\n        c.idOperationType,\n        c.operation,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idOperationType = chg.idOperationType\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkOperationType, -- Mantener la clave subrogada original\n    c.idOperationType,\n    c.operation,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idOperationType = chg.idOperationType\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkOperationType,\n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkOperationType, \n    idOperationType,\n    operation,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkOperationType;\n\n\n\n-----------------\n-- PostalCodes --\n-----------------\n--DROP EXTERNAL TABLE  scd.PostalCodes \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.PostalCodes') )\n    DROP EXTERNAL TABLE scd.PostalCodes; \n\nCREATE EXTERNAL TABLE  scd.PostalCodes \nWITH\n(\n\tLOCATION = 'silver/SCD/PostalCodes', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idPostalCodes AS NVARCHAR) + s.postalCode + s.region + s.countryCode + s.country)) AS [$hash]\n    FROM etl.vw_dim_PostalCodes s\n    LEFT JOIN current_data c\n        ON s.idPostalCodes = c.idPostalCodes\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkPostalCodes), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[PostalCodes]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idPostalCodes) + max_key AS new_idSkPostalCodes, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idPostalCodes = c.idPostalCodes\n    CROSS JOIN max_surrogate_key\n    WHERE c.idPostalCodes IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkPostalCodes AS new_idSkPostalCodes,\n        c.idPostalCodes,\n        c.postalCode,\n        c.region,\n        c.countryCode,\n        c.country,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idPostalCodes = chg.idPostalCodes\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkPostalCodes, -- Mantener la clave subrogada original\n    c.idPostalCodes,\n    c.postalCode,\n    c.region,\n    c.countryCode,\n    c.country,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idPostalCodes = chg.idPostalCodes\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkPostalCodes,\n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkPostalCodes, \n    idPostalCodes,\n    postalCode,\n    region,\n    countryCode,\n    country,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkPostalCodes;\n\n\n\n------------\n-- Tariff --\n------------\n--DROP EXTERNAL TABLE  scd.Tariff \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Tariff') )\n   DROP EXTERNAL TABLE scd.Tariff;   \n\n\nCREATE EXTERNAL TABLE  scd.Tariff \nWITH\n(\n\tLOCATION = 'silver/SCD/Tariff', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Tariff]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.*,\n        HASHBYTES('SHA2_256',(CAST(s.idTariff AS NVARCHAR) + s.tariff)) AS [$hash]\n    FROM etl.vw_dim_Tariff s\n    LEFT JOIN current_data c\n        ON s.idTariff = c.idTariff\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkTariff), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Tariff]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idTariff) + max_key AS new_idSkTariff, -- Asignar nueva clave subrogada\n        chg.*,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idTariff = c.idTariff\n    CROSS JOIN max_surrogate_key\n    WHERE c.idTariff IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkTariff AS new_idSkTariff,\n        c.idTariff,\n        c.tariff,\n        c.fromDate,\n        c.toDate,\n        c.isCurrent,\n        c.loadDate,\n        c.deltaDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idTariff = chg.idTariff\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nfinal AS (\nSELECT\n    c.idSkTariff, -- Mantener la clave subrogada original\n    c.idTariff,\n    c.tariff,\n    c.fromDate,\n    GETDATE() AS toDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.loadDate,\n    c.deltaDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idTariff = chg.idTariff\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkTariff, \n    idTariff,\n    tariff,\n    fromDate,\n    toDate,\n    isCurrent,\n    loadDate,\n    deltaDate,\n    [$hash]\nFROM unchanged_data\n)\nSELECT     \n    * \nFROM final\nORDER BY idSkTariff;\n\n\n\n---------------\n-- Warehouse --\n---------------\n-- DROP EXTERNAL TABLE  scd.Warehouse \nIF EXISTS ( SELECT * FROM sys.external_tables WHERE object_id = OBJECT_ID('scd.Warehouse') )\n   DROP EXTERNAL TABLE scd.Warehouse;   \n\n\nCREATE EXTERNAL TABLE  scd.Warehouse \nWITH\n(\n\tLOCATION = 'silver/SCD/Warehouse', \n\tDATA_SOURCE = datalake1pgc,\n\tFILE_FORMAT = ParquetFileFormat\n) AS \n\nWITH current_data AS (\n    SELECT *\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n    WHERE isCurrent = 1\n),\n-- Identificar los registros que han cambiado o son nuevos\nchanges AS (\n    SELECT\n        s.idWarehouse,\n        s.warehouse,\n        s.externalCode,\n        s.countryCode,\n        s.country,\n        s.city,\n        s.address,\n        s.description,\n        s.loadDate,\n        s.deltaDate,\n        HASHBYTES('SHA2_256',(CAST(s.idWarehouse AS NVARCHAR) + s.warehouse + s.externalcode + s.countryCode + s.country + s.city + s.address + s.description)) AS [$hash]\n    FROM etl.vw_dim_Warehouse s\n    LEFT JOIN current_data c\n        ON s.idWarehouse = c.idWarehouse\n),\n-- Obtener el valor mÃ¡ximo de la clave subrogada existente en la tabla\nmax_surrogate_key AS (\n    SELECT COALESCE(MAX(idSkWarehouse), 0) AS max_key\n    FROM [GoldenlessDWH].[dim].[Warehouse]\n),\n-- Calcular claves subrogadas para los registros nuevos o modificados\nnew_or_updated_data AS (\n    SELECT\n        ROW_NUMBER() OVER (ORDER BY chg.idWarehouse) + max_key AS new_idSkWarehouse, -- Asignar nueva clave subrogada\n        chg.idWarehouse,\n        chg.warehouse,\n        chg.externalCode,\n        chg.countryCode,\n        chg.country,\n        chg.city,\n        chg.address,\n        chg.description,\n        chg.loadDate,\n        chg.deltaDate,\n        1 AS isCurrent,\n        GETDATE() AS fromDate,\n        '9999-12-31' AS toDate,\n        chg.[$hash]\n    FROM changes chg\n    LEFT JOIN current_data c\n        ON chg.idWarehouse = c.idWarehouse\n    CROSS JOIN max_surrogate_key\n    WHERE c.idWarehouse IS NULL  OR (c.[$hash] != chg.[$hash] )\n),\n-- Obtener los registros que no han cambiado (mantener clave subrogada)\nunchanged_data AS (\n    SELECT\n        c.idSkWarehouse AS new_idSkWarehouse,\n        c.idWarehouse,\n        c.warehouse,\n        c.externalCode,\n        c.countryCode,\n        c.country,\n        c.city,\n        c.address,\n        c.description,\n        c.loadDate,\n        c.deltaDate,\n        c.isCurrent,\n        c.fromDate,\n        c.toDate,\n        c.[$hash]\n    FROM current_data c\n    JOIN changes chg\n        ON c.idWarehouse = chg.idWarehouse\n    -- Seleccionar solo las filas que no han cambiado\n    WHERE c.[$hash] = chg.[$hash]\n),\nfinal AS (\n-- UniÃ³n final con las versiones antiguas, actualizadas y sin cambios\nSELECT\n    c.idSkWarehouse, -- Mantener la clave subrogada original\n    c.idWarehouse,\n    c.warehouse,\n    c.externalCode,\n    c.countryCode,\n    c.country,\n    c.city,\n    c.address,\n    c.description,\n    c.loadDate,\n    c.deltaDate,\n    0 AS isCurrent, -- Marcar las versiones antiguas como no actuales\n    c.fromDate,\n    GETDATE() AS toDate,\n    c.[$hash]\nFROM current_data c\nJOIN changes chg\n    ON c.idWarehouse = chg.idWarehouse\nWHERE c.[$hash] != chg.[$hash]\n\nUNION ALL\n\n-- Insertar los registros nuevos o modificados con nuevas claves subrogadas\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM new_or_updated_data\n\nUNION ALL\n\n-- Insertar los registros que no han cambiado\nSELECT\n    new_idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM unchanged_data\n\n)\nselect     idSkWarehouse, \n    idWarehouse,\n    warehouse,\n    externalCode,\n    countryCode,\n    country,\n    city,\n    address,\n    description,\n    loadDate,\n    deltaDate,\n    isCurrent,\n    fromDate,\n    toDate,\n    [$hash]\nFROM final\nORDER BY idSkWarehouse;\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SilverlessSTG",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoadSCD2AndFact')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f6ebca11-59f4-42f2-a33b-adef6c1fd908"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Import modules**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format,concat_ws,xxhash64,current_timestamp\r\n",
							"from pyspark.sql import SparkSession,Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"import json\r\n",
							"import re\r\n",
							"from datetime import datetime\r\n",
							"from azure.storage.blob import BlobServiceClient\r\n",
							"import io\r\n",
							"import pandas as pd\r\n",
							"import csv"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#%%sql\r\n",
							"# USE default;\r\n",
							"# DROP DATABASE  gold CASCADE ;\r\n",
							"# USE default; \r\n",
							"# CREATE DATABASE IF NOT EXISTS gold; fact_Sales"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Fact Load and Loockup query**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"factList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Sales\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idDesgloseVenta AS idSales,\r\n",
							"                        d.idArticulo AS idArticles,\r\n",
							"                        c.idAlmacen AS idWarehouse,\r\n",
							"                        c.idCliente AS idClient,\r\n",
							"                        c.idCodigoPostal AS idPostalCode,\r\n",
							"                        c.idDivisa AS idCurrency,\r\n",
							"                        c.idTarifa AS idTariff,\r\n",
							"                        c.idTipoOperacion AS idOperationType,\r\n",
							"                        c.idHora AS idHours,\r\n",
							"                        c.idFecha AS idDate,\r\n",
							"                        f.fecha AS  date,\r\n",
							"                        CAST(CONCAT(DATE_FORMAT(f.fecha, 'yyyy-MM-dd'), ' ', h.horaCompleta) AS TIMESTAMP) AS dateOp,\r\n",
							"                        COALESCE(c.codigoTicket, 'D') AS ticketNumber,\r\n",
							"                        COALESCE(d.Cantidad, 0) AS quantity,\r\n",
							"                        COALESCE(d.PrecioUnitario, 0)  AS unitPrice,\r\n",
							"                        COALESCE(d.CosteUnitario, 0)  AS unitCost,\r\n",
							"                        COALESCE(d.importeBruto, 0)  AS amtTotalEuros,\r\n",
							"                        COALESCE(d.importeNeto, 0)  AS amtNetEuros,\r\n",
							"                        COALESCE(d.importeDescuento, 0)  AS amtTotalEurosDiscount,\r\n",
							"                        COALESCE(d.importeNetoDescuento, 0) AS amtNetEurosDiscount,\r\n",
							"                        CASE WHEN idTarifa IN (2,3) THEN 1\r\n",
							"                            ELSE 0 \r\n",
							"                        END AS discountSale,\r\n",
							"                        CASE WHEN idTarifa = 0 THEN 0\r\n",
							"                            ELSE (d.importeBruto - d.importeDescuento) \r\n",
							"                        END AS amtTotalDiscount,\r\n",
							"                        CASE WHEN idTarifa = 0 THEN 0 \r\n",
							"                            ELSE (d.importeNeto - d.importeNetoDescuento) \r\n",
							"                        END AS amtNetDiscount,\r\n",
							"                        GREATEST(d.fechaCarga, c.fechaCarga) AS loadDate,\r\n",
							"                        GREATEST(d.fechaDelta, c.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.desgloseVenta AS d\r\n",
							"                    INNER JOIN silver.cabeceraVenta AS c\r\n",
							"                        ON d.idcabecerasVentas = c.idcabeceraVenta\r\n",
							"                    LEFT JOIN silver.Fechas  AS f\r\n",
							"                        On f.idFechas = c.idFecha\r\n",
							"                    LEFT JOIN silver.horas  AS h\r\n",
							"                        On h.idHoras = c.idHora\r\n",
							"                    WHERE GREATEST(d.fechaDelta, c.fechaDelta) >=\r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"lookupFact =  [ \r\n",
							"        {\r\n",
							"            \"name\": \"Sales\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT    \r\n",
							"                        s.*,\r\n",
							"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
							"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
							"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
							"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
							"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
							"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
							"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
							"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
							"                        COALESCE(d.skDate, -1) AS skDate\r\n",
							"                FROM temp_sales_view as s\r\n",
							"                LEFT JOIN gold.dim_Articles AS a\r\n",
							"                    ON s.idArticles = a.idArticles AND s.dateOp >= a.fromDate AND s.dateOp < a.toDate\r\n",
							"                LEFT JOIN gold.dim_Client AS cl\r\n",
							"                    ON s.idClient = cl.idClient AND s.dateOp >= cl.fromDate AND s.dateOp < cl.toDate\r\n",
							"                LEFT JOIN gold.dim_Currency AS cu\r\n",
							"                    ON s.idCurrency = cu.idCurrency AND s.dateOp >= cu.fromDate AND s.dateOp < cu.toDate\r\n",
							"                LEFT JOIN gold.dim_Date AS d\r\n",
							"                    ON s.idDate = d.idDate AND s.dateOp >= d.fromDate AND s.dateOp < d.toDate\r\n",
							"                LEFT JOIN gold.dim_Hours AS h\r\n",
							"                    ON s.idHours = h.idHours AND s.dateOp >= h.fromDate AND s.dateOp < h.toDate\r\n",
							"                LEFT JOIN gold.dim_OperationType AS o\r\n",
							"                    ON s.idOperationType = o.idOperationType AND s.dateOp >= o.fromDate AND s.dateOp < o.toDate\r\n",
							"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
							"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp >= p.fromDate AND s.dateOp < p.toDate\r\n",
							"                LEFT JOIN gold.dim_Tariff AS t\r\n",
							"                    ON s.idTariff = t.idTariff AND s.dateOp >= t.fromDate AND s.dateOp < t.toDate\r\n",
							"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
							"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp >= w.fromDate AND s.dateOp < w.toDate\r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Dim load query**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dimensionsList = [ \r\n",
							"        {\r\n",
							"            \"name\": \"Articles\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idArticulos                                                                                             AS idArticles,\r\n",
							"                            COALESCE(nombre, 'D')                                                                                   AS name,\r\n",
							"                            COALESCE(descripcion, 'D')                                                                              AS description,\r\n",
							"                            COALESCE(codigoReferencia, 'D')                                                                         AS externalCode,\r\n",
							"                            COALESCE(t.talla, 'D')                                                                                  AS size,\r\n",
							"                            COALESCE( t.numeroTalla, -1)                                                                            AS numSize,\r\n",
							"                            COALESCE(co.color, 'D')                                                                                 AS colour,\r\n",
							"                            COALESCE(ca.categoria, 'D')                                                                             AS category,\r\n",
							"                            COALESCE(l.codigoLinea, 'D')                                                                            AS codLine,\r\n",
							"                            COALESCE(l.Linea, 'D')                                                                                  AS line, \r\n",
							"                            CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                                 WHEN a.idCategoria IN (2,4,6,7,9) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                                 ELSE CAST(CONCAT('PV',CAST(a.idTemporada AS string)) AS string)\r\n",
							"                            END                                                                                                     AS season,              \r\n",
							"                            GREATEST(a.fechaCarga, t.fechaCarga, co.fechaCarga, ca.fechaCarga, l.fechaCarga)                        AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta)                        AS deltaDate\r\n",
							"                    FROM silver.articulos AS a\r\n",
							"                        LEFT JOIN silver.talla AS t\r\n",
							"                            ON t.idTalla = a.idTalla\r\n",
							"                        LEFT JOIN silver.color AS co\r\n",
							"                            ON co.idColor = a.idColor\r\n",
							"                        LEFT JOIN silver.categoria AS ca -- select * from silver.categoria\r\n",
							"                            ON ca.idCategoria = a.idCategoria\r\n",
							"                        LEFT JOIN silver.Linea AS l\r\n",
							"                            ON l.idLinea = a.idLinea\r\n",
							"                    WHERE GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Client\",\r\n",
							"            \"query\": \"\"\"              \r\n",
							"                    SELECT  idCliente                                           AS idClient,\r\n",
							"                            COALESCE(c.nombre, 'D')                             AS name,\r\n",
							"                            COALESCE(apellido1, 'D')                            AS lastName1,\r\n",
							"                            COALESCE(apellido2, 'D')                            AS lastName2, \r\n",
							"                            COALESCE(email, 'D')                                AS email,\r\n",
							"                            COALESCE(telefono, 'D')                             AS phoneNumber,\r\n",
							"                            COALESCE(CAST(cumpleanos AS STRING), '1900-01-01')  AS birthDay,       \r\n",
							"                            YEAR(current_date()) - YEAR(CAST(cumpleanos AS DATE)) \r\n",
							"                                - CASE \r\n",
							"                                    WHEN MONTH(current_date()) < MONTH(CAST(cumpleanos AS DATE)) \r\n",
							"                                        OR (MONTH(current_date()) = MONTH(CAST(cumpleanos AS DATE)) AND DAY(current_date()) < DAY(CAST(cumpleanos AS DATE)))\r\n",
							"                                    THEN 1\r\n",
							"                                    ELSE 0\r\n",
							"                            END                                                 AS age,\r\n",
							"                            CASE WHEN hombre = 1 THEN 'Hombre' \r\n",
							"                                 ELSE 'Mujer' \r\n",
							"                            END                                                 AS gender,\r\n",
							"                            COALESCE(p.nombre, 'D')                             AS country,\r\n",
							"                            COALESCE(p.codigoPais, 'D')                         AS countryCode,\r\n",
							"                            COALESCE(cp.region, 'D')                            AS region,\r\n",
							"                            COALESCE(c.Direcion, 'D')                           AS address,  \r\n",
							"                            COALESCE(codigoPostal, 'D')                         AS postalCode,\r\n",
							"                            COALESCE(activo, false)                             AS active,  -- Cambiado 0 a false aquÃ­\r\n",
							"                            GREATEST(c.fechaCarga, p.fechaCarga, cp.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) AS deltaDate\r\n",
							"                        FROM silver.cliente AS c\r\n",
							"                            LEFT JOIN silver.pais AS p \r\n",
							"                                ON c.idPais = p.idPais\r\n",
							"                            INNER JOIN silver.codigoPostal AS cp \r\n",
							"                                ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\r\n",
							"                        WHERE GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Currency\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idDivisa              AS idCurrency,\r\n",
							"                            COALESCE(nombre, 'D') AS name,\r\n",
							"                            COALESCE(Divisa, 'D') AS currency,\r\n",
							"                            fechaCarga            AS loadDate,\r\n",
							"                            fechaDelta            AS deltaDate\r\n",
							"                    FROM silver.divisa\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Date\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idFechas                                  AS idDate,\r\n",
							"                            ClaveFecha                                AS dateKey,\r\n",
							"                            Fecha                                     AS date,\r\n",
							"                            COALESCE(Mes, 'D')                        AS month,\r\n",
							"                            COALESCE(NumeroMes, -1)                   AS monthNumber,\r\n",
							"                            COALESCE(Ano, -1)                         AS year,\r\n",
							"                            COALESCE(NumeroSemana, -1)                AS weekNumber,\r\n",
							"                            COALESCE(DiaSemana, 'D')                  AS dayWeek,\r\n",
							"                            COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\r\n",
							"                            COALESCE(DiaAno, -1)                      AS yearDay,\r\n",
							"                            CASE WHEN Trimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Trimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Trimestre = 3 THEN 'Q3'\r\n",
							"                                ELSE '-1' END                         AS quarter,\r\n",
							"                            CASE WHEN Cuatrimestre = 1 THEN 'Q1'\r\n",
							"                                WHEN Cuatrimestre = 2 THEN 'Q2'\r\n",
							"                                WHEN Cuatrimestre = 3 THEN 'Q3'\r\n",
							"                                WHEN Cuatrimestre = 4 THEN 'Q4'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS quadrimester,\r\n",
							"                            CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\r\n",
							"                                WHEN Cuatrimestre IN (2,4) THEN 'S2'\r\n",
							"                                ELSE '-1' \r\n",
							"                            END                                       AS semester,\r\n",
							"                            fechaCarga                                AS loadDate,\r\n",
							"                            fechaDelta                                AS deltaDate\r\n",
							"                    FROM silver.fechas\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Hours\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idHoras                     AS idHours,\r\n",
							"                            COALESCE(Hora, -1)          AS hour,\r\n",
							"                            COALESCE(Minuto, -1)        AS minute,\r\n",
							"                            COALESCE(HoraCompleta, 'D') AS fullHour,\r\n",
							"                            fechaCarga                  AS loadDate,\r\n",
							"                            fechaDelta                  AS deltaDate\r\n",
							"                    FROM silver.horas\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"OperationType\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTipoOperacion          AS idOperationType,\r\n",
							"                            COALESCE(Operacion, 'D') AS operation,\r\n",
							"                            fechaCarga               AS loadDate,\r\n",
							"                            fechaDelta               AS deltaDate\r\n",
							"                    FROM silver.tipoOperacion\r\n",
							"                    WHERE fechaDelta >=\r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"PostalCode\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idCodigoPostal                       AS idPostalCode,\r\n",
							"                            COALESCE(codigoPostal, 'D')          AS postalCode,\r\n",
							"                            COALESCE(region, 'D')                AS region,\r\n",
							"                            COALESCE(c.codigoPais, 'D')          AS countryCode,\r\n",
							"                            COALESCE(nombre, 'D')                AS country,\r\n",
							"                            GREATEST(c.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(c.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.codigoPostal AS c\r\n",
							"                    LEFT JOIN silver.pais AS p\r\n",
							"                        ON p.codigoPais = c.codigoPais\r\n",
							"                    WHERE GREATEST(c.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"name\": \"Tariff\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idTarifa              AS idTariff,\r\n",
							"                            COALESCE(Tarifa, 'D') AS tariff,\r\n",
							"                            fechaCarga            AS loadDate,\r\n",
							"                            fechaDelta            AS deltaDate       \r\n",
							"                    FROM silver.tarifa\r\n",
							"                    WHERE fechaDelta >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"        ,\r\n",
							"        {\r\n",
							"            \"name\": \"Warehouse\",\r\n",
							"            \"query\": \"\"\"\r\n",
							"                    SELECT  idAlmacenes                          AS idWarehouse,\r\n",
							"                            COALESCE(a.Nombre, 'D')              AS warehouse,\r\n",
							"                            COALESCE(a.codigoAlmacen, 'D')       AS externalCode,\r\n",
							"                            COALESCE(p.codigoPais, 'D')          AS countryCode, \r\n",
							"                            COALESCE(p.nombre, 'D')              AS country,\r\n",
							"                            COALESCE(a.ciudad, 'D')              AS city,\r\n",
							"                            COALESCE(a.Direccion, 'D')           AS address,\r\n",
							"                            COALESCE(a.descripcion, 'D')         AS description,\r\n",
							"                            GREATEST(a.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
							"                            GREATEST(a.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
							"                    FROM silver.almacenes AS a\r\n",
							"                    LEFT JOIN silver.pais AS p\r\n",
							"                        ON p.idpais = a.idPais\r\n",
							"                    WHERE GREATEST(a.fechaDelta, p.fechaDelta) >= \r\n",
							"                     \"\"\"\r\n",
							"        }\r\n",
							"    ]"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Configuration**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# parameter string to array\r\n",
							"tables_to_load = json.loads(tables_to_load)\r\n",
							"\r\n",
							"today_datetime = datetime.now()\r\n",
							"\r\n",
							"# delta_date_filter = (today_datetime.strftime('%Y%m%d')) # 'yyyyMMdd'\r\n",
							"# delta_date_filter = '19900101' # Debud\r\n",
							"\r\n",
							"update_date_string = today_datetime.strftime('%Y-%m-%d') # 'yyyy-mm-dd'\r\n",
							"#update_date_string = '1990-01-01' # test\r\n",
							"\r\n",
							"dict_tables_to_load = []\r\n",
							"\r\n",
							"for row in tables_to_load:\r\n",
							"    dict_tables_to_load.append({\r\n",
							"    \"SchemaName\": row[\"SchemaName\"],\r\n",
							"    \"TableName\": row[\"TableName\"],\r\n",
							"    \"UpdateDate\": row[\"UpdateDate\"],\r\n",
							"    \"Load\": row[\"Load\"]\r\n",
							"})\r\n",
							"\r\n",
							"dim_tables = []\r\n",
							"fact_tables = []\r\n",
							"fact_lockup = []\r\n",
							"\r\n",
							"# Clasificar los datos\r\n",
							"for row in dict_tables_to_load:\r\n",
							"    if row['SchemaName'] == 'dim':\r\n",
							"        dim_tables.append(row)\r\n",
							"    elif row['SchemaName'] == 'fact':\r\n",
							"        fact_tables.append(row)\r\n",
							"\r\n",
							"# Filter only table with Load = 'Y'\r\n",
							"load_y_tables = {table['TableName'] for table in dim_tables if table['Load'] == 'Y'}\r\n",
							"filtered_dimensionsList = [\r\n",
							"    dim for dim in dimensionsList if dim[\"name\"] in load_y_tables\r\n",
							"]\r\n",
							"\r\n",
							"load_y_tables2 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
							"filtered_factList = [\r\n",
							"    dim for dim in factList if dim[\"name\"] in load_y_tables2\r\n",
							"]\r\n",
							"\r\n",
							"load_y_tables3 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
							"filtered_factList_Lookup = [\r\n",
							"    dim for dim in lookupFact if dim[\"name\"] in load_y_tables3\r\n",
							"]\r\n",
							"\r\n",
							"def update_query_with_update_date(table_info_list, query_list):\r\n",
							"    for table_info in table_info_list:\r\n",
							"        table_name = table_info['TableName']\r\n",
							"        update_date = table_info['UpdateDate']\r\n",
							"        \r\n",
							"        for query_info in query_list:\r\n",
							"            if query_info['name'] == table_name:\r\n",
							"                query_info['query'] += f\" '{update_date}'\\n\"\r\n",
							"    return query_list\r\n",
							"\r\n",
							"# deltaDate to filter Load\r\n",
							"updated_list_Dim = update_query_with_update_date(dim_tables, filtered_dimensionsList)\r\n",
							"updated_list2_Fact = update_query_with_update_date(fact_tables, filtered_factList)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Load SCD2 Dimensions**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"results = []\r\n",
							"exception = []\r\n",
							"\r\n",
							"for file in updated_list_Dim:\r\n",
							"    file_result = {\r\n",
							"        'table': f\"dim_{file['name']}\",\r\n",
							"        'status': 'Incorrect',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now()) \r\n",
							"    }\r\n",
							"\r\n",
							"    try:\r\n",
							"\r\n",
							"        table_name = file[\"name\"].split('_')[0]\r\n",
							"        key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
							"\r\n",
							"        key_columns = key_columns_str.split(',')\r\n",
							"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"        delta_table_path_dimensions = f\"{data_lake_container}/{golden_folder_dimensions}/{table_name}\"\r\n",
							"\r\n",
							"        df = spark.sql(file[\"query\"].strip()) \r\n",
							"        sdf = df.cache()\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"There is no new data\"\r\n",
							"            results.append(file_result)\r\n",
							"            continue\r\n",
							"\r\n",
							"        \r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path_dimensions):\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            all_columns = sdf.columns\r\n",
							"            start_columns = [f\"id{table_name}\"]\r\n",
							"            end_columns = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
							"            other_columns = [col for col in all_columns if col not in start_columns + end_columns]\r\n",
							"            new_order_columns = start_columns + other_columns + end_columns\r\n",
							"            sdf = sdf.select(*new_order_columns)\r\n",
							"\r\n",
							"            next_surrogate_key = spark.sql(f\"SELECT COALESCE(MAX(sk{table_name}), 0) + 1 AS next_key FROM gold.dim_{table_name}\").collect()[0][\"next_key\"]\r\n",
							"\r\n",
							"            sdf.createOrReplaceTempView(\"temp_view\")\r\n",
							"\r\n",
							"            # End current version\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                MERGE INTO gold.dim_{table_name}  \r\n",
							"                AS existing\r\n",
							"                USING temp_view AS updates\r\n",
							"                ON {\" AND \".join([f\"existing.{key}=updates.{key}\" for key in key_columns])}\r\n",
							"                WHEN MATCHED AND existing.isCurrent = 1 AND existing.hash != updates.hash THEN\r\n",
							"                UPDATE SET\r\n",
							"                    existing.toDate = current_date(),\r\n",
							"                    existing.isCurrent = 0\r\n",
							"            \"\"\")\r\n",
							"            \r\n",
							"            # Load new versions\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                INSERT INTO gold.dim_{table_name}    \r\n",
							"                SELECT \r\n",
							"                    {next_surrogate_key} + row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS sk{table_name},  \r\n",
							"                    updates.*\r\n",
							"                FROM temp_view AS updates\r\n",
							"                LEFT JOIN gold.dim_{table_name} AS existing\r\n",
							"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])} AND existing.isCurrent = 1\r\n",
							"                WHERE \r\n",
							"                    existing.id{table_name} IS NULL OR \r\n",
							"                    (existing.hash != updates.hash)\r\n",
							"            \"\"\")\r\n",
							"\r\n",
							"            # Save metadata\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.dim_{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            if last_result:\r\n",
							"                file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
							"                file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
							"            else:\r\n",
							"                print(f\"No hay registros de operaciÃ³n MERGE para la tabla {file['name']}\")\r\n",
							"                file_result['inserted_rows'] = 0\r\n",
							"                file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"        else: # New table\r\n",
							"      \r\n",
							"            last_surrogate_key = 0  \r\n",
							"            sdf = sdf.withColumn(f\"sk{table_name}\", F.col(f\"id{table_name}\")) # Firs load sk = id\r\n",
							"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"            # Hash\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\" ,'fechaCarga', 'fechaDelta','fromDate', 'toDate', 'isCurrent')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"            # Reorganize columns\r\n",
							"            all_columns = sdf.columns\r\n",
							"            start_columns = [f\"sk{table_name}\", f\"id{table_name}\"]\r\n",
							"            end_columns = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
							"            other_columns = [col for col in all_columns if col not in start_columns + end_columns]\r\n",
							"            new_order_columns = start_columns + other_columns + end_columns\r\n",
							"            sdf_reordenado = sdf.select(*new_order_columns)\r\n",
							"            \r\n",
							"            # Save delta table \r\n",
							"            sdf_reordenado.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path_dimensions)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.dim_{table_name} USING DELTA LOCATION \\'{delta_table_path_dimensions}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['inserted_rows'] = sdf.count()\r\n",
							"            file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Load Fact Table**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for file, file2 in zip(updated_list2_Fact, filtered_factList_Lookup):\r\n",
							"    file_result = {\r\n",
							"        'table': f\"fact_{file['name']}\",\r\n",
							"        'status': 'Incorrect',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now())  \r\n",
							"    }\r\n",
							"    table_name = file[\"name\"].split('_')[0]\r\n",
							"    key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
							"\r\n",
							"    key_columns = key_columns_str.split(',')\r\n",
							"    conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"    delta_table_path_fact = f\"{data_lake_container}/{golden_folder_fact}/{table_name}\"\r\n",
							"    \r\n",
							"    df = spark.sql(file[\"query\"].strip())\r\n",
							"    sdf = df.cache()\r\n",
							"\r\n",
							"    try:\r\n",
							"\r\n",
							"        # Check dataframe content\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ la tabla {file['name']} porque no contiene nuevos datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"There is no new data\"\r\n",
							"            results.append(file_result)\r\n",
							"            continue\r\n",
							"\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path_fact):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path_fact)\r\n",
							"            \r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
							"            sdf =spark.sql(f\"\"\"\r\n",
							"                        {file2[\"query\"].strip()} \r\n",
							"                        \"\"\")\r\n",
							"\r\n",
							"            max_skSales = spark.sql(f\"SELECT COALESCE(MAX(sk{table_name}), 0) AS next_key FROM gold.fact_{table_name}\").collect()[0][\"next_key\"]  # +1???           \r\n",
							"            sdf = sdf.withColumn(\"skSales\", F.expr(f\"{max_skSales} + row_number() OVER (ORDER BY id{table_name})\"))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('loadDate', 'deltaDate', 'skSales')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"                      \r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view_merge\")\r\n",
							"\r\n",
							"            table_schema = spark.table(f\"gold.fact_{table_name}\").schema\r\n",
							"\r\n",
							"            # Get columns of sdf\r\n",
							"            columns = [col for col in sdf.columns if col not in ('skSales', 'hash')]\r\n",
							"\r\n",
							"            update_set = \", \".join([f\"existing.{col} = updates.{col}\" for col in columns] + [\"existing.hash = updates.hash\"])\r\n",
							"            insert_columns = \", \".join(columns + ['skSales', 'hash'])\r\n",
							"            insert_values = \", \".join([f\"updates.{col}\" for col in columns] + ['updates.skSales', 'updates.hash'])\r\n",
							"\r\n",
							"            # MERGE / INSERT\r\n",
							"            spark.sql(f\"\"\"\r\n",
							"                MERGE INTO gold.fact_{table_name} AS existing\r\n",
							"                USING temp_sales_view_merge AS updates\r\n",
							"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
							"\r\n",
							"                WHEN MATCHED AND existing.hash != updates.hash THEN\r\n",
							"                UPDATE SET {update_set} \r\n",
							"\r\n",
							"                WHEN NOT MATCHED THEN\r\n",
							"                INSERT ({insert_columns}) \r\n",
							"                VALUES ({insert_values})\r\n",
							"            \"\"\")\r\n",
							"            \r\n",
							"            # Save metadata\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.fact_{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            if last_result:\r\n",
							"                file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
							"                file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
							"            else:\r\n",
							"                file_result['inserted_rows'] = 0\r\n",
							"                file_result['updated_rows'] = 0\r\n",
							"\r\n",
							"        else:\r\n",
							"            \r\n",
							"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
							"            sdf =spark.sql(f\"\"\"\r\n",
							"                        {file2[\"query\"].strip()} \r\n",
							"                        \"\"\")\r\n",
							"\r\n",
							"            max_skSales = 0\r\n",
							"            sdf = sdf.withColumn(\"skSales\", F.expr(f\"{max_skSales} + row_number() OVER (ORDER BY id{table_name})\"))\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('loadDate', 'deltaDate', 'skSales')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))            \r\n",
							"            \r\n",
							"            sdf.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_fact)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.fact_{table_name} USING DELTA LOCATION \\'{delta_table_path_fact}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"            \r\n",
							"        # Save results\r\n",
							"        file_result['status'] = 'Correct'\r\n",
							"        file_result['inserted_rows'] = sdf.count()\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result)\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Save Log Files**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"log_file_name = f\"Log_{executionID}.json\"\r\n",
							"\r\n",
							"# Blob client\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"# dicc to json\r\n",
							"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
							"\r\n",
							"destination_blob_name = f\"{gold_folder_logs}/{year}/{month}/{log_file_name}\"\r\n",
							"\r\n",
							"# Destination client\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=container_name, blob=destination_blob_name)\r\n",
							"\r\n",
							"# Save json\r\n",
							"destination_blob.upload_blob(json_data, overwrite=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Update ConfiguracionReporting.csv**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#update_date_string = '1900-01-01'\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"csv_blob_name = \"ConfiguracionReporting.csv\" # Reporting configuration file\r\n",
							"\r\n",
							"# Client to read csv\r\n",
							"blob_client = blob_service_client.get_blob_client(container=metadata_folder, blob=csv_blob_name)\r\n",
							"\r\n",
							"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
							"df = pd.read_csv(stream, sep=',')\r\n",
							"df.columns = df.columns.str.strip()\r\n",
							"\r\n",
							"names_dimensionsList = [item['name'] for item in filtered_dimensionsList]\r\n",
							"for i in names_dimensionsList:\r\n",
							"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{update_date_string}'\r\n",
							"\r\n",
							"names_factList = [item['name'] for item in filtered_factList]\r\n",
							"for i in names_factList:\r\n",
							"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{update_date_string}'\r\n",
							"\r\n",
							"# Save\r\n",
							"output_stream = io.BytesIO()\r\n",
							"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')\r\n",
							"output_stream.seek(0)\r\n",
							"\r\n",
							"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
						],
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MergeLandingFilesToSilver')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "730fc08a-d278-4b54-93a7-f9c1d68402e2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Import modules**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format, concat_ws, xxhash64\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
							"import re\r\n",
							"import json\r\n",
							"from datetime import datetime\r\n",
							"import pandas as pd\r\n",
							"import io\r\n",
							"from pyspark.sql import functions as F"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#%%sql\r\n",
							"# USE default;\r\n",
							"# DROP DATABASE  silver CASCADE ;\r\n",
							"# USE default;\r\n",
							"# CREATE DATABASE IF NOT EXISTS silver;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#%%sql\r\n",
							"#DROP TABLE IF EXISTS gold.fact_sales"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Configuration**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Connection to our ADLG2\r\n",
							"\r\n",
							"today_date = datetime.now()\r\n",
							"\r\n",
							"delta_date_filter = (today_date.strftime('%Y%m%d')) # Format 'yyyymmdd'\r\n",
							"#delta_date_filter = '20241010' # test \r\n",
							"\r\n",
							"delta_date_string = today_date.strftime('%Y-%m-%d') # Format 'yyyy-mm-dd'\r\n",
							"#delta_date_string = '2024-10-10' # test\r\n",
							"\r\n",
							"landing_files = json.loads(landing_files)  # String parameter to array again\r\n",
							"\r\n",
							"# List to save results\r\n",
							"results = []\r\n",
							"correct_files = []\r\n",
							"incorrect_files = []"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Load/Merge parquet Landing files to silver**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for file in landing_files:\r\n",
							"    file_result = {\r\n",
							"        'table': file[\"name\"][:file[\"name\"].index('_')],  \r\n",
							"        'file_name': file['name'],\r\n",
							"        'status': 'Incorrecto',\r\n",
							"        'inserted_rows': 0,\r\n",
							"        'updated_rows': 0,\r\n",
							"        'exception': 'N/A',\r\n",
							"        'datetime': str(datetime.now())  \r\n",
							"    }\r\n",
							"\r\n",
							"\r\n",
							"    try:\r\n",
							"        table_name = file[\"name\"][:file[\"name\"].index('_')]  # Name of the table\r\n",
							"        source_wildcard = f\"{table_name}*.parquet\"   # Wildcard for files\r\n",
							"        key_columns_str = f\"id{table_name}\" \r\n",
							"\r\n",
							"        source_path = data_lake_container + '/' + bronze_landing + '/' + source_wildcard\r\n",
							"        delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
							"        sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
							"        sdf.cache()\r\n",
							"\r\n",
							"        if sdf.limit(1).count() == 0:\r\n",
							"            print(f\"No se procesarÃ¡ el archivo {file['name']} porque no contiene datos.\")\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            file_result['exception'] = \"File with no data\"\r\n",
							"            results.append(file_result)\r\n",
							"            correct_files.append(file['name'])\r\n",
							"            continue\r\n",
							"\r\n",
							"        key_columns = key_columns_str.split(',')\r\n",
							"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
							"\r\n",
							"        sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")  # DROP column we don't need\r\n",
							"\r\n",
							"        # Check Delta Table exists\r\n",
							"        if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
							"            delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"            existing_columns = delta_table.toDF().columns\r\n",
							"            \r\n",
							"            sdf = sdf.withColumn(\"fechaDelta\", lit(delta_date_filter)) # ADD fechaDelta\r\n",
							"\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash))) # ADD hash\r\n",
							"\r\n",
							"            # Create update set excluding fechaCarga\r\n",
							"            update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
							"\r\n",
							"            # Merge new data into existing table y captura el conteo\r\n",
							"            merge_result = delta_table.alias(\"existing\").merge(\r\n",
							"                source=sdf.alias(\"updates\"),\r\n",
							"                condition=\" AND \".join(conditions_list)\r\n",
							"            ).whenMatchedUpdate(\r\n",
							"                condition=\" AND \".join(conditions_list + [\"existing.hash != updates.hash\"]),\r\n",
							"                set={\r\n",
							"                    #\"fechaCarga\": \"existing.fechaCarga\",  # comment to not update\r\n",
							"                    \"fechaDelta\": delta_date_filter,\r\n",
							"                    **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}\r\n",
							"                }\r\n",
							"            ).whenNotMatchedInsert(\r\n",
							"                values={\r\n",
							"                    \"fechaCarga\": \"updates.fechaCarga\",\r\n",
							"                    \"fechaDelta\": delta_date_filter,\r\n",
							"                    **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}\r\n",
							"                }\r\n",
							"            ).execute()\r\n",
							"\r\n",
							"            # Save metadata\r\n",
							"            file_result['status'] = 'Correct'\r\n",
							"            history_df = spark.sql(f\"DESCRIBE HISTORY silver.{table_name}\")\r\n",
							"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
							"            last_result = last_merge_operation.collect()\r\n",
							"\r\n",
							"            if last_result:\r\n",
							"                file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
							"                file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
							"            else:\r\n",
							"                print(f\"No hay registros de operaciÃ³n MERGE para la tabla {file['name']}\")\r\n",
							"                file_result['inserted_rows'] = 0\r\n",
							"                file_result['updated_rows'] = 0\r\n",
							"            \r\n",
							"            correct_files.append(file['name'])  # Add to correct files list\r\n",
							"\r\n",
							"\r\n",
							"        else:\r\n",
							"            # Create new Delta Table\r\n",
							"            sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga) \r\n",
							"            sdf = sdf.withColumn(\"fechaDelta\", lit(delta_date_filter))\r\n",
							"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta')]\r\n",
							"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"            sdf.write.format('delta').save(delta_table_path)\r\n",
							"\r\n",
							"            spark.sql(f'CREATE TABLE IF NOT EXISTS silver.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
							"            spark.sql(f\"ALTER TABLE silver.{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
							"            spark.sql(f\"ALTER TABLE silver.{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
							"\r\n",
							"        \r\n",
							"            # Save metadata\r\n",
							"            file_result['status'] = 'Correcto'\r\n",
							"            file_result['inserted_rows'] = sdf.count()  # Count new rows\r\n",
							"            file_result['updated_rows'] = 0  # No updates rows in the firs load\r\n",
							"            correct_files.append(file['name'])  # Add to correct files list\r\n",
							"\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
							"        file_result['exception'] = f\"{e}\"\r\n",
							"        results.append(file_result) \r\n",
							"        incorrect_files.append(file['name'])  # Add to incorrect files list\r\n",
							"        continue\r\n",
							"\r\n",
							"    results.append(file_result)  # Append to result\r\n",
							"\r\n",
							"\r\n",
							"# Print result to debug\r\n",
							"for result in results:\r\n",
							"    print(f\"Archivo: {result['file_name']}, Estado: {result['status']}, Filas insertadas: {result['inserted_rows']}, Filas actualizadas: {result['updated_rows']}\")\r\n",
							"\r\n",
							"# Save correct and incorrect file names in pipeline variables\r\n",
							"correct_files_variable = correct_files\r\n",
							"incorrect_files_variable = incorrect_files\r\n",
							"\r\n",
							"# Print result to debug\r\n",
							"print(\"Archivos correctos:\", correct_files_variable)\r\n",
							"print(\"Archivos incorrectos:\", incorrect_files_variable)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Save log silver merge/load**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get current date\r\n",
							"# actual_time = datetime.now()\r\n",
							"# year = actual_time.strftime('%Y')\r\n",
							"# month = actual_time.strftime('%m')\r\n",
							"# day = actual_time.strftime('%d')\r\n",
							"# hour = actual_time.strftime('%H%M%S')\r\n",
							"\r\n",
							"log_file_name = f\"Log_{executionID}.json\"\r\n",
							"\r\n",
							"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
							"\r\n",
							"destination_blob_name = f\"{year}/{month}/{log_file_name}\"\r\n",
							"\r\n",
							"# Client to connect\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=silver_logs, blob=destination_blob_name)\r\n",
							"\r\n",
							"destination_blob.upload_blob(json_data, overwrite=True)"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Move files to /Processed or /Error and delete from /Pending**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"results_dict = []\r\n",
							"\r\n",
							"# Loop correct files\r\n",
							"for i in correct_files_variable:\r\n",
							"    source_blob_name = i\r\n",
							"    name_part = i.split('_')[0]\r\n",
							"    \r\n",
							"    # Extract datetime\r\n",
							"    date_match = re.search(r'_(\\d{8})\\.parquet$', i)\r\n",
							"    if date_match:\r\n",
							"        date_str = date_match.group(1)\r\n",
							"        year = date_str[:4] \r\n",
							"        month = date_str[4:6]\r\n",
							"        destination_blob_name = f\"{name_part}/{year}/{month}/{i}\"\r\n",
							"    else:\r\n",
							"        destination_blob_name = i\r\n",
							"    \r\n",
							"    # Create the blob clients\r\n",
							"    source_blob = blob_service_client.get_blob_client(container=bronze_pending, blob=source_blob_name)\r\n",
							"    destination_blob_processed = blob_service_client.get_blob_client(container=bronze_processed, blob=destination_blob_name)\r\n",
							"    \r\n",
							"    # Copy\r\n",
							"    copy_properties = destination_blob_processed.start_copy_from_url(source_blob.url)\r\n",
							"    \r\n",
							"    # Check and delele\r\n",
							"    if copy_properties[\"copy_status\"] == \"success\":\r\n",
							"        source_blob.delete_blob()  # Delete if success\r\n",
							"        results_dict.append({\r\n",
							"            \"file\": source_blob_name,\r\n",
							"            \"status\": \"Correct\",\r\n",
							"            \"destination\": f\"bronze/Processed/{destination_blob_name}\" })\r\n",
							"    else:\r\n",
							"        results_dict.append({\r\n",
							"            \"file\": source_blob_name,\r\n",
							"            \"status\": \"Fail to copy file\",\r\n",
							"            \"destination\": f\"bronze/Processed/{destination_blob_name}\" })\r\n",
							"\r\n",
							"# Loop incorrect files\r\n",
							"for i in incorrect_files_variable:\r\n",
							"    source_blob_name = i\r\n",
							"    name_part = i.split('_')[0]\r\n",
							"\r\n",
							"    # Extraer fecha\r\n",
							"    date_match = re.search(r'_(\\d{8})\\.parquet$', i)\r\n",
							"    if date_match:\r\n",
							"        date_str = date_match.group(1) \r\n",
							"        year = date_str[:4]  \r\n",
							"        month = date_str[4:6] \r\n",
							"        destination_blob_name = f\"{name_part}/{year}/{month}/{i}\"\r\n",
							"    else:\r\n",
							"        destination_blob_name = i\r\n",
							"\r\n",
							"    source_blob = blob_service_client.get_blob_client(container=bronze_pending, blob=source_blob_name)\r\n",
							"    destination_blob_error = blob_service_client.get_blob_client(container=bronze_error, blob=destination_blob_name)\r\n",
							"\r\n",
							"    # Copy\r\n",
							"    copy_properties = destination_blob_error.start_copy_from_url(source_blob.url)\r\n",
							"\r\n",
							"    # Check and delele\r\n",
							"    if copy_properties[\"copy_status\"] == \"success\":\r\n",
							"        source_blob.delete_blob()  # Delete if success\r\n",
							"        results_dict.append({\r\n",
							"            \"file\": source_blob_name,\r\n",
							"            \"status\": \"Incorrect\",\r\n",
							"            \"destination\": f\"bronze/Landing/Error/{destination_blob_name}\" })\r\n",
							"    else:\r\n",
							"        results_dict.append({\r\n",
							"            \"file\": source_blob_name,\r\n",
							"            \"status\": \"Fail to copy File\",\r\n",
							"            \"destination\": f\"bronze/Landing/Error/{destination_blob_name}\" })"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Save logs bronze parquet files**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"log_file_name = f\"Log_{executionID}.json\"\r\n",
							"\r\n",
							"json_data = json.dumps(results_dict, indent=4, ensure_ascii=False)\r\n",
							"\r\n",
							"destination_blob_name = f\"{year}/{month}/{log_file_name}\"\r\n",
							"\r\n",
							"destination_blob = blob_service_client.get_blob_client(container=bronze_logs, blob=destination_blob_name)\r\n",
							"\r\n",
							"destination_blob.upload_blob(json_data, overwrite=True)"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Update ConfiguracionOrigenes.csv**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#delta_date_string = '2024-10-08'\r\n",
							"\r\n",
							"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
							"\r\n",
							"csv_blob_name = \"ConfiguracionOrigenes.csv\"  # File name\r\n",
							"\r\n",
							"# Read file\r\n",
							"blob_client = blob_service_client.get_blob_client(container=metadata_folder, blob=csv_blob_name)\r\n",
							"\r\n",
							"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
							"df = pd.read_csv(stream, sep=',')  \r\n",
							"df.columns = df.columns.str.strip()\r\n",
							"\r\n",
							"for i in landing_files: # UpdateDate \r\n",
							"    df.loc[df['FileName'] == i[\"name\"][:i[\"name\"].index('_')], 'UpdateDate'] = f'{delta_date_string}'\r\n",
							"\r\n",
							"# Save DataFrame\r\n",
							"output_stream = io.BytesIO()\r\n",
							"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')\r\n",
							"output_stream.seek(0)\r\n",
							"\r\n",
							"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkTFM",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "752b4b12-f6a7-4cde-a985-7edf4ef9c971"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
						"name": "sparkTFM",
						"type": "Spark",
						"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from delta.tables import DeltaTable\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import when, lit, current_date, date_format,concat_ws,xxhash64,current_timestamp\r\n",
							"from pyspark.sql import SparkSession,Window\r\n",
							"from pyspark.sql import functions as F\r\n",
							"import json\r\n",
							"import re\r\n",
							"from datetime import datetime\r\n",
							"from azure.storage.blob import BlobServiceClient\r\n",
							"import io\r\n",
							"import pandas as pd\r\n",
							"import csv"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"# Crear una instancia de SparkSession\r\n",
							"spark = SparkSession.builder.getOrCreate()\r\n",
							"\r\n",
							"# Eliminar la tabla Delta y su contenido\r\n",
							"delta_table_path = \"/mnt/data/tablas_delta/ejemplo_tabla_delta\"\r\n",
							"\r\n",
							"# Usar Spark para eliminar la carpeta\r\n",
							"fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\r\n",
							"path = spark._jvm.org.apache.hadoop.fs.Path(delta_table_path)\r\n",
							"\r\n",
							"if fs.exists(path):\r\n",
							"    fs.delete(path, True)  # True para eliminar recursivamente\r\n",
							"    print(f\"Tabla Delta eliminada en: {delta_table_path}\")\r\n",
							"else:\r\n",
							"    print(f\"No se encontrÃ³ la tabla Delta en la ruta: {delta_table_path}\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"# Datos iniciales sin usar F.to_timestamp\r\n",
							"data = [\r\n",
							"    (1, 1, \"Pedro\", \"Lopez\", \"EspaÃ±a\", \"1990-01-01 00:00:00\", \"9999-12-31 00:00:00\", 1),\r\n",
							"    (2, 2, \"Juan\", \"GarcÃ­a\", \"EspaÃ±a\", \"1990-01-01 00:00:00\", \"9999-12-31 00:00:00\", 1),\r\n",
							"    (3, 3, \"MarÃ­a\", \"FernÃ¡ndez\", \"Francia\", \"1990-01-01 00:00:00\", \"9999-12-31 00:00:00\", 1),\r\n",
							"    (4, 4, \"LucÃ­a\", \"Caballero\", \"Alemania\", \"1990-01-01 00:00:00\", \"9999-12-31 00:00:00\", 1),\r\n",
							"    (5, 5, \"Sanda\", \"Ruiz\", \"Francia\", \"1990-01-01 00:00:00\", \"9999-12-31 00:00:00\", 1),\r\n",
							"    (6, 6, \"Manuel\", \"SÃ¡nchez\", \"Suiza\", \"1990-01-01 00:00:00\", \"9999-12-31 00:00:00\", 1)\r\n",
							"]\r\n",
							"\r\n",
							"columns = [\"sk\", \"id\", \"name\", \"lastName\", \"country\", \"fromDate\", \"toDate\", \"isCurrent\"]\r\n",
							"\r\n",
							"# Crear el DataFrame\r\n",
							"df = spark.createDataFrame(data, columns)\r\n",
							"\r\n",
							"# Convertir las columnas 'fromDate' y 'toDate' a formato timestamp\r\n",
							"df = df.withColumn(\"fromDate\", F.to_timestamp(F.col(\"fromDate\"), \"yyyy-MM-dd HH:mm:ss\")) \\\r\n",
							"       .withColumn(\"toDate\", F.to_timestamp(F.col(\"toDate\"), \"yyyy-MM-dd HH:mm:ss\"))\r\n",
							"\r\n",
							"# Generar la columna de hash\r\n",
							"columns_to_hash = [col for col in df.columns if col not in ('fromDate', 'toDate', 'isCurrent')]\r\n",
							"df = df.withColumn(\"hash\", F.xxhash64(F.concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"# Mostrar el DataFrame actualizado\r\n",
							"df.show(truncate=False)\r\n",
							"\r\n",
							"# Ruta para almacenar la tabla Delta\r\n",
							"delta_table_path = \"/mnt/data/tablas_delta/ejemplo_tabla_delta\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\r\n",
							"\r\n",
							"print(f\"Tabla Delta creada en: {delta_table_path}\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"DROP TABLE IF EXISTS tabla_delta\")"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(f\"\"\"\r\n",
							"CREATE TABLE tabla_delta\r\n",
							"USING DELTA\r\n",
							"LOCATION '{delta_table_path}'\r\n",
							"\"\"\")\r\n",
							"print(\"Tabla registrada en el catÃ¡logo como 'tabla_delta'\")"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"result_df = spark.sql(\"SELECT * FROM tabla_delta\")\r\n",
							"result_df.show()"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dataUpdate = [\r\n",
							"    (2, \"Juan\", \"GarcÃ­a\", \"Portugal\"),\r\n",
							"    (6, \"Manuel\", \"SÃ¡nchez\", \"Italia\")\r\n",
							"    (6, \"Martina\", \"DomÃ­nguez\", \"Italia\")\r\n",
							"]\r\n",
							"\r\n",
							"columns = [\"id\", \"name\", \"lastName\", \"country\"]\r\n",
							"\r\n",
							"sdf = spark.createDataFrame(dataUpdate, columns)\r\n",
							"\r\n",
							"# Mostrar el DataFrame\r\n",
							"sdf.show()"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"key_column = \"id\" # PodrÃ­a parametrizarse de forma dinamica. Ej: f\"id{NombreDimension}\"\" si se recorre una lista\r\n",
							"\r\n",
							"# AÃ±adimos los campos de las SCD2\r\n",
							"sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
							"sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
							"\r\n",
							"# Creamos el hash por el que versionamos\r\n",
							"columns_to_hash = [col for col in sdf.columns if col not in (f\"{key_column}\", 'fromDate', 'toDate', 'isCurrent')]\r\n",
							"sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
							"\r\n",
							"# Calculamos el valor de la Ãºltima sk\r\n",
							"next_surrogate_key = spark.sql(f\"SELECT COALESCE(MAX(sk), 0) + 1 AS next_key FROM tabla_delta\").collect()[0][\"next_key\"] \r\n",
							"\r\n",
							"# Almacenamos en una vista temporal el df de lo que entra nuevo\r\n",
							"sdf.createOrReplaceTempView(\"temp_view\")\r\n",
							"\r\n",
							"# Finalizamos las versiÃ³n actuales\r\n",
							"spark.sql(f\"\"\"\r\n",
							"    MERGE INTO tabla_delta  \r\n",
							"    AS existing\r\n",
							"    USING temp_view AS updates\r\n",
							"    ON {\" AND \".join([f\"existing.{key_column}=updates.{key_column}\"])}\r\n",
							"    WHEN MATCHED AND existing.isCurrent = 1 AND existing.hash != updates.hash THEN\r\n",
							"    UPDATE SET\r\n",
							"        existing.toDate = current_date(),\r\n",
							"        existing.isCurrent = 0\r\n",
							"\"\"\")\r\n",
							"\r\n",
							"# Insertamos las nuevas versiones y los valores nuevos\r\n",
							"spark.sql(f\"\"\"\r\n",
							"    INSERT INTO tabla_delta    \r\n",
							"    SELECT \r\n",
							"        {next_surrogate_key} + row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS sk,  \r\n",
							"        updates.*\r\n",
							"    FROM temp_view AS updates\r\n",
							"    LEFT JOIN tabla_delta AS existing\r\n",
							"    ON {\" AND \".join([f\"existing.{key_column} = updates.{key_column}\"])} AND existing.isCurrent = 1\r\n",
							"    WHERE \r\n",
							"        existing.id IS NULL OR \r\n",
							"        (existing.hash != updates.hash)\r\n",
							"\"\"\")"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"final = spark.sql(\"SELECT * FROM tabla_delta order by sk asc\")\r\n",
							"final.show()"
						],
						"outputs": [],
						"execution_count": 57
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_ML')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0a0c3e7c-fde1-4f58-bb31-f15f973a142c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"from sklearn.model_selection import train_test_split\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.metrics import mean_squared_error, r2_score\n",
							"import pyodbc\n",
							"\n",
							"import pandas as pd\n",
							"from sklearn.model_selection import train_test_split, KFold\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
							"import pyodbc\n",
							"\n",
							"# Ridge() o Lasso() sklearn.linear_model\n",
							"# DecisionTreeRegressor() sklearn.tree\n",
							"# RandomForestRegressor() de sklearn.ensemble\n",
							"# SVR() sklearn.svm\n",
							"# MLPRegressor() sklearn.neural_network\n",
							"from sklearn.linear_model import Ridge, Lasso\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.feature_selection import RFE\n",
							"from sklearn.feature_selection import SelectFromModel\n",
							"from sklearn.linear_model import LogisticRegression\n",
							"\n",
							"from sklearn.feature_selection import VarianceThreshold\n",
							"from sklearn import preprocessing\n",
							"\n",
							"\n",
							"from sklearn.feature_selection import VarianceThreshold\n",
							"from sklearn.feature_selection import SelectKBest\n",
							"from sklearn.feature_selection import chi2\n",
							"# chi2_selector = SelectKBest(chi2, k=2)\n",
							"# X_kbest = chi2_selector.fit_transform(X, y)\n",
							"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
							"from sklearn.metrics import roc_auc_score \n",
							"from mlxtend.feature_selection import SequentialFeatureSelector \n",
							"# feature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1),\n",
							"#                           k_features=15, forward=False, \n",
							"#                           verbose=2, scoring=âroc_aucâ, cv=4)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# Query Clima\n",
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"\n",
							"# Consulta para obtener los datos climÃ¡ticos y de incidencia de enfermedad\n",
							"query = \"\"\"\n",
							"SELECT --[Country],\n",
							"      --t.[Year],\n",
							"      [Weathercode]\n",
							"      ,[temperature_2m_max]\n",
							"      ,[temperature_2m_min]\n",
							"      ,[temperature_2m_mean]\n",
							"      ,[apparent_temperature_max]\n",
							"      ,[apparent_temperature_min]\n",
							"      ,[apparent_temperature_mean]\n",
							"      ,[precipitation_sum_mm]\n",
							"      ,[rain_sum_mm]\n",
							"      ,[snowfall_sum_cm]\n",
							"      ,[precipitation_hours_h]\n",
							"      ,[windspeed_10m_max_km/h]\n",
							"      ,[windgusts_10m_max_km/h]\n",
							"      ,[winddirection_10m_dominant]\n",
							"      ,[shortwave_radiation_sum]\n",
							"\t  , Value as Incidencia\n",
							"  FROM [IncidenceDWH].[dbo].[VectorTiempo] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.Country AND I.Year = T.Year \n",
							"  where i.ID_Cause = 569 and ID_Sex = 1\"\"\""
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"source": [
							"# Query Quimicos \n",
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"\n",
							"# Consulta para obtener los datos climÃ¡ticos y de incidencia de enfermedad\n",
							"query = \"\"\"\n",
							"/****** Script para el comando SelectTopNRows de SSMS  ******/\n",
							"SELECT --[Name]\n",
							"      --[Year]\n",
							"      [BC],[CO],[CH4] ,[N20],[Buildings],[CO2],[bio_HCB] ,[fossil_HCB],[NH3],[NMVOC]\n",
							"      ,[NOx],[OC],[bio_PAH_BaP],[fossil_PAH_BaP]\n",
							"      ,[PAH_BbF],[bio_PAH_IcdP],[fossil_PAH_IcdP],[bio_PAH_BkF],[fossil_PAH_BbF],[bio_PCB],[fossil_PCB],[bio_PCDD_F],[fossil_PCDD_F],[PM10],[PM25],[SO2]\n",
							"\t  , Value as Incidencia\n",
							"  FROM [IncidenceDWH].[dbo].[vectorQuimicosFloat] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.[Name] AND I.Year = T.Year \n",
							"  where i.ID_Cause = 559 and ID_Sex = 1\n",
							"  AND   [BC] IS NOT NULL AND [CO] IS NOT NULL AND  [CH4] IS NOT NULL\n",
							"   AND   [N20] IS NOT NULL  AND   [Buildings]  IS NOT NULL AND   [CO2] IS NOT NULL\n",
							"   AND   [bio_HCB] IS NOT NULL AND   [fossil_HCB] IS NOT NULL\n",
							"   AND   [NH3] IS NOT NULL AND   [NMVOC] IS NOT NULL\n",
							"     AND [NOx] IS NOT NULL AND [OC] IS NOT NULL AND [bio_PAH_BaP] IS NOT NULL AND [fossil_PAH_BaP] IS NOT NULL AND [PAH_BbF] IS NOT NULL AND [bio_PAH_IcdP] IS NOT NULL AND [fossil_PAH_IcdP] IS NOT NULL AND [bio_PAH_BkF] IS NOT NULL AND [fossil_PAH_BbF] IS NOT NULL AND [bio_PCB] IS NOT NULL AND [fossil_PCB] IS NOT NULL AND [bio_PCDD_F] IS NOT NULL AND [fossil_PCDD_F] IS NOT NULL AND[PM10] IS NOT NULL AND [PM25] IS NOT NULL AND [SO2] IS NOT NULL\"\"\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# S x Q\n",
							"\n",
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"\n",
							"query = \"\"\"\n",
							"-- Socio x Quimicos\n",
							"SELECT --[Name]\n",
							"      --[Year]\n",
							"      [BC]\n",
							"      ,[CO]\n",
							"      ,[CH4]\n",
							"      ,[N20]\n",
							"      ,[Buildings]\n",
							"      ,[CO2]\n",
							"      ,[bio_HCB]\n",
							"      ,[fossil_HCB]\n",
							"      ,[NH3]\n",
							"      ,[NMVOC]\n",
							"      ,[NOx]\n",
							"      ,[OC]\n",
							"      ,[bio_PAH_BaP]\n",
							"      ,[fossil_PAH_BaP]\n",
							"      ,[PAH_BbF]\n",
							"      ,[bio_PAH_IcdP]\n",
							"      ,[fossil_PAH_IcdP]\n",
							"      ,[bio_PAH_BkF]\n",
							"      ,[fossil_PAH_BbF]\n",
							"      ,[bio_PCB]\n",
							"      ,[fossil_PCB]\n",
							"      ,[bio_PCDD_F]\n",
							"      ,[fossil_PCDD_F]\n",
							"      ,[PM10]\n",
							"      ,[PM25]\n",
							"      ,[SO2]\n",
							"\t        [GDP]\n",
							"      ,[GDP_per_capita]\n",
							"      ,[GDP_per_capita_growth]\n",
							"      ,[Gross_value_added]\n",
							"      ,[Life_expectancy_birth(years)]\n",
							"      ,[Access_electricity(%)]\n",
							"      ,[Agricultural_land]\n",
							"      ,[Services]\n",
							"      ,[Unemployment]\n",
							"      ,[Control_Corruption]\n",
							"      ,[Contributing_family]\n",
							"      ,[Government Effectiveness]\n",
							"      ,[Political_Stability]\n",
							"      ,[LAW]\n",
							"      ,[Regulatory_Quality]\n",
							"\t  , Value as Incidencia\n",
							"  FROM [IncidenceDWH].[dbo].[vectorQuimicosFloat] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.[Name] AND I.Year = T.Year \n",
							"  INNER JOIN dbo.vectorSocio2 AS t1  ON I.Location = T1.Country AND I.Year = T1.Year \n",
							"  where i.ID_Cause = 574 and ID_Sex = 2\n",
							"\n",
							"  AND   [BC] IS NOT NULL\n",
							"   AND   [CO] IS NOT NULL\n",
							"   AND  [CH4] IS NOT NULL\n",
							"   AND   [N20] IS NOT NULL\n",
							"   AND   [Buildings]  IS NOT NULL\n",
							"   AND   [CO2] IS NOT NULL\n",
							"   AND   [bio_HCB] IS NOT NULL \n",
							"   AND   [fossil_HCB] IS NOT NULL\n",
							"   AND   [NH3] IS NOT NULL\n",
							"   AND   [NMVOC] IS NOT NULL\n",
							"     AND [NOx] IS NOT NULL\n",
							"      AND [OC] IS NOT NULL\n",
							"      AND [bio_PAH_BaP] IS NOT NULL\n",
							"      AND [fossil_PAH_BaP] IS NOT NULL\n",
							"      AND [PAH_BbF] IS NOT NULL\n",
							"      AND [bio_PAH_IcdP] IS NOT NULL\n",
							"      AND [fossil_PAH_IcdP] IS NOT NULL\n",
							"      AND [bio_PAH_BkF] IS NOT NULL\n",
							"      AND [fossil_PAH_BbF] IS NOT NULL\n",
							"      AND [bio_PCB] IS NOT NULL\n",
							"      AND [fossil_PCB] IS NOT NULL\n",
							"      AND [bio_PCDD_F] IS NOT NULL\n",
							"      AND [fossil_PCDD_F] IS NOT NULL\n",
							"      AND[PM10] IS NOT NULL\n",
							"      AND [PM25] IS NOT NULL\n",
							"      AND [SO2] IS NOT NULL\n",
							"\t  AND\n",
							"  [GDP] IS NOT NULL\n",
							"      AND [GDP_per_capita] IS NOT NULL\n",
							"      AND [GDP_per_capita_growth] IS NOT NULL\n",
							"      AND [Gross_value_added] IS NOT NULL\n",
							"      AND [Life_expectancy_birth(years)] IS NOT NULL\n",
							"      AND [Access_electricity(%)] IS NOT NULL\n",
							"     -- AND [Broad_money] IS NOT NULL\n",
							"      AND [Agricultural_land] IS NOT NULL\n",
							"\n",
							"      --AND [Tax_revenue] IS NOT NULL\n",
							"      \n",
							"\t  AND [Services] IS NOT NULL\n",
							"\n",
							"\t        AND[Unemployment] IS NOT NULL\n",
							"      AND[Control_Corruption] IS NOT NULL\n",
							"      AND[Contributing_family] IS NOT NULL\n",
							"      AND[Government Effectiveness] IS NOT NULL\n",
							"      AND[Political_Stability] IS NOT NULL\n",
							"      AND[LAW] IS NOT NULL\n",
							"      AND[Regulatory_Quality] IS NOT NULL\"\"\""
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"source": [
							"# Query Clima X Quimicos\n",
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"\n",
							"query = \"\"\"\n",
							"SELECT --[Name]\n",
							"      --[Year]\n",
							"      [BC]\n",
							"      ,[CO]\n",
							"      ,[CH4]\n",
							"      ,[N20]\n",
							"      ,[Buildings]\n",
							"      ,[CO2]\n",
							"      ,[bio_HCB]\n",
							"      ,[fossil_HCB]\n",
							"      ,[NH3]\n",
							"      ,[NMVOC]\n",
							"      ,[NOx]\n",
							"      ,[OC]\n",
							"      ,[bio_PAH_BaP]\n",
							"      ,[fossil_PAH_BaP]\n",
							"      ,[PAH_BbF]\n",
							"      ,[bio_PAH_IcdP]\n",
							"      ,[fossil_PAH_IcdP]\n",
							"      ,[bio_PAH_BkF]\n",
							"      ,[fossil_PAH_BbF]\n",
							"      ,[bio_PCB]\n",
							"      ,[fossil_PCB]\n",
							"      ,[bio_PCDD_F]\n",
							"      ,[fossil_PCDD_F]\n",
							"      ,[PM10]\n",
							"      ,[PM25]\n",
							"      ,[SO2]\n",
							"\t        ,[Weathercode]\n",
							"      ,[temperature_2m_max]\n",
							"      ,[temperature_2m_min]\n",
							"      ,[temperature_2m_mean]\n",
							"      ,[apparent_temperature_max]\n",
							"      ,[apparent_temperature_min]\n",
							"      ,[apparent_temperature_mean]\n",
							"      ,[precipitation_sum_mm]\n",
							"      ,[rain_sum_mm]\n",
							"      ,[snowfall_sum_cm]\n",
							"      ,[precipitation_hours_h]\n",
							"      ,[windspeed_10m_max_km/h]\n",
							"      ,[windgusts_10m_max_km/h]\n",
							"      ,[winddirection_10m_dominant]\n",
							"      ,[shortwave_radiation_sum]\n",
							"\t  , Value as Incidencia\n",
							"  FROM [IncidenceDWH].[dbo].[vectorQuimicosFloat] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.[Name] AND I.Year = T.Year \n",
							"  INNER JOIN dbo.VectorTiempo AS t1  ON I.Location = T1.Country AND I.Year = T1.Year \n",
							"  where i.ID_Cause = 571 and ID_Sex = 2\n",
							"\n",
							"  AND   [BC] IS NOT NULL\n",
							"   AND   [CO] IS NOT NULL\n",
							"   AND  [CH4] IS NOT NULL\n",
							"   AND   [N20] IS NOT NULL\n",
							"   AND   [Buildings]  IS NOT NULL\n",
							"   AND   [CO2] IS NOT NULL\n",
							"   AND   [bio_HCB] IS NOT NULL\n",
							"   AND   [fossil_HCB] IS NOT NULL\n",
							"   AND   [NH3] IS NOT NULL\n",
							"   AND   [NMVOC] IS NOT NULL\n",
							"     AND [NOx] IS NOT NULL\n",
							"      AND [OC] IS NOT NULL\n",
							"      AND [bio_PAH_BaP] IS NOT NULL\n",
							"      AND [fossil_PAH_BaP] IS NOT NULL\n",
							"      AND [PAH_BbF] IS NOT NULL\n",
							"      AND [bio_PAH_IcdP] IS NOT NULL\n",
							"      AND [fossil_PAH_IcdP] IS NOT NULL\n",
							"      AND [bio_PAH_BkF] IS NOT NULL\n",
							"      AND [fossil_PAH_BbF] IS NOT NULL\n",
							"      AND [bio_PCB] IS NOT NULL\n",
							"      AND [fossil_PCB] IS NOT NULL\n",
							"      AND [bio_PCDD_F] IS NOT NULL\n",
							"      AND [fossil_PCDD_F] IS NOT NULL\n",
							"      AND[PM10] IS NOT NULL\n",
							"      AND [PM25] IS NOT NULL\n",
							"      AND [SO2] IS NOT NULL\"\"\""
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"source": [
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"\n",
							"# Consulta para obtener los datos climÃ¡ticos y de incidencia de enfermedad\n",
							"query = \"\"\" \n",
							"SELECT --[Country],\n",
							"      --t.[Year],\n",
							"      [Weathercode]\n",
							"      ,[temperature_2m_max]\n",
							"      ,[temperature_2m_min]\n",
							"      ,[temperature_2m_mean]\n",
							"      ,[apparent_temperature_max]\n",
							"      ,[apparent_temperature_min]\n",
							"      ,[apparent_temperature_mean]\n",
							"      ,[precipitation_sum_mm]\n",
							"      ,[rain_sum_mm]\n",
							"      ,[snowfall_sum_cm]\n",
							"      ,[precipitation_hours_h]\n",
							"      ,[windspeed_10m_max_km/h]\n",
							"      ,[windgusts_10m_max_km/h]\n",
							"      ,[winddirection_10m_dominant]\n",
							"      ,[shortwave_radiation_sum]\n",
							"\t  , Value as incidencia\n",
							"\t  , GDP\n",
							"\t  ,GDP_per_capita\n",
							"\t  ,CO2_emissions\n",
							"  FROM [IncidenceDWH].[dbo].[VectorTiempo] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.Country AND I.Year = T.Year \n",
							"  INNER JOIN vectorPib AS P ON P.Location = T.Country AND P.Year = T.Year \n",
							"  where i.ID_Cause = 568 and ID_Sex = 1\n",
							"  AND GDP NOT LIKE '..' AND GDP_per_capita NOT LIKE '..'  AND CO2_emissions NOT LIKE '..' \"\"\""
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"source": [
							"# Todo junto\n",
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"query = \"\"\"SELECT --[Name]\n",
							"      --[Year]\n",
							"      [BC]\n",
							"      ,[CO]\n",
							"      ,[CH4]\n",
							"      ,[N20]\n",
							"      ,[Buildings]\n",
							"      ,[CO2]\n",
							"      ,[bio_HCB]\n",
							"      ,[fossil_HCB]\n",
							"      ,[NH3]\n",
							"      ,[NMVOC]\n",
							"      ,[NOx]\n",
							"      ,[OC]\n",
							"      ,[bio_PAH_BaP]\n",
							"      ,[fossil_PAH_BaP]\n",
							"      ,[PAH_BbF]\n",
							"      ,[bio_PAH_IcdP]\n",
							"      ,[fossil_PAH_IcdP]\n",
							"      ,[bio_PAH_BkF]\n",
							"      ,[fossil_PAH_BbF]\n",
							"      ,[bio_PCB]\n",
							"      ,[fossil_PCB]\n",
							"      ,[bio_PCDD_F]\n",
							"      ,[fossil_PCDD_F]\n",
							"      ,[PM10]\n",
							"      ,[PM25]\n",
							"      ,[SO2]\n",
							"\t        ,[Weathercode]\n",
							"      ,[temperature_2m_max]\n",
							"      ,[temperature_2m_min]\n",
							"      ,[temperature_2m_mean]\n",
							"      ,[apparent_temperature_max]\n",
							"      ,[apparent_temperature_min]\n",
							"      ,[apparent_temperature_mean]\n",
							"      ,[precipitation_sum_mm]\n",
							"      ,[rain_sum_mm]\n",
							"      ,[snowfall_sum_cm]\n",
							"      ,[precipitation_hours_h]\n",
							"      ,[windspeed_10m_max_km/h]\n",
							"      ,[windgusts_10m_max_km/h]\n",
							"      ,[winddirection_10m_dominant]\n",
							"      ,[shortwave_radiation_sum]\n",
							"      ,[GDP]\n",
							"      ,[GDP_per_capita]\n",
							"      ,[GDP_per_capita_growth]\n",
							"      ,[Gross_value_added]\n",
							"      ,[Life_expectancy_birth(years)]\n",
							"      ,[Access_electricity(%)]\n",
							"      --,[Broad_money]\n",
							"      ,[Agricultural_land]\n",
							"      --,[Unemployment(%)]\n",
							"      --,[Tax_revenue]\n",
							"      ,[Services]\n",
							"\t        ,[Unemployment]\n",
							"      ,[Control_Corruption]\n",
							"      ,[Contributing_family]\n",
							"      ,[Government Effectiveness]\n",
							"      ,[Political_Stability]\n",
							"      ,[LAW]\n",
							"      ,[Regulatory_Quality]\n",
							"\t  , Value as Incidencia\n",
							"  FROM [IncidenceDWH].[dbo].[vectorQuimicosFloat] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.[Name] AND I.Year = T.Year \n",
							"  INNER JOIN dbo.vectorSocio2 AS t2 ON I.Location = T2.Country AND I.Year = T2.Year \n",
							"  INNER JOIN dbo.VectorTiempo AS t1  ON I.Location = T1.Country AND I.Year = T1.Year \n",
							"  where i.ID_Cause = 559 and ID_Sex = 1\n",
							"\n",
							"  AND   [BC] IS NOT NULL\n",
							"   AND   [CO] IS NOT NULL\n",
							"   AND  [CH4] IS NOT NULL\n",
							"   AND   [N20] IS NOT NULL\n",
							"   AND   [Buildings]  IS NOT NULL\n",
							"   AND   [CO2] IS NOT NULL\n",
							"   AND   [bio_HCB] IS NOT NULL\n",
							"   AND   [fossil_HCB] IS NOT NULL\n",
							"   AND   [NH3] IS NOT NULL\n",
							"   AND   [NMVOC] IS NOT NULL\n",
							"     AND [NOx] IS NOT NULL\n",
							"      AND [OC] IS NOT NULL\n",
							"      AND [bio_PAH_BaP] IS NOT NULL\n",
							"      AND [fossil_PAH_BaP] IS NOT NULL\n",
							"      AND [PAH_BbF] IS NOT NULL\n",
							"      AND [bio_PAH_IcdP] IS NOT NULL\n",
							"      AND [fossil_PAH_IcdP] IS NOT NULL\n",
							"      AND [bio_PAH_BkF] IS NOT NULL\n",
							"      AND [fossil_PAH_BbF] IS NOT NULL\n",
							"      AND [bio_PCB] IS NOT NULL\n",
							"      AND [fossil_PCB] IS NOT NULL\n",
							"      AND [bio_PCDD_F] IS NOT NULL\n",
							"      AND [fossil_PCDD_F] IS NOT NULL\n",
							"      AND[PM10] IS NOT NULL\n",
							"      AND [PM25] IS NOT NULL\n",
							"      AND [SO2] IS NOT NULL\n",
							"\n",
							"\t      AND   [GDP] IS NOT NULL\n",
							"      AND [GDP_per_capita] IS NOT NULL\n",
							"      AND [GDP_per_capita_growth] IS NOT NULL\n",
							"      AND [Gross_value_added] IS NOT NULL\n",
							"      AND [Life_expectancy_birth(years)] IS NOT NULL\n",
							"      AND [Access_electricity(%)] IS NOT NULL\n",
							"     -- AND [Broad_money] IS NOT NULL\n",
							"      AND [Agricultural_land] IS NOT NULL\n",
							"\n",
							"      --AND [Tax_revenue] IS NOT NULL\n",
							"      \n",
							"\t  AND [Services] IS NOT NULL\n",
							"\n",
							"\t        AND[Unemployment] IS NOT NULL\n",
							"      AND[Control_Corruption] IS NOT NULL\n",
							"      AND[Contributing_family] IS NOT NULL\n",
							"      AND[Government Effectiveness] IS NOT NULL\n",
							"      AND[Political_Stability] IS NOT NULL\n",
							"      AND[LAW] IS NOT NULL\n",
							"      AND[Regulatory_Quality] IS NOT NULL \"\"\""
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"source": [
							"# C x Q\n",
							"\n",
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"\n",
							"# Consulta para obtener los datos climÃ¡ticos y de incidencia de enfermedad\n",
							"query = \"\"\" \n",
							"-- Clima x Quimicos\n",
							"\n",
							"SELECT --[Name]\n",
							"      --[Year]\n",
							"      [BC]\n",
							"      ,[CO]\n",
							"      ,[CH4]\n",
							"      ,[N20]\n",
							"      ,[Buildings]\n",
							"      ,[CO2]\n",
							"      ,[bio_HCB]\n",
							"      ,[fossil_HCB]\n",
							"      ,[NH3]\n",
							"      ,[NMVOC]\n",
							"      ,[NOx]\n",
							"      ,[OC]\n",
							"      ,[bio_PAH_BaP]\n",
							"      ,[fossil_PAH_BaP]\n",
							"      ,[PAH_BbF]\n",
							"      ,[bio_PAH_IcdP]\n",
							"      ,[fossil_PAH_IcdP]\n",
							"      ,[bio_PAH_BkF]\n",
							"      ,[fossil_PAH_BbF]\n",
							"      ,[bio_PCB]\n",
							"      ,[fossil_PCB]\n",
							"      ,[bio_PCDD_F]\n",
							"      ,[fossil_PCDD_F]\n",
							"      ,[PM10]\n",
							"      ,[PM25]\n",
							"      ,[SO2]\n",
							"\t        ,[Weathercode]\n",
							"      ,[temperature_2m_max]\n",
							"      ,[temperature_2m_min]\n",
							"      ,[temperature_2m_mean]\n",
							"      ,[apparent_temperature_max]\n",
							"      ,[apparent_temperature_min]\n",
							"      ,[apparent_temperature_mean]\n",
							"      ,[precipitation_sum_mm]\n",
							"      ,[rain_sum_mm]\n",
							"      ,[snowfall_sum_cm]\n",
							"      ,[precipitation_hours_h]\n",
							"      ,[windspeed_10m_max_km/h]\n",
							"      ,[windgusts_10m_max_km/h]\n",
							"      ,[winddirection_10m_dominant]\n",
							"      ,[shortwave_radiation_sum]\n",
							"\t  , Value as Incidencia\n",
							"  FROM [IncidenceDWH].[dbo].[vectorQuimicosFloat] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.[Name] AND I.Year = T.Year \n",
							"  INNER JOIN dbo.VectorTiempo AS t1  ON I.Location = T1.Country AND I.Year = T1.Year \n",
							"  where i.ID_Cause = 570 and ID_Sex = 1\n",
							"\n",
							"  AND   [BC] IS NOT NULL\n",
							"   AND   [CO] IS NOT NULL\n",
							"   AND  [CH4] IS NOT NULL\n",
							"   AND   [N20] IS NOT NULL\n",
							"   AND   [Buildings]  IS NOT NULL\n",
							"   AND   [CO2] IS NOT NULL\n",
							"   AND   [bio_HCB] IS NOT NULL\n",
							"   AND   [fossil_HCB] IS NOT NULL\n",
							"   AND   [NH3] IS NOT NULL\n",
							"   AND   [NMVOC] IS NOT NULL\n",
							"     AND [NOx] IS NOT NULL\n",
							"      AND [OC] IS NOT NULL\n",
							"      AND [bio_PAH_BaP] IS NOT NULL\n",
							"      AND [fossil_PAH_BaP] IS NOT NULL\n",
							"      AND [PAH_BbF] IS NOT NULL\n",
							"      AND [bio_PAH_IcdP] IS NOT NULL\n",
							"      AND [fossil_PAH_IcdP] IS NOT NULL\n",
							"      AND [bio_PAH_BkF] IS NOT NULL\n",
							"      AND [fossil_PAH_BbF] IS NOT NULL\n",
							"      AND [bio_PCB] IS NOT NULL\n",
							"      AND [fossil_PCB] IS NOT NULL\n",
							"      AND [bio_PCDD_F] IS NOT NULL\n",
							"      AND [fossil_PCDD_F] IS NOT NULL\n",
							"      AND[PM10] IS NOT NULL\n",
							"      AND [PM25] IS NOT NULL\n",
							"      AND [SO2] IS NOT NULL\n",
							"\n",
							"\"\"\"\n",
							""
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"\n",
							"# Consulta para obtener los datos climÃ¡ticos y de incidencia de enfermedad\n",
							"query = \"\"\"\n",
							"SELECT  --[Country]\n",
							"      --,[Year]\n",
							"      [GDP]\n",
							"      ,[GDP_per_capita]\n",
							"      ,[GDP_per_capita_growth]\n",
							"      ,[Gross_value_added]\n",
							"      ,[Life_expectancy_birth(years)]\n",
							"      ,[Access_electricity(%)]\n",
							"      ,[Agricultural_land]\n",
							"      ,[Services]\n",
							"      ,[Unemployment]\n",
							"      ,[Control_Corruption]\n",
							"      ,[Contributing_family]\n",
							"      ,[Government Effectiveness]\n",
							"      ,[Political_Stability]\n",
							"      ,[LAW]\n",
							"      ,[Regulatory_Quality]\n",
							"\t  ,Value AS Incidencia\n",
							"  FROM [IncidenceDWH].[dbo].[vectorSocio2] as T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.Country AND I.Year = T.Year \n",
							"\n",
							"  WHERE   \n",
							"    i.ID_Cause = 579 and ID_Sex = 2 AND\n",
							"  [GDP] IS NOT NULL\n",
							"      AND [GDP_per_capita] IS NOT NULL\n",
							"      AND [GDP_per_capita_growth] IS NOT NULL\n",
							"      AND [Gross_value_added] IS NOT NULL\n",
							"      AND [Life_expectancy_birth(years)] IS NOT NULL\n",
							"      AND [Access_electricity(%)] IS NOT NULL\n",
							"     -- AND [Broad_money] IS NOT NULL\n",
							"      AND [Agricultural_land] IS NOT NULL\n",
							"\n",
							"      --AND [Tax_revenue] IS NOT NULL\n",
							"      \n",
							"\t  AND [Services] IS NOT NULL\n",
							"\n",
							"\t        AND[Unemployment] IS NOT NULL\n",
							"      AND[Control_Corruption] IS NOT NULL\n",
							"      AND[Contributing_family] IS NOT NULL\n",
							"      AND[Government Effectiveness] IS NOT NULL\n",
							"      AND[Political_Stability] IS NOT NULL\n",
							"      AND[LAW] IS NOT NULL\n",
							"      AND[Regulatory_Quality] IS NOT NULL\"\"\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"df = pd.read_sql(query, connection)\n",
							"\n",
							"# Calcular la desviaciÃ³n estÃ¡ndar de cada columna\n",
							"std_devs = df.std()\n",
							"\n",
							"# Ordenar las columnas por desviaciÃ³n estÃ¡ndar en orden ascendente\n",
							"sorted_columns = std_devs.sort_values(ascending=True)\n",
							"\n",
							"# Seleccionar las 5 columnas con las desviaciones estÃ¡ndar mÃ¡s bajas\n",
							"top_5_similar_columns = sorted_columns.index[:5]\n",
							"\n",
							"print(f\"Las 5 columnas con los valores mÃ¡s similares son:\")\n",
							"print(top_5_similar_columns)"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"source": [
							"# 579 1 Junto Original\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"source": [
							"# 579 2 Junto Varianza\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"source": [
							"# 575 1 Junto Orginal\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"source": [
							"# 575 2 Junto Backward\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"source": [
							"# 574 1 Junto Varianza\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"source": [
							"# 574 2 SxQ Original \n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"source": [
							"# 573 1 Junto Original\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"source": [
							"# 573 2 SxQ Varianza\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"source": [
							"# 571 1 Junto Varianza\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"source": [
							"# 571 2 CxQ Backward\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"source": [
							"# 570 1 CxQ Backward\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"# 570 2 C xQ Original\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"# 568 1\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"# 568 2   # Repetir \n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# 559 1\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# NO # 559 3\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"from sklearn.ensemble import RandomForestClassifier\n",
							"import pandas as pd\n",
							"\n",
							"# Crear un DataFrame de ejemplo (reemplaza esto con tus datos)\n",
							"data = {\n",
							"    'Feature1': [1, 2, 3, 4, 5],\n",
							"    'Feature2': [2, 3, 4, 5, 6],\n",
							"    'Feature3': [1, 1, 0, 1, 1],\n",
							"    'Target': [0, 1, 0, 1, 0]\n",
							"}\n",
							"\n",
							"df = pd.DataFrame(data)\n",
							"\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Target', axis=1)\n",
							"y = df['Target']\n",
							"\n",
							"# Crear un modelo de Random Forest\n",
							"rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
							"\n",
							"# Entrenar el modelo\n",
							"rf.fit(X, y)\n",
							"\n",
							"# Obtener la importancia de las caracterÃ­sticas\n",
							"feature_importances = rf.feature_importances_\n",
							"\n",
							"# Crear un DataFrame para visualizar las importancias\n",
							"importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
							"\n",
							"# Ordenar las caracterÃ­sticas por importancia en orden descendente\n",
							"importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
							"\n",
							"# Imprimir las caracterÃ­sticas mÃ¡s importantes\n",
							"print(importance_df)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    #'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    #('Variance Threshold', VarianceThreshold()),\n",
							"    #('Percentile', SelectPercentile()),\n",
							"    #('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"\n",
							"\n",
							"        # Calcular la importancia de caracterÃ­sticas si el modelo es un Random Forest\n",
							"        if isinstance(model, RandomForestRegressor):\n",
							"            feature_importances = model.feature_importances_\n",
							"            feature_names = X_train.columns.tolist()\n",
							"            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
							"            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
							"            print(f'Importancia de caracterÃ­sticas para {model_name}:')\n",
							"            print(feature_importance_df)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# 573 1 climaXsocio\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    #'Linear Regression': LinearRegression(),\n",
							"    #'Decision Tree': DecisionTreeRegressor(),\n",
							"    #'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"source": [
							"# Socio x Quimicos\n",
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"\n",
							"query = \"\"\"\n",
							"\n",
							"SELECT --[Name]\n",
							"      --[Year]\n",
							"      [BC]\n",
							"      ,[CO]\n",
							"      ,[CH4]\n",
							"      ,[N20]\n",
							"      ,[Buildings]\n",
							"      ,[CO2]\n",
							"      ,[bio_HCB]\n",
							"      ,[fossil_HCB]\n",
							"      ,[NH3]\n",
							"      ,[NMVOC]\n",
							"      ,[NOx]\n",
							"      ,[OC]\n",
							"      ,[bio_PAH_BaP]\n",
							"      ,[fossil_PAH_BaP]\n",
							"      ,[PAH_BbF]\n",
							"      ,[bio_PAH_IcdP]\n",
							"      ,[fossil_PAH_IcdP]\n",
							"      ,[bio_PAH_BkF]\n",
							"      ,[fossil_PAH_BbF]\n",
							"      ,[bio_PCB]\n",
							"      ,[fossil_PCB]\n",
							"      ,[bio_PCDD_F]\n",
							"      ,[fossil_PCDD_F]\n",
							"      ,[PM10]\n",
							"      ,[PM25]\n",
							"      ,[SO2]\n",
							"\t        [GDP]\n",
							"      ,[GDP_per_capita]\n",
							"      ,[GDP_per_capita_growth]\n",
							"      ,[Gross_value_added]\n",
							"      ,[Life_expectancy_birth(years)]\n",
							"      ,[Access_electricity(%)]\n",
							"      ,[Agricultural_land]\n",
							"      ,[Services]\n",
							"      ,[Unemployment]\n",
							"      ,[Control_Corruption]\n",
							"      ,[Contributing_family]\n",
							"      ,[Government Effectiveness]\n",
							"      ,[Political_Stability]\n",
							"      ,[LAW]\n",
							"      ,[Regulatory_Quality]\n",
							"\t  , Value as Incidencia\n",
							"  FROM [IncidenceDWH].[dbo].[vectorQuimicosFloat] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.[Name] AND I.Year = T.Year \n",
							"  INNER JOIN dbo.vectorSocio2 AS t1  ON I.Location = T1.Country AND I.Year = T1.Year \n",
							"  where i.ID_Cause = 575 and ID_Sex = 1\n",
							"\n",
							"  AND   [BC] IS NOT NULL\n",
							"   AND   [CO] IS NOT NULL\n",
							"   AND  [CH4] IS NOT NULL\n",
							"   AND   [N20] IS NOT NULL\n",
							"   AND   [Buildings]  IS NOT NULL\n",
							"   AND   [CO2] IS NOT NULL\n",
							"   AND   [bio_HCB] IS NOT NULL\n",
							"   AND   [fossil_HCB] IS NOT NULL\n",
							"   AND   [NH3] IS NOT NULL\n",
							"   AND   [NMVOC] IS NOT NULL\n",
							"     AND [NOx] IS NOT NULL\n",
							"      AND [OC] IS NOT NULL\n",
							"      AND [bio_PAH_BaP] IS NOT NULL\n",
							"      AND [fossil_PAH_BaP] IS NOT NULL\n",
							"      AND [PAH_BbF] IS NOT NULL\n",
							"      AND [bio_PAH_IcdP] IS NOT NULL\n",
							"      AND [fossil_PAH_IcdP] IS NOT NULL\n",
							"      AND [bio_PAH_BkF] IS NOT NULL\n",
							"      AND [fossil_PAH_BbF] IS NOT NULL\n",
							"      AND [bio_PCB] IS NOT NULL\n",
							"      AND [fossil_PCB] IS NOT NULL\n",
							"      AND [bio_PCDD_F] IS NOT NULL\n",
							"      AND [fossil_PCDD_F] IS NOT NULL\n",
							"      AND[PM10] IS NOT NULL\n",
							"      AND [PM25] IS NOT NULL\n",
							"      AND [SO2] IS NOT NULL\n",
							"\t  AND\n",
							"  [GDP] IS NOT NULL\n",
							"      AND [GDP_per_capita] IS NOT NULL\n",
							"      AND [GDP_per_capita_growth] IS NOT NULL\n",
							"      AND [Gross_value_added] IS NOT NULL\n",
							"      AND [Life_expectancy_birth(years)] IS NOT NULL\n",
							"      AND [Access_electricity(%)] IS NOT NULL\n",
							"     -- AND [Broad_money] IS NOT NULL\n",
							"      AND [Agricultural_land] IS NOT NULL\n",
							"\n",
							"      --AND [Tax_revenue] IS NOT NULL\n",
							"      \n",
							"\t  AND [Services] IS NOT NULL\n",
							"\n",
							"\t        AND[Unemployment] IS NOT NULL\n",
							"      AND[Control_Corruption] IS NOT NULL\n",
							"      AND[Contributing_family] IS NOT NULL\n",
							"      AND[Government Effectiveness] IS NOT NULL\n",
							"      AND[Political_Stability] IS NOT NULL\n",
							"      AND[LAW] IS NOT NULL\n",
							"      AND[Regulatory_Quality] IS NOT NULL \"\"\""
						],
						"outputs": [],
						"execution_count": 89
					},
					{
						"cell_type": "code",
						"source": [
							"# Socio x Clima\n",
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"\n",
							"query = \"\"\"\n",
							"SELECT --[Name]\n",
							"      --[Year]\n",
							"\t        [GDP]\n",
							"      ,[GDP_per_capita]\n",
							"      ,[GDP_per_capita_growth]\n",
							"      ,[Gross_value_added]\n",
							"      ,[Life_expectancy_birth(years)]\n",
							"      ,[Access_electricity(%)]\n",
							"      ,[Agricultural_land]\n",
							"      ,[Services]\n",
							"      ,[Unemployment]\n",
							"      ,[Control_Corruption]\n",
							"      ,[Contributing_family]\n",
							"      ,[Government Effectiveness]\n",
							"      ,[Political_Stability]\n",
							"      ,[LAW]\n",
							"      ,[Regulatory_Quality]\n",
							"\n",
							"\t       ,[Weathercode]\n",
							"      ,[temperature_2m_max]\n",
							"      ,[temperature_2m_min]\n",
							"      ,[temperature_2m_mean]\n",
							"      ,[apparent_temperature_max]\n",
							"      ,[apparent_temperature_min]\n",
							"      ,[apparent_temperature_mean]\n",
							"      ,[precipitation_sum_mm]\n",
							"      ,[rain_sum_mm]\n",
							"      ,[snowfall_sum_cm]\n",
							"      ,[precipitation_hours_h]\n",
							"      ,[windspeed_10m_max_km/h]\n",
							"      ,[windgusts_10m_max_km/h]\n",
							"      ,[winddirection_10m_dominant]\n",
							"      ,[shortwave_radiation_sum]\n",
							"\n",
							"\t  , Value as Incidencia\n",
							"  FROM [IncidenceDWH].[dbo].[VectorTiempo] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.Country AND I.Year = T.Year \n",
							"  INNER JOIN dbo.vectorSocio2 AS t1  ON I.Location = T1.Country AND I.Year = T1.Year \n",
							"  where i.ID_Cause = 573 and ID_Sex = 1\n",
							"  AND [GDP] IS NOT NULL\n",
							"      AND [GDP_per_capita] IS NOT NULL\n",
							"      AND [GDP_per_capita_growth] IS NOT NULL\n",
							"      AND [Gross_value_added] IS NOT NULL\n",
							"      AND [Life_expectancy_birth(years)] IS NOT NULL\n",
							"      AND [Access_electricity(%)] IS NOT NULL\n",
							"     -- AND [Broad_money] IS NOT NULL\n",
							"      AND [Agricultural_land] IS NOT NULL\n",
							"\n",
							"      --AND [Tax_revenue] IS NOT NULL\n",
							"      \n",
							"\t  AND [Services] IS NOT NULL\n",
							"\n",
							"\t        AND[Unemployment] IS NOT NULL\n",
							"      AND[Control_Corruption] IS NOT NULL\n",
							"      AND[Contributing_family] IS NOT NULL\n",
							"      AND[Government Effectiveness] IS NOT NULL\n",
							"      AND[Political_Stability] IS NOT NULL\n",
							"      AND[LAW] IS NOT NULL\n",
							"      AND[Regulatory_Quality] IS NOT NULL\"\"\" "
						],
						"outputs": [],
						"execution_count": 95
					},
					{
						"cell_type": "code",
						"source": [
							"# 573 1 climaXsocio\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "code",
						"source": [
							"# 570 1 climaXsocio\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 94
					},
					{
						"cell_type": "code",
						"source": [
							"# 559 2 climaXsocio\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 92
					},
					{
						"cell_type": "code",
						"source": [
							"# 575 1 socioXquimicos\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"source": [
							"# 570 2 socioXquimicos\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"source": [
							"# 570 1 socioXquimicos\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"source": [
							"# 568 1 socioXquimicos\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "code",
						"source": [
							"# 559 2 socioXquimicos\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"source": [
							"# 559 1 socioXquimicos\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 80
					},
					{
						"cell_type": "code",
						"source": [
							"# 571 1 climaXquimicos\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"source": [
							"# Terminar Clima x Quimicos  arriba"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"## Todo junto terminado"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# 575 1\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 76
					},
					{
						"cell_type": "code",
						"source": [
							"# 573 1\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 74
					},
					{
						"cell_type": "code",
						"source": [
							"# 570 1\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 72
					},
					{
						"cell_type": "code",
						"source": [
							"# 568 2\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias FutureWarning relacionadas con SequentialFeatureSelector\n",
							"warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.feature_selection\")\n",
							"\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 70
					},
					{
						"cell_type": "code",
						"source": [
							"## Junto"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# 568 1\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"source": [
							"######## Quimicos abajo"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# 579 1\n",
							"\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"source": [
							"# 574 2\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"source": [
							"# 573 1\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"source": [
							"# 570 2\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"source": [
							"# 568 1\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"source": [
							"# 559 1\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"source": [
							"############################## Clima abajo"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# 569 2\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"source": [
							"# 579 2\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"source": [
							"# 575 1\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"source": [
							"# 574 1\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"source": [
							"# 570 2\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"source": [
							"# 559 2\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"source": [
							"# 559 1\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"source": [
							"#568 2\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR()\n",
							"    #'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"import numpy as np\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SequentialFeatureSelector\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"import time\n",
							"from sklearn.pipeline import Pipeline\n",
							"import warnings\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"\n",
							"# Suprime la advertencia temporalmente\n",
							"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"import warnings\n",
							"\n",
							"# Ignora las advertencias RuntimeWarning\n",
							"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
							"\n",
							"# Cargar el DataFrame\n",
							"# df = pd.read_csv('tu_archivo.csv')  # Reemplaza 'tu_archivo.csv' con tu archivo de datos\n",
							"# Asumiendo que el DataFrame tiene una columna 'Incidencia' y el resto son caracterÃ­sticas\n",
							"df = pd.read_sql(query, connection)\n",
							"# Separar las caracterÃ­sticas (X) y la variable objetivo (y)\n",
							"X = df.drop('Incidencia', axis=1)\n",
							"y = df['Incidencia']\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Crear un objeto StandardScaler para normalizar los datos\n",
							"scaler = StandardScaler()\n",
							"\n",
							"# Crear diccionarios para almacenar los modelos y sus nombres\n",
							"models = {\n",
							"    'Linear Regression': LinearRegression(),\n",
							"    'Decision Tree': DecisionTreeRegressor(),\n",
							"    'Random Forest': RandomForestRegressor(),\n",
							"    'SVM': SVR(),\n",
							"    'MLP': MLPRegressor()\n",
							"}\n",
							"\n",
							"# Lista de tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"feature_selectors = [\n",
							"    ('Original', None),\n",
							"    ('Variance Threshold', VarianceThreshold()),\n",
							"    ('Percentile', SelectPercentile()),\n",
							"    ('Backward Search', SequentialFeatureSelector(LinearRegression(), direction='backward'))\n",
							"]\n",
							"\n",
							"# Bucle principal para iterar a travÃ©s de modelos y tÃ©cnicas de selecciÃ³n de caracterÃ­sticas\n",
							"for model_name, model in models.items():\n",
							"    for selector_name, selector in feature_selectors:\n",
							"        # Crear una lista de pasos para el pipeline (escalado y selecciÃ³n de caracterÃ­sticas)\n",
							"        steps = [('scaler', scaler)]\n",
							"        if selector is not None:\n",
							"            steps.append(('feature_selector', selector))\n",
							"        \n",
							"        # Crear el pipeline con el modelo y los pasos de preprocesamiento\n",
							"        pipeline = Pipeline(steps + [('model', model)])\n",
							"        \n",
							"        # Realizar validaciÃ³n cruzada con KFold\n",
							"        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"        \n",
							"        #start_time = time.time()\n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n durante la validaciÃ³n cruzada\n",
							"        rmse_scores = np.sqrt(-cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
							"        mae_scores = -cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
							"        r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='r2')\n",
							"        #end_time = time.time()\n",
							"        \n",
							"        # Ajustar el modelo al conjunto de entrenamiento completo\n",
							"        pipeline.fit(X_train, y_train)\n",
							"        \n",
							"        # Predecir en el conjunto de prueba\n",
							"        y_pred = pipeline.predict(X_test)\n",
							"        \n",
							"        # Calcular las mÃ©tricas de evaluaciÃ³n en el conjunto de prueba\n",
							"        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
							"        test_mae = mean_absolute_error(y_test, y_pred)\n",
							"        test_r2 = r2_score(y_test, y_pred)\n",
							"        \n",
							"        # Imprimir resultados\n",
							"        print(f'Modelo: {model_name}, Selector de CaracterÃ­sticas: {selector_name}')\n",
							"        print(f'RMSE (Promedio durante CV): {rmse_scores.mean():.4f}')\n",
							"        print(f'MAE (Promedio durante CV): {mae_scores.mean():.4f}')\n",
							"        print(f'R2 (Promedio durante CV): {r2_scores.mean():.4f}')\n",
							"        #print(f'Tiempo de EjecuciÃ³n (s): {end_time - start_time:.2f}')\n",
							"        print(f'Test RMSE: {test_rmse:.4f}')\n",
							"        print(f'Test MAE: {test_mae:.4f}')\n",
							"        print(f'Test R2: {test_r2:.4f}')\n",
							"        print('-' * 50)\n",
							""
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectPercentile, f_regression, RFECV, chi2\n",
							"import numpy as np\n",
							"\n",
							"# Lee los datos desde la base de datos o tu fuente de datos\n",
							"df = pd.read_sql(query, connection)\n",
							"\n",
							"# Crea variables de caracterÃ­sticas y la variable objetivo\n",
							"X = df.drop('incidencia', axis=1)\n",
							"y = df['incidencia']\n",
							"\n",
							"# Normaliza los valores de las caracterÃ­sticas utilizando StandardScaler\n",
							"scaler = StandardScaler()\n",
							"X_scaled = scaler.fit_transform(X)\n",
							"\n",
							"# Divide los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=101)\n",
							"\n",
							"# Crea un modelo de regresiÃ³n lineal\n",
							"model = LinearRegression()\n",
							"\n",
							"# Realiza la validaciÃ³n cruzada con K-Fold (en este caso, 5-Fold)\n",
							"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"\n",
							"# Agrega el selector de caracterÃ­sticas basado en la varianza\n",
							"variance_selector = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
							"X_variance_selected = variance_selector.fit_transform(X_scaled)\n",
							"\n",
							"# Agrega el selector de caracterÃ­sticas basado en percentiles\n",
							"#percentile_selector = SelectPercentile(score_func=f_regression, percentile=10)\n",
							"percentile_selector = SelectPercentile(score_func=chi2, percentile=10)  # Cambia 'percentile' segÃºn el valor deseado\n",
							"X_percentile_selected = percentile_selector.fit_transform(X_scaled, y)\n",
							"\n",
							"# Agrega el enfoque de eliminaciÃ³n hacia atrÃ¡s (backward search) con RFECV\n",
							"rfecv_selector = RFECV(estimator=model, cv=kf, scoring='neg_mean_squared_error')  # Puedes cambiar la mÃ©trica\n",
							"X_rfecv_selected = rfecv_selector.fit_transform(X_scaled, y)\n",
							"\n",
							"# Ajusta el modelo de regresiÃ³n lineal utilizando las caracterÃ­sticas seleccionadas por cada mÃ©todo\n",
							"models = {\n",
							"    'Original Features': model.fit(X_train, y_train),\n",
							"    'Variance Selected': model.fit(X_variance_selected, y),\n",
							"    'Percentile Selected': model.fit(X_percentile_selected, y),\n",
							"    'RFECV Selected': model.fit(X_rfecv_selected, y)\n",
							"}\n",
							"\n",
							"# EvalÃºa los modelos y muestra las mÃ©tricas\n",
							"for model_name, fitted_model in models.items():\n",
							"    if model_name == 'Original Features':\n",
							"        X_test_selected = X_test  # No se seleccionan caracterÃ­sticas para el modelo original\n",
							"    elif model_name == 'Variance Selected':\n",
							"        X_test_selected = variance_selector.transform(X_test)\n",
							"    elif model_name == 'Percentile Selected':\n",
							"        X_test_selected = percentile_selector.transform(X_test)\n",
							"    elif model_name == 'RFECV Selected':\n",
							"        X_test_selected = rfecv_selector.transform(X_test)\n",
							"    \n",
							"    # AsegÃºrate de que el nÃºmero de caracterÃ­sticas coincida con las del modelo\n",
							"    if X_test_selected.shape[1] != X_train.shape[1]:\n",
							"        print(f'El nÃºmero de caracterÃ­sticas seleccionadas para {model_name} no coincide con el modelo original.')\n",
							"    else:\n",
							"        predictions = fitted_model.predict(X_test_selected)\n",
							"        mse = mean_squared_error(y_test, predictions)\n",
							"        mae = mean_absolute_error(y_test, predictions)\n",
							"        rmse = np.sqrt(mse)\n",
							"        r2 = r2_score(y_test, predictions)\n",
							"        \n",
							"        print(f'Model: {model_name}')\n",
							"        print('Mean Squared Error (MSE): ', mse)\n",
							"        print('Mean Absolute Error (MAE): ', mae)\n",
							"        print('Root Mean Squared Error (RMSE): ', rmse)\n",
							"        print('R^2 Score: ', r2)\n",
							"        print('---')"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"\n",
							"# Lee los datos desde la base de datos o tu fuente de datos\n",
							"df = pd.read_sql(query, connection)\n",
							"\n",
							"# Crea variables de caracterÃ­sticas y la variable objetivo\n",
							"X = df.drop('incidencia', axis=1)\n",
							"y = df['incidencia']\n",
							"\n",
							"# Normaliza los valores de las caracterÃ­sticas utilizando StandardScaler\n",
							"scaler = StandardScaler()\n",
							"X_scaled = scaler.fit_transform(X)\n",
							"\n",
							"# Divide los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=101)\n",
							"\n",
							"# Crea un modelo de regresiÃ³n lineal\n",
							"model = LinearRegression()\n",
							"\n",
							"# Realiza la validaciÃ³n cruzada con K-Fold (en este caso, 5-Fold)\n",
							"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"\n",
							"# Calcula las mÃ©tricas de rendimiento para cada fold\n",
							"mse_scores = -cross_val_score(model, X_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
							"mae_scores = -cross_val_score(model, X_scaled, y, cv=kf, scoring='neg_mean_absolute_error')\n",
							"r2_scores = cross_val_score(model, X_scaled, y, cv=kf, scoring='r2')\n",
							"\n",
							"# Calcula la mÃ©trica RMSE para cada fold\n",
							"rmse_scores = (mse_scores.mean()) ** 0.5\n",
							"\n",
							"# Imprime todas las mÃ©tricas\n",
							"print('Mean MSE: ', mse_scores.mean())\n",
							"print('Mean MAE: ', mae_scores.mean())\n",
							"print('Mean RMSE: ', rmse_scores)\n",
							"print('Mean R^2: ', r2_scores.mean())\n",
							""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"# Establece la conexiÃ³n con la base de datos SQL Server\n",
							"#connection = pyodbc.connect('DRIVER={SQL Server};SERVER=NombreDelServidor;DATABASE=NombreDeLaBaseDeDatos;Trusted_Connection=yes;')\n",
							"\n",
							"resultado = []\n",
							"\n",
							"for i in range(5):\n",
							"    # Carga los datos en un DataFrame\n",
							"    data = pd.read_sql(query, connection)\n",
							"\n",
							"    # DivisiÃ³n de datos en entrenamiento y prueba\n",
							"    X = data.drop('incidencia', axis=1)\n",
							"    y = data['incidencia']\n",
							"    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
							"\n",
							"    # CreaciÃ³n y entrenamiento del modelo de regresiÃ³n lineal\n",
							"    model = LinearRegression()\n",
							"    model.fit(X_train, y_train)\n",
							"\n",
							"    # Predicciones en el conjunto de prueba\n",
							"    y_pred = model.predict(X_test)\n",
							"\n",
							"    # EvaluaciÃ³n del modelo\n",
							"    mse = mean_squared_error(y_test, y_pred)\n",
							"    r2 = r2_score(y_test, y_pred)\n",
							"\n",
							"    print(f'Mean Squared Error: {mse}')\n",
							"    print(f'R-squared: {r2}')\n",
							"    resultado.append(r2)\n",
							"print(\"....\")\n",
							"mean = sum(resultado)/len(resultado)\n",
							"print(mean)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"from sklearn.model_selection import train_test_split, KFold\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
							"import pyodbc\n",
							"\n",
							"# Ridge() o Lasso() sklearn.linear_model\n",
							"# DecisionTreeRegressor() sklearn.tree\n",
							"# RandomForestRegressor() de sklearn.ensemble\n",
							"# SVR() sklearn.svm\n",
							"# MLPRegressor() sklearn.neural_network\n",
							"from sklearn.linear_model import Ridge, Lasso\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.feature_selection import RFE\n",
							"from sklearn.feature_selection import SelectFromModel\n",
							"from sklearn.linear_model import LogisticRegression\n",
							"\n",
							"from sklearn.feature_selection import VarianceThreshold\n",
							"from sklearn import preprocessing\n",
							"\n",
							"\n",
							"from sklearn.feature_selection import VarianceThreshold\n",
							"from sklearn.feature_selection import SelectKBest\n",
							"from sklearn.feature_selection import chi2\n",
							"# chi2_selector = SelectKBest(chi2, k=2)\n",
							"# X_kbest = chi2_selector.fit_transform(X, y)\n",
							"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
							"from sklearn.metrics import roc_auc_score \n",
							"from mlxtend.feature_selection import SequentialFeatureSelector \n",
							"# feature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1),\n",
							"#                           k_features=15, forward=False, \n",
							"#                           verbose=2, scoring=âroc_aucâ, cv=4)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							"\n",
							"# Consulta para obtener los datos climÃ¡ticos y de incidencia de enfermedad\n",
							"query1 = \"\"\"\n",
							"SELECT --[Country],\n",
							"      --t.[Year],\n",
							"      [Weathercode]\n",
							"      ,[temperature_2m_max]\n",
							"      ,[temperature_2m_min]\n",
							"      ,[temperature_2m_mean]\n",
							"      ,[apparent_temperature_max]\n",
							"      ,[apparent_temperature_min]\n",
							"      ,[apparent_temperature_mean]\n",
							"      ,[precipitation_sum_mm]\n",
							"      ,[rain_sum_mm]\n",
							"      ,[snowfall_sum_cm]\n",
							"      ,[precipitation_hours_h]\n",
							"      ,[windspeed_10m_max_km/h]\n",
							"      ,[windgusts_10m_max_km/h]\n",
							"      ,[winddirection_10m_dominant]\n",
							"      ,[shortwave_radiation_sum]\n",
							"\t  , Value as incidencia\n",
							"\t  , GDP\n",
							"\t  ,GDP_per_capita\n",
							"\t  ,CO2_emissions\n",
							"  FROM [IncidenceDWH].[dbo].[VectorTiempo] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.Country AND I.Year = T.Year \n",
							"  INNER JOIN vectorPib AS P ON P.Location = T.Country AND P.Year = T.Year \n",
							"  where i.ID_Cause = 568 \n",
							"  AND ID_Sex = 1\n",
							"  AND GDP NOT LIKE '..' AND GDP_per_capita NOT LIKE '..'  AND CO2_emissions NOT LIKE '..' \"\"\"\n",
							"query2 = \"\"\"\n",
							"SELECT --[Country],\n",
							"      --t.[Year],\n",
							"      [Weathercode]\n",
							"      ,[temperature_2m_max]\n",
							"      ,[temperature_2m_min]\n",
							"      ,[temperature_2m_mean]\n",
							"      ,[apparent_temperature_max]\n",
							"      ,[apparent_temperature_min]\n",
							"      ,[apparent_temperature_mean]\n",
							"      ,[precipitation_sum_mm]\n",
							"      ,[rain_sum_mm]\n",
							"      ,[snowfall_sum_cm]\n",
							"      ,[precipitation_hours_h]\n",
							"      ,[windspeed_10m_max_km/h]\n",
							"      ,[windgusts_10m_max_km/h]\n",
							"      ,[winddirection_10m_dominant]\n",
							"      ,[shortwave_radiation_sum]\n",
							"\t  , Value as incidencia\n",
							"\t  , GDP\n",
							"\t  ,GDP_per_capita\n",
							"\t  ,CO2_emissions\n",
							"  FROM [IncidenceDWH].[dbo].[VectorTiempo] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.Country AND I.Year = T.Year \n",
							"  INNER JOIN vectorPib AS P ON P.Location = T.Country AND P.Year = T.Year \n",
							"  where i.ID_Cause = 571 --568\n",
							"  AND ID_Sex = 2\n",
							"   AND GDP NOT LIKE '..' AND GDP_per_capita NOT LIKE '..'  AND CO2_emissions NOT LIKE '..' \"\"\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"import numpy as np\n",
							"import matplotlib.pyplot as plt\n",
							"import seaborn as sns\n",
							"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.neural_network import MLPRegressor\n",
							"from sklearn.tree import DecisionTreeRegressor\n",
							"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression, SequentialFeatureSelector\n",
							"from mlxtend.feature_selection import SequentialFeatureSelector\n",
							"from warnings import simplefilter\n",
							"import sys\n",
							"from sklearn.exceptions import ConvergenceWarning\n",
							"import pyodbc"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"query1 = \"\"\"SELECT -- [Country]\n",
							"     -- ,t.[Year],\n",
							"      [Weathercode]\n",
							"      ,[temperature_2m_max]\n",
							"      ,[temperature_2m_min]\n",
							"      ,[temperature_2m_mean]\n",
							"      ,[apparent_temperature_max]\n",
							"      ,[apparent_temperature_min]\n",
							"      ,[apparent_temperature_mean]\n",
							"      ,[precipitation_sum_mm]\n",
							"      ,[rain_sum_mm]\n",
							"      ,[snowfall_sum_cm]\n",
							"      ,[precipitation_hours_h]\n",
							"      ,[windspeed_10m_max_km/h]\n",
							"      ,[windgusts_10m_max_km/h]\n",
							"      ,[winddirection_10m_dominant]\n",
							"      ,[shortwave_radiation_sum]\n",
							"\t\t, Value as incidencia\n",
							"\n",
							"  FROM [IncidenceDWH].[dbo].[VectorTiempo] AS T\n",
							"  INNER JOIN [dbo].[factIncidences] AS I ON I.Location = T.Country AND I.Year = T.Year \n",
							"  --INNER JOIN vectorPib AS P ON P.Location = T.Country AND P.Year = T.Year \n",
							"  where i.ID_Cause = 559 \n",
							"  AND ID_Sex = 1\"\"\"\n",
							"\n",
							"connection = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-6UKL08J;DATABASE=IncidenceDWH;Trusted_Connection=yes;')\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"# Ignorar el aviso de convergencia para el MLPRegressor\n",
							"simplefilter(\"ignore\", category=ConvergenceWarning)\n",
							"\n",
							"# importing data\n",
							"# df = pd.read_csv('Real estate.csv')\n",
							"df = pd.read_sql(query1, connection)\n",
							"\n",
							"# creating feature variables\n",
							"X = df.drop('incidencia', axis=1)\n",
							"y = df['incidencia']\n",
							"\n",
							"# creating train and test sets\n",
							"X_train, X_test, y_train, y_test = train_test_split(\n",
							"    X, y, test_size=0.3, random_state=101)\n",
							"\n",
							"# creating a list of models\n",
							"models = [\n",
							"    ('Linear Regression', LinearRegression()),\n",
							"    ('Decision Tree', DecisionTreeRegressor()),\n",
							"    ('Random Forest', RandomForestRegressor()),\n",
							"    ('SVR', SVR()),  # Support Vector Machine\n",
							"    ('MLP', MLPRegressor())  # Limit MLP to 1 iteration to suppress warning\n",
							"]\n",
							"\n",
							"# Standardize features\n",
							"scaler = StandardScaler()\n",
							"X_train_scaled = scaler.fit_transform(X_train)\n",
							"X_test_scaled = scaler.transform(X_test)\n",
							"\n",
							"# Variance-based feature selection\n",
							"selector_variance = VarianceThreshold()\n",
							"X_train_variance = selector_variance.fit_transform(X_train_scaled)\n",
							"X_test_variance = selector_variance.transform(X_test_scaled)\n",
							"\n",
							"# Univariate feature selection (k-best)\n",
							"selector_kbest = SelectKBest(score_func=f_regression, k=5)  # Adjust 'k' as needed\n",
							"X_train_kbest = selector_kbest.fit_transform(X_train_scaled, y_train)\n",
							"X_test_kbest = selector_kbest.transform(X_test_scaled)\n",
							"\n",
							"# Model training and evaluation with Variance-Based Selection\n",
							"print(\"Variance-Based Feature Selection:\")\n",
							"for name, model in models:\n",
							"    kfold = KFold(n_splits=5, shuffle=True, random_state=101)\n",
							"    cv_mse = -cross_val_score(model, X_train_variance, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
							"    cv_mae = -cross_val_score(model, X_train_variance, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
							"    cv_r2 = cross_val_score(model, X_train_variance, y_train, cv=kfold, scoring='r2')\n",
							"\n",
							"    model.fit(X_train_variance, y_train)\n",
							"    predictions = model.predict(X_test_variance)\n",
							"\n",
							"    mse = mean_squared_error(y_test, predictions)\n",
							"    rmse = np.sqrt(mse)  # Calculate RMSE\n",
							"\n",
							"    print(f'Model: {name}')\n",
							"    print(f'Cross-validated MSE: {cv_mse.mean()}')\n",
							"    print(f'Cross-validated MAE: {cv_mae.mean()}')\n",
							"    print(f'Cross-validated R^2: {cv_r2.mean()}')\n",
							"    print(f'Test MSE: {mse}')\n",
							"    print(f'Test RMSE: {rmse}')  # Print RMSE\n",
							"    print(f'Test MAE: {mean_absolute_error(y_test, predictions)}')\n",
							"    print(f'Test R^2: {r2_score(y_test, predictions)}')\n",
							"    print('---')\n",
							"\n",
							"# Model training and evaluation with k-best Selection\n",
							"print(\"k-best Feature Selection:\")\n",
							"for name, model in models:\n",
							"    kfold = KFold(n_splits=5, shuffle=True, random_state=101)\n",
							"    cv_mse = -cross_val_score(model, X_train_kbest, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
							"    cv_mae = -cross_val_score(model, X_train_kbest, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
							"    cv_r2 = cross_val_score(model, X_train_kbest, y_train, cv=kfold, scoring='r2')\n",
							"\n",
							"    model.fit(X_train_kbest, y_train)\n",
							"    predictions = model.predict(X_test_kbest)\n",
							"\n",
							"    mse = mean_squared_error(y_test, predictions)\n",
							"    rmse = np.sqrt(mse)  # Calculate RMSE\n",
							"\n",
							"    print(f'Model: {name}')\n",
							"    print(f'Cross-validated MSE: {cv_mse.mean()}')\n",
							"    print(f'Cross-validated MAE: {cv_mae.mean()}')\n",
							"    print(f'Cross-validated R^2: {cv_r2.mean()}')\n",
							"    print(f'Test MSE: {mse}')\n",
							"    print(f'Test RMSE: {rmse}')  # Print RMSE\n",
							"    print(f'Test MAE: {mean_absolute_error(y_test, predictions)}')\n",
							"    print(f'Test R^2: {r2_score(y_test, predictions)}')\n",
							"    print('---')\n",
							"\n",
							"# Model training and evaluation with Sequential Feature Selector (Backward Selection)\n",
							"print(\"Sequential Feature Selector (Backward Selection):\")\n",
							"for name, model in models:\n",
							"    kfold = KFold(n_splits=5, shuffle=True, random_state=101)\n",
							"\n",
							"    # Create a Sequential Feature Selector object for backward selection\n",
							"    sfs = SequentialFeatureSelector(model, k_features=5, forward=False, scoring='neg_mean_squared_error', cv=kfold)\n",
							"\n",
							"    # Fit the selector to the training data\n",
							"    sfs.fit(X_train_scaled, y_train)\n",
							"\n",
							"    # Get the selected feature indices\n",
							"    selected_indices = list(sfs.k_feature_idx_)\n",
							"\n",
							"    # Subset the training and test data based on selected features\n",
							"    X_train_sfs = X_train_scaled[:, selected_indices]\n",
							"    X_test_sfs = X_test_scaled[:, selected_indices]\n",
							"\n",
							"    # Cross-validation\n",
							"    cv_mse = -cross_val_score(model, X_train_sfs, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
							"    cv_mae = -cross_val_score(model, X_train_sfs, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
							"    cv_r2 = cross_val_score(model, X_train_sfs, y_train, cv=kfold, scoring='r2')\n",
							"\n",
							"    # Model training and evaluation\n",
							"    model.fit(X_train_sfs, y_train)\n",
							"    predictions = model.predict(X_test_sfs)\n",
							"\n",
							"    mse = mean_squared_error(y_test, predictions)\n",
							"    rmse = np.sqrt(mse)  # Calculate RMSE\n",
							"\n",
							"    print(f'Model: {name}')\n",
							"    print(f'Selected Features: {selected_indices}')\n",
							"    print(f'Cross-validated MSE: {cv_mse.mean()}')\n",
							"    print(f'Cross-validated MAE: {cv_mae.mean()}')\n",
							"    print(f'Cross-validated R^2: {cv_r2.mean()}')\n",
							"    print(f'Test MSE: {mse}')\n",
							"    print(f'Test RMSE: {rmse}')  # Print RMSE\n",
							"    print(f'Test MAE: {mean_absolute_error(y_test, predictions)}')\n",
							"    print(f'Test R^2: {r2_score(y_test, predictions)}')\n",
							"    print('---')\n",
							"\n",
							"# Model training and evaluation without feature selection\n",
							"print(\"No Feature Selection:\")\n",
							"for name, model in models:\n",
							"    kfold = KFold(n_splits=5, shuffle=True, random_state=101)\n",
							"    cv_mse = -cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
							"    cv_mae = -cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
							"    cv_r2 = cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring='r2')\n",
							"\n",
							"    model.fit(X_train_scaled, y_train)\n",
							"    predictions = model.predict(X_test_scaled)\n",
							"\n",
							"    mse = mean_squared_error(y_test, predictions)\n",
							"    rmse = np.sqrt(mse)  # Calculate RMSE\n",
							"\n",
							"    print(f'Model: {name}')\n",
							"    print(f'Cross-validated MSE: {cv_mse.mean()}')\n",
							"    print(f'Cross-validated MAE: {cv_mae.mean()}')\n",
							"    print(f'Cross-validated R^2: {cv_r2.mean()}')\n",
							"    print(f'Test MSE: {mse}')\n",
							"    print(f'Test RMSE: {rmse}')  # Print RMSE\n",
							"    print(f'Test MAE: {mean_absolute_error(y_test, predictions)}')\n",
							"    print(f'Test R^2: {r2_score(y_test, predictions)}')\n",
							"    print('---')\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# importing data\n",
							"# df = pd.read_csv('Real estate.csv')\n",
							"df = pd.read_sql(query1, connection)\n",
							"  \n",
							"# creating feature variables\n",
							"X = df.drop('incidencia', axis=1)\n",
							"y = df['incidencia']\n",
							"  \n",
							"# creating train and test sets\n",
							"X_train, X_test, y_train, y_test = train_test_split(\n",
							"    X, y, test_size=0.3, random_state=101)\n",
							"  \n",
							"# creating a list of models\n",
							"models = [\n",
							"    ('Linear Regression', LinearRegression()),\n",
							"    ('Decision Tree', DecisionTreeRegressor()),\n",
							"    ('Random Forest', RandomForestRegressor()),\n",
							"    ('SVR', SVR()),  # Support Vector Machine\n",
							"    ('MLP', MLPRegressor())\n",
							"]\n",
							"\n",
							"# Standardize features\n",
							"scaler = StandardScaler()\n",
							"X_train_scaled = scaler.fit_transform(X_train)\n",
							"X_test_scaled = scaler.transform(X_test)\n",
							"\n",
							"# Variance-based feature selection\n",
							"selector = VarianceThreshold()\n",
							"X_train_selected = selector.fit_transform(X_train_scaled)\n",
							"X_test_selected = selector.transform(X_test_scaled)\n",
							"\n",
							"# Model training and evaluation\n",
							"for name, model in models:\n",
							"    kfold = KFold(n_splits=5, shuffle=True, random_state=101)\n",
							"    cv_mse = -cross_val_score(model, X_train_selected, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
							"    cv_mae = -cross_val_score(model, X_train_selected, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
							"    cv_r2 = cross_val_score(model, X_train_selected, y_train, cv=kfold, scoring='r2')\n",
							"    \n",
							"    model.fit(X_train_selected, y_train)\n",
							"    predictions = model.predict(X_test_selected)\n",
							"    \n",
							"    print(f'Model: {name}')\n",
							"    print(f'Cross-validated MSE: {cv_mse.mean()}')\n",
							"    print(f'Cross-validated MAE: {cv_mae.mean()}')\n",
							"    print(f'Cross-validated R^2: {cv_r2.mean()}')\n",
							"    print(f'Test MSE: {mean_squared_error(y_test, predictions)}')\n",
							"    print(f'Test MAE: {mean_absolute_error(y_test, predictions)}')\n",
							"    print(f'Test R^2: {r2_score(y_test, predictions)}')\n",
							"    print('---')\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"## Aqui\n",
							"\n",
							"# import pandas as pd\n",
							"# import numpy as np\n",
							"# import matplotlib.pyplot as plt\n",
							"# import seaborn as sns\n",
							"# from sklearn.model_selection import train_test_split\n",
							"# from sklearn.linear_model import LinearRegression\n",
							"# from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
							"# from sklearn import preprocessing\n",
							"  \n",
							"# importing data\n",
							"#df = pd.read_csv('Real estate.csv')\n",
							"df = pd.read_sql(query, connection)\n",
							"  \n",
							"# creating feature variables\n",
							"X = df.drop('incidencia', axis=1)\n",
							"y = df['incidencia']\n",
							"\n",
							"# X = data.drop('incidencia', axis=1)\n",
							"# y = data['incidencia']\n",
							"\n",
							"#print(X)\n",
							"#print(y)\n",
							"  \n",
							"# creating train and test sets\n",
							"X_train, X_test, y_train, y_test = train_test_split(\n",
							"    X, y, test_size=0.3, random_state=101)\n",
							"  \n",
							"# creating a regression model\n",
							"model = LinearRegression()\n",
							"model2 = DecisionTreeRegressor()\n",
							"  \n",
							"# fitting the model\n",
							"model.fit(X_train, y_train)\n",
							"model2.fit(X_train, y_train)\n",
							"# making predictions\n",
							"predictions = model.predict(X_test)\n",
							"predictions2 = model2.predict(X_test) \n",
							"\n",
							"# model evaluation\n",
							"print('mean_squared_error : ', mean_squared_error(y_test, predictions))\n",
							"print('mean_absolute_error : ', mean_absolute_error(y_test, predictions))\n",
							"print('r2 : ', r2_score(y_test, predictions))\n",
							"#print('accuracy : ', accuracy_score(y_test, predictions))\n",
							"print(\"-----\")\n",
							"print('mean_squared_error : ', mean_squared_error(y_test, predictions2))\n",
							"print('mean_absolute_error : ', mean_absolute_error(y_test, predictions2))\n",
							"print('r2 : ', r2_score(y_test, predictions2))\n",
							"#print('accuracy : ', accuracy_score(y_test, predictions2))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Con kfold\n",
							"\n",
							"data = pd.read_sql(query1, connection)\n",
							"data2 = pd.read_sql(query2, connection)\n",
							"\n",
							"# data = preprocessing.normalize(data, norm='l1')\n",
							"# data2 = preprocessing.normalize(data2, norm='l1')\n",
							"# ... (ConexiÃ³n a la base de datos y carga de datos como se muestra en el cÃ³digo anterior)\n",
							"X = data.drop('incidencia', axis=1)\n",
							"y = data['incidencia']\n",
							"\n",
							"\n",
							"\n",
							"X2 = data2.drop('incidencia', axis=1)\n",
							"y2 = data2['incidencia']\n",
							"\n",
							"\n",
							"X = preprocessing.normalize(X, norm='l1')\n",
							"y = preprocessing.normalize(y, norm='l1')\n",
							"\n",
							"X2 = preprocessing.normalize(X2, norm='l1')\n",
							"y2 = preprocessing.normalize(y2, norm='l1')\n",
							"\n",
							"# CreaciÃ³n del objeto KFold\n",
							"n_splits = 5  # NÃºmero de divisiones para K-Fold\n",
							"kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
							"\n",
							"scaler = StandardScaler()\n",
							"sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
							"#sel.fit_transform(X)\n",
							"\n",
							"# Lista para almacenar las mÃ©tricas de evaluaciÃ³n en cada fold\n",
							"mse_scores = []\n",
							"r2_scores = []\n",
							"mean_absolute_error_score = []\n",
							"\n",
							"# Ciclo de K-Fold cross-validation\n",
							"for train_index, test_index in kf.split(X):\n",
							"    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
							"    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
							"    \n",
							"    # Ajustar el scaler en el conjunto de entrenamiento y aplicarlo a ambos conjuntos\n",
							"    # X_train_scaled = scaler.fit_transform(X_train)\n",
							"    # X_test_scaled = scaler.transform(X_test)\n",
							"\n",
							"    X_train_scaled = sel.fit_transform(X_train) #scaleer.fit\n",
							"    X_test_scaled = sel.transform(X_test)\n",
							"\n",
							"    # CreaciÃ³n y entrenamiento del modelo de regresiÃ³n lineal para cada fold\n",
							"    #model = LinearRegression()\n",
							"    #model = LogisticRegression()\n",
							"    #model = Ridge() \n",
							"    #model = Lasso()\n",
							"    model = DecisionTreeRegressor()\n",
							"    #model = RandomForestRegressor()\n",
							"    #model = SVR()\n",
							"    #model = MLPRegressor()\n",
							"\n",
							"    #model.fit(X_train, y_train)\n",
							"    model.fit(X_train_scaled, y_train)\n",
							"    \n",
							"    # Predicciones en el conjunto de prueba\n",
							"    #y_pred = model.predict(X_test)\n",
							"    y_pred = model.predict(X_test_scaled)\n",
							"    \n",
							"    # EvaluaciÃ³n del modelo y almacenamiento de mÃ©tricas\n",
							"\n",
							"    mse_scores.append(mean_squared_error(y_test, y_pred))\n",
							"    r2_scores.append(r2_score(y_test, y_pred))\n",
							"    mean_absolute_error_score.append(mean_absolute_error(y_test, y_pred))\n",
							"\n",
							"# CÃ¡lculo de las mÃ©tricas promedio\n",
							"average_mse = sum(mse_scores) / n_splits\n",
							"average_r2 = sum(r2_scores) / n_splits\n",
							"average_mean_absolute_error = sum(mean_absolute_error_score) / n_splits\n",
							"\n",
							"\n",
							"print(f'Average Mean Squared Error Males: {average_mse}')\n",
							"print(f'Average R-squared Males: {average_r2}')\n",
							"print(f'Average R-squared Males: {average_mean_absolute_error}')\n",
							"\n",
							"mse_scores = []\n",
							"r2_scores = []\n",
							"mean_absolute_error_score = []\n",
							"\n",
							"for train_index, test_index in kf.split(X2):\n",
							"    X_train, X_test = X2.iloc[train_index], X2.iloc[test_index]\n",
							"    y_train, y_test = y2.iloc[train_index], y2.iloc[test_index]\n",
							"    \n",
							"    # Ajustar el scaler en el conjunto de entrenamiento y aplicarlo a ambos conjuntos\n",
							"    X_train_scaled = scaler.fit_transform(X_train)\n",
							"    X_test_scaled = scaler.transform(X_test)\n",
							"\n",
							"    # CreaciÃ³n y entrenamiento del modelo de regresiÃ³n lineal para cada fold\n",
							"    #model = LinearRegression()\n",
							"    #model = LogisticRegression()\n",
							"    #model = Ridge() \n",
							"    #model = Lasso()\n",
							"    model = DecisionTreeRegressor()\n",
							"    #model = RandomForestRegressor()\n",
							"    #model = SVR()\n",
							"    #model = MLPRegressor()\n",
							"\n",
							"    #model.fit(X_train, y_train)\n",
							"    model.fit(X_train_scaled, y_train)\n",
							"    \n",
							"    # Predicciones en el conjunto de prueba\n",
							"    #y_pred = model.predict(X_test)\n",
							"    y_pred = model.predict(X_test_scaled)\n",
							"    \n",
							"    # EvaluaciÃ³n del modelo y almacenamiento de mÃ©tricas\n",
							"    mse_scores.append(mean_squared_error(y_test, y_pred))\n",
							"    r2_scores.append(r2_score(y_test, y_pred))\n",
							"    mean_absolute_error_score.append(mean_absolute_error(y_test, y_pred))\n",
							"#aaaaaa\n",
							"# CÃ¡lculo de las mÃ©tricas promedio\n",
							"average_mse = sum(mse_scores) / n_splits\n",
							"average_r2 = sum(r2_scores) / n_splits\n",
							"average_mean_absolute_error = sum(mean_absolute_error_score) / n_splits\n",
							"\n",
							"\n",
							"print(f'Average Mean Squared Error Males: {average_mse}')\n",
							"print(f'Average R-squared Males: {average_r2}')\n",
							"print(f'Average Mean Absolute rror: {average_mean_absolute_error}')"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"from sklearn.feature_selection import SelectFromModel\n",
							"from sklearn.preprocessing import StandardScaler\n",
							"from sklearn.linear_model import LinearRegression\n",
							"from sklearn.ensemble import RandomForestRegressor\n",
							"from sklearn.svm import SVR\n",
							"from sklearn.datasets import make_regression\n",
							"from sklearn.model_selection import train_test_split\n",
							"import pandas as pd\n",
							"\n",
							"# Crear datos de ejemplo\n",
							"#X, y = make_regression(n_samples=100, n_features=20, random_state=42)\n",
							"\n",
							"data = pd.read_sql(query, connection)\n",
							"# ... (ConexiÃ³n a la base de datos y carga de datos como se muestra en el cÃ³digo anterior)\n",
							"X = data.drop('incidencia', axis=1)\n",
							"y = data['incidencia']\n",
							"\n",
							"\n",
							"# Dividir datos en entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
							"\n",
							"# Crear un modelo para evaluar la importancia de las caracterÃ­sticas\n",
							"model = RandomForestRegressor()  # Puedes usar otros modelos como LinearRegression, SVR, etc.\n",
							"\n",
							"# Crear un objeto SelectFromModel para realizar la selecciÃ³n\n",
							"selector = SelectFromModel(model)\n",
							"\n",
							"# Aplicar el selector en el conjunto de entrenamiento\n",
							"X_train_selected = selector.fit_transform(X_train, y_train)\n",
							"\n",
							"# Obtener las caracterÃ­sticas seleccionadas\n",
							"selected_features = X_train.columns[selector.get_support()]\n",
							"\n",
							"# Imprimir las caracterÃ­sticas seleccionadas\n",
							"print(\"CaracterÃ­sticas seleccionadas:\")\n",
							"print(selected_features)"
						],
						"outputs": [],
						"execution_count": 50
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kaggle1_entrenamiento_pablo_garcia')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5678b2ad-58ca-4be7-a8a9-bfd4fb3d7ca3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Primera entrega Kaggle\n",
							"\n",
							"**Pablo GarcÃ­a Caballero**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Importamos librerias"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"import numpy as np\n",
							"import pickle\n",
							"import re\n",
							"\n",
							"# TransformaciÃ³n de datos\n",
							"from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder #, TargetEncoder\n",
							"#import category_encoders as ce\n",
							"from category_encoders import TargetEncoder\n",
							"\n",
							"from sklearn.impute import SimpleImputer\n",
							"\n",
							"# Modelos\n",
							"from sklearn.tree import DecisionTreeClassifier\n",
							"import lightgbm as lgb\n",
							"\n",
							"# Seleccion de variables y tuning de hiperparÃ¡metros\n",
							"from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
							"\n",
							"# MÃ©tricas para evaluar un modelo de clasificaciÃ³n\n",
							"from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve, auc, roc_curve, roc_auc_score, average_precision_score, confusion_matrix\n",
							"\n",
							"# LibrerÃ­as para visualizaciÃ³n de resultados\n",
							"import matplotlib.pyplot as plt\n",
							"import seaborn as sns\n",
							"\n",
							"pd.set_option('display.max_columns', None)\n",
							"pd.set_option('display.max_rows', None)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"Definimos la funciÃ³n disponible en el campus para clasificar las columnas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def guess_type(column_name:str , df:pd.DataFrame, m=None, original_df=None) -> str:\n",
							"\n",
							"    if df[column_name].dtype == 'O':\n",
							"        return 'text'\n",
							"    try:\n",
							"        if m is not None and bool(m.get_all(name=column_name)):\n",
							"            return m.get_all(name=column_name).type[0]\n",
							"        elif m is not None and bool(m.get_all(name=column_name[:-2])) and original_df is not None:\n",
							"            #lets check if this is a date:\n",
							"            original_colname = column_name[:-2]\n",
							"            atype = original_df[original_colname].dtype\n",
							"            typ = str(atype)\n",
							"            if re.search(\"^date.\", typ):\n",
							"                return 'categorical'\n",
							"            if atype == 'O':\n",
							"                row_sample = original_df[original_colname].iloc[0]\n",
							"                if is_date(row_sample):\n",
							"                    return 'categorical'\n",
							"\n",
							"        #if isinstance(row_sample, float):\n",
							"        #    return 'numerical'\n",
							"        if df[column_name].dtype in ['float64', 'float32', 'float16']:\n",
							"            # we have to check if column was wrong catalogued as numerical\n",
							"            grouped = df[column_name].unique()\n",
							"            if len(grouped) < 10:\n",
							"                #check there are no decimals\n",
							"                for v in grouped:\n",
							"                    are_decimals = v - int(v)\n",
							"                    if are_decimals > 0.0:\n",
							"                        return 'numerical'\n",
							"                return 'categorical'\n",
							"            return 'numerical'\n",
							"\n",
							"        elif df[column_name].dtype in ['int64', 'int32', 'int16', 'int8']:\n",
							"            max = df[column_name].max()\n",
							"            min = df[column_name].min()\n",
							"            diff = max - min\n",
							"            diff2 = int(max) - int(min)\n",
							"            if diff == diff2:\n",
							"                if diff < 10:\n",
							"                    grouped = df[column_name].unique()\n",
							"                    if len(grouped)< 10:\n",
							"                        return 'categorical'\n",
							"            return \"numerical\"\n",
							"        else:\n",
							"            return \"categorical\"\n",
							"    except Exception as e:\n",
							"        return 'categorical'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"Carga de datos"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain = pd.read_csv(\"MasterBI_Train.csv\", low_memory=False)\n",
							"label = \"HasDetections\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain.head()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"**EDA**. En primer lugar, realizamos un analisis exploratorio del conjunto de datos. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain.info()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"Agrupamos las columnas que sean identificadores para posteriormermente elimarlas del df, ya que no son utiles para el entramiento"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"id_colums = [\"MachineIdentifier\"]#, \"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]\n",
							"dfTrain.drop(columns=id_colums, inplace=True)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"Convertimos a categoricas los campos que correspondan en funciÃ³n de la funciÃ³n guess_type"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"for i in dfTrain.columns:\n",
							"    print(f\"{i} = {guess_type(i, dfTrain)}\")\n",
							"    if guess_type(i, dfTrain) == \"categorical\":\n",
							"        dfTrain[i] = dfTrain[i].astype('category')\n",
							"        print(\"convertido\")\n",
							"    else:\n",
							"        print(\"no\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"columnas_cat = dfTrain.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
							"columnas_num = dfTrain.select_dtypes(include=[\"float64\", \"int64\"]).columns.to_list()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"print(columnas_cat)\n",
							"print(columnas_num)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"Eliminamos el label de nuestro conjunto de categÃ³ricas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"if label in columnas_cat:\n",
							"    columnas_cat.remove(label)\n",
							"    print(\"Label borrado de las cat\")\n",
							"else:\n",
							"    columnas_num.remove(label)\n",
							"    print(\"Label borrado de las num\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"Imputamos los nulos mediante la media para las numericas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# esto esta mal , primero dividir\n",
							"\n",
							"imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
							"#dfTrain[columnas_num] = imp_num.fit_transform(dfTrain[columnas_num])"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"Imputamos los nulos mediante la moda para las categoricas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"imp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
							"#dfTrain[columnas_cat] = imp_cat.fit_transform(dfTrain[columnas_cat])"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"Preparamos la codificaciÃ³n de variables"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"cat_num = columnas_cat+columnas_num \n",
							"dfTrain = dfTrain[cat_num+['HasDetections']]"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"te = TargetEncoder()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Modelo**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Dividimos el df en train y test"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# X = dfTrain.drop('HasDetections', axis=1)\n",
							"# y = dfTrain['HasDetections']\n",
							"# X = dfTrain.drop(label, axis=1)\n",
							"# y = dfTrain[label]\n",
							"X = dfTrain.drop(label, axis=1)\n",
							"y = dfTrain[label]\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"X_train[columnas_num] = imp_num.fit_transform(X_train[columnas_num])\n",
							"X_train[columnas_cat] = imp_cat.fit_transform(X_train[columnas_cat])\n",
							"#X_train.isnull().sum()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"X_test[columnas_num] = imp_num.transform(X_test[columnas_num])\n",
							"X_test[columnas_cat] = imp_cat.transform(X_test[columnas_cat])\n",
							"#X_test.isnull().sum()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"Aplicamos el encoder"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(columnas_cat)\n",
							"print(columnas_num)"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"source": [
							"dfX_train_prep = pd.DataFrame(X_train , columns=columnas)\n",
							"dfX_test_prep = pd.DataFrame(X_test , columns=columnas)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"X_train_prep = te.fit_transform(X_train[columnas_cat], y_train)\n",
							"X_test_prep = te.transform(X_test[columnas_cat])\n",
							"#X_train.isnull().sum()"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"X_test[columnas_num] = imp_num.transform(X_test[columnas_num])\n",
							"X_test[columnas_cat] = imp_cat.transform(X_test[columnas_cat])\n",
							"#X_test.isnull().sum()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"X_train_prep = te.fit_transform(X_train,y_train)\n",
							"\n",
							"#X_test_prep = te.transform(X_test)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"te.fit_transform(X_train, y_train)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"X_train_prep = te.transform(X_train)\n",
							"X_test_prep = te.transform(X_test)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# X_train_prep = te.fit_transform(X_train, y_train)\n",
							"# X_test_prep = te.fit_transform(X_test, y_test)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Convertimos a df de nuevo"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"columnas = dfTrain.columns.to_list()\n",
							"columnas.remove(label)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"dfX_train_prep = pd.DataFrame(X_train_prep , columns=columnas)\n",
							"dfX_test_prep = pd.DataFrame(X_test_prep , columns=columnas)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Entrenamos el modelo"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"model = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
							"model.fit(dfX_train_prep, y_train)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"**PrediciÃ³n y evaluaciÃ³n**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"prediciones = model.predict(X = dfX_train_prep, ) # , )\n",
							"pred_proba = model.predict_proba( X = dfX_train_prep)\n",
							"\n",
							"data = confusion_matrix(y_train, prediciones)\n",
							"\n",
							"print(\"Matriz de confusiÃ³n\")\n",
							"plt.figure(figsize=(6,5))\n",
							"sns.set(font_scale=1.4)\n",
							"sns.heatmap(data, cmap=\"Blues\", annot=True, annot_kws={\"size\":12})"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"print(f\"Acurracy: {round(100*accuracy_score(y_train, prediciones),1)}% \\n\")\n",
							"print(classification_report(y_train, prediciones, digits=3, zero_division=True))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Curva ROC y AUX"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"fpr, tpr, _ = roc_curve(y_test, pred_proba[:,1])\n",
							"roc_auc = auc(fpr, tpr)\n",
							"\n",
							"plt.figure(figsize = (6,5))\n",
							"lw = 2\n",
							"plt.plot(fpr, tpr, color='darkorange',\n",
							"         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
							"plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
							"plt.xlim([0.0, 1.0])\n",
							"plt.ylim([0.0, 1.05])\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.title('Receiver operating characteristic example')\n",
							"plt.legend(loc=\"lower right\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Pase a producciÃ³n**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Replicamos los pasos anteriores, pero sin dividir el df en test/train"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrainFull = pd.read_csv(\"MasterBI_Train.csv\", low_memory=False)\n",
							"label = \"HasDetections\""
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "code",
						"source": [
							"id_colums = [\"MachineIdentifier\"]#, \"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]\n",
							"dfTrainFull.drop(columns=id_colums, inplace=True)"
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"source": [
							"for i in dfTrainFull.columns:\n",
							"    print(f\"{i} = {guess_type(i, dfTrainFull)}\")\n",
							"    if guess_type(i, dfTrainFull) == \"categorical\":\n",
							"        dfTrainFull[i] = dfTrainFull[i].astype('category')\n",
							"        print(\"convertido\")\n",
							"    else:\n",
							"        print(\"no\")"
						],
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "code",
						"source": [
							"columnas_cat = dfTrainFull.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
							"columnas_num = dfTrainFull.select_dtypes(include=[\"float64\", \"int64\"]).columns.to_list()"
						],
						"outputs": [],
						"execution_count": 100
					},
					{
						"cell_type": "code",
						"source": [
							"columnas_cat.remove(label)"
						],
						"outputs": [],
						"execution_count": 101
					},
					{
						"cell_type": "code",
						"source": [
							"imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
							"dfTrainFull[columnas_num] = imp_num.fit_transform(dfTrainFull[columnas_num])"
						],
						"outputs": [],
						"execution_count": 102
					},
					{
						"cell_type": "code",
						"source": [
							"imp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
							"dfTrainFull[columnas_cat] = imp_cat.fit_transform(dfTrainFull[columnas_cat])"
						],
						"outputs": [],
						"execution_count": 103
					},
					{
						"cell_type": "code",
						"source": [
							"cat_num = columnas_cat+columnas_num \n",
							"dfTrainFull = dfTrainFull[cat_num+['HasDetections']]"
						],
						"outputs": [],
						"execution_count": 104
					},
					{
						"cell_type": "code",
						"source": [
							"te = TargetEncoder()"
						],
						"outputs": [],
						"execution_count": 105
					},
					{
						"cell_type": "code",
						"source": [
							"X = dfTrainFull.drop(label, axis=1)\n",
							"y = dfTrainFull[label]"
						],
						"outputs": [],
						"execution_count": 106
					},
					{
						"cell_type": "code",
						"source": [
							"X_train_prep = te.fit_transform(X, y)"
						],
						"outputs": [],
						"execution_count": 107
					},
					{
						"cell_type": "code",
						"source": [
							"columnas = dfTrain.columns.to_list()\n",
							"columnas.remove(label)"
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "code",
						"source": [
							"dfX_train_prep = pd.DataFrame(X_train_prep , columns=columnas)"
						],
						"outputs": [],
						"execution_count": 110
					},
					{
						"cell_type": "code",
						"source": [
							"model = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
							"model.fit(dfX_train_prep, y)"
						],
						"outputs": [],
						"execution_count": 111
					},
					{
						"cell_type": "markdown",
						"source": [
							"Por Ãºltimo, utilizando las funcionalidades de la librerÃ­a pickle, guardamos todo aquello ques nos va a hacer falta para la inferencia con los datos de test"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Modelo\n",
							"with open(\"output/modelo.pkl\", \"wb\") as c:\n",
							"    pickle.dump(model, c)\n",
							"\n",
							"# TransformaciÃ³n\n",
							"with open(\"output/num_imputer.pkl\", \"wb\") as c:\n",
							"    pickle.dump(imp_num, c)\n",
							"with open(\"output/cat_imputer.pkl\", \"wb\") as c:\n",
							"    pickle.dump(imp_cat, c)\n",
							"with open(\"output/encoder.pkl\", \"wb\") as c:\n",
							"    pickle.dump(te, c)\n",
							"\n",
							"# Lista de todas columnas\n",
							"todas_columnas = dfTrainFull.columns.to_list()\n",
							"with open(\"output/columns.pkl\", \"wb\") as c:\n",
							"    pickle.dump(todas_columnas, c)\n",
							"\n",
							"# Columnas Cat\n",
							"with open(\"output/cat_columns.pkl\", \"wb\") as c:\n",
							"     pickle.dump(columnas_cat, c)\n",
							"\n",
							"# Columnas Num\n",
							"with open(\"output/num_columns.pkl\", \"wb\") as c:\n",
							"     pickle.dump(columnas_num, c)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kaggle1_entrenamiento_pablo_garcia_final')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9131a7fa-a006-40d0-b834-5d5e20e76a5b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Primera entrega Kaggle\n",
							"\n",
							"**Pablo GarcÃ­a Caballero**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Importamos librerias"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"import numpy as np\n",
							"import pickle\n",
							"import re\n",
							"\n",
							"# TransformaciÃ³n de datos\n",
							"from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder #, TargetEncoder\n",
							"#import category_encoders as ce\n",
							"from category_encoders import TargetEncoder\n",
							"\n",
							"from sklearn.impute import SimpleImputer\n",
							"from sklearn.compose import ColumnTransformer\n",
							"\n",
							"# Modelos\n",
							"from sklearn.tree import DecisionTreeClassifier\n",
							"import lightgbm as lgb\n",
							"\n",
							"# Seleccion de variables y tuning de hiperparÃ¡metros\n",
							"from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
							"\n",
							"# MÃ©tricas para evaluar un modelo de clasificaciÃ³n\n",
							"from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve, auc, roc_curve, roc_auc_score, average_precision_score, confusion_matrix\n",
							"\n",
							"# LibrerÃ­as para visualizaciÃ³n de resultados\n",
							"import matplotlib.pyplot as plt\n",
							"import seaborn as sns\n",
							"\n",
							"pd.set_option('display.max_columns', None)\n",
							"pd.set_option('display.max_rows', None)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"Definimos la funciÃ³n disponible en el campus para clasificar las columnas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def guess_type(column_name:str , df:pd.DataFrame, m=None, original_df=None) -> str:\n",
							"\n",
							"    if df[column_name].dtype == 'O':\n",
							"        return 'text'\n",
							"    try:\n",
							"        if m is not None and bool(m.get_all(name=column_name)):\n",
							"            return m.get_all(name=column_name).type[0]\n",
							"        elif m is not None and bool(m.get_all(name=column_name[:-2])) and original_df is not None:\n",
							"            #lets check if this is a date:\n",
							"            original_colname = column_name[:-2]\n",
							"            atype = original_df[original_colname].dtype\n",
							"            typ = str(atype)\n",
							"            if re.search(\"^date.\", typ):\n",
							"                return 'categorical'\n",
							"            if atype == 'O':\n",
							"                row_sample = original_df[original_colname].iloc[0]\n",
							"                if is_date(row_sample):\n",
							"                    return 'categorical'\n",
							"\n",
							"        #if isinstance(row_sample, float):\n",
							"        #    return 'numerical'\n",
							"        if df[column_name].dtype in ['float64', 'float32', 'float16']:\n",
							"            # we have to check if column was wrong catalogued as numerical\n",
							"            grouped = df[column_name].unique()\n",
							"            if len(grouped) < 10:\n",
							"                #check there are no decimals\n",
							"                for v in grouped:\n",
							"                    are_decimals = v - int(v)\n",
							"                    if are_decimals > 0.0:\n",
							"                        return 'numerical'\n",
							"                return 'categorical'\n",
							"            return 'numerical'\n",
							"\n",
							"        elif df[column_name].dtype in ['int64', 'int32', 'int16', 'int8']:\n",
							"            max = df[column_name].max()\n",
							"            min = df[column_name].min()\n",
							"            diff = max - min\n",
							"            diff2 = int(max) - int(min)\n",
							"            if diff == diff2:\n",
							"                if diff < 10:\n",
							"                    grouped = df[column_name].unique()\n",
							"                    if len(grouped)< 10:\n",
							"                        return 'categorical'\n",
							"            return \"numerical\"\n",
							"        else:\n",
							"            return \"categorical\"\n",
							"    except Exception as e:\n",
							"        return 'categorical'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"Carga de datos"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#dfTrain = pd.read_csv(\"MasterBI_Train.csv\", low_memory=False)\n",
							"dfTrain = pd.read_csv(\"D:\\Master BI\\ML\\MasterBI_Train.csv\", low_memory=False)\n",
							"label = \"HasDetections\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain.head()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"**EDA**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"SelecciÃ³n de variables. Identificamos variables que son identificadores y variables que dada su informaciÃ³n pueden ir juntas."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# id_colums = [\"MachineIdentifier\"]#, \"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]\n",
							"# dfTrain.drop(columns=id_colums, inplace=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain[[\"Census_OSEdition\", \"Census_OSSkuName\"]].drop_duplicates().head(20) #.sort_values(by=\"Census_OSEdition\")\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain[[\"Platform\", \"Processor\"]].drop_duplicates().sort_values(by=\"Platform\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain[[\"ProductName\", \"EngineVersion\", \"IsBeta\"]].drop_duplicates().sort_values(by=\"EngineVersion\").head(20)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"Como tienen relaciÃ³n vamos a unifircarlas, y despues a dropearlas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain[\"ProductNameIsBetaEngineVersion\"] = dfTrain[\"ProductName\"] + '_' + dfTrain[\"IsBeta\"].astype(str) + '_' + dfTrain[\"EngineVersion\"]\n",
							"\n",
							"dfTrain[\"PlatformProcessor\"] = dfTrain[\"Platform\"] + '_' + dfTrain[\"Processor\"] \n",
							"\n",
							"# Eliminar las columnas originales \n",
							"#dfTrain.drop([\"ProductName\", \"EngineVersion\", \"IsBeta\", \"Platform\", \"Processor\", \"Census_OSSkuName\"], axis=1, inplace=True)\n",
							"dfTrain.drop([\"MachineIdentifier\", \"ProductName\", \"EngineVersion\", \"IsBeta\", \"Platform\", \"Processor\"], axis=1, inplace=True)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain.info()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"Agrupamos las columnas que sean identificadores para posteriormermente elimarlas del df, ya que no son utiles para el entramiento"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# id_colums = [\"MachineIdentifier\"]#, \"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]\n",
							"# dfTrain.drop(columns=id_colums, inplace=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Convertimos a categoricas los campos que correspondan en funciÃ³n de la funciÃ³n guess_type"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"for i in dfTrain.columns:\n",
							"    print(f\"{i} = {guess_type(i, dfTrain)}\")\n",
							"    if guess_type(i, dfTrain) == \"categorical\":\n",
							"        dfTrain[i] = dfTrain[i].astype('category')\n",
							"        print(\"convertido\")\n",
							"    else:\n",
							"        print(\"no\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"ImputaciÃ³n de nulos mediante la media para las numericas y mediante la moda para las categoricas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"columnas_cat = dfTrain.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
							"columnas_num = dfTrain.select_dtypes(include=[\"float64\", \"int64\"]).columns.to_list()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"Numericas:\", columnas_cat)\n",
							"print(\"CategorÃ­cas:\", columnas_num)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"Eliminamos el label de nuestro conjunto de categÃ³ricas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"if label in columnas_cat:\n",
							"    columnas_cat.remove(label)\n",
							"    print(\"Label borrado de las cat\")\n",
							"else:\n",
							"    columnas_num.remove(label)\n",
							"    print(\"Label borrado de las num\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
							"imp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"Preparamos la codificaciÃ³n de variables"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"columnas_todas = columnas_cat+columnas_num \n",
							"dfTrain = dfTrain[columnas_todas+['HasDetections']]"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Modelo**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Dividimos el df en train y test"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
							"from sklearn.pipeline import Pipeline\n",
							"from sklearn.compose import ColumnTransformer\n",
							"from sklearn.impute import SimpleImputer\n",
							"from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
							"from lightgbm import LGBMClassifier\n",
							"from sklearn.metrics import accuracy_score, classification_report\n",
							"from scipy.stats import randint, uniform\n",
							"\n",
							"# Dividir el conjunto de datos\n",
							"X = dfTrain.drop(label, axis=1)\n",
							"y = dfTrain[label]\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
							"\n",
							"# Separar columnas numÃ©ricas y categÃ³ricas\n",
							"numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
							"categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
							"\n",
							"# Preprocesamiento para variables numÃ©ricas\n",
							"numeric_transformer = Pipeline(steps=[\n",
							"    ('imputer', SimpleImputer(strategy='mean')),  # Imputar con la media\n",
							"    ('scaler', StandardScaler())                 # Estandarizar\n",
							"])\n",
							"\n",
							"# Preprocesamiento para variables categÃ³ricas\n",
							"categorical_transformer = Pipeline(steps=[\n",
							"    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputar con el valor mÃ¡s frecuente\n",
							"    ('encoder', OneHotEncoder(handle_unknown='ignore'))    # Codificar variables categÃ³ricas\n",
							"])\n",
							"\n",
							"# CombinaciÃ³n de transformaciones\n",
							"preprocessor = ColumnTransformer(\n",
							"    transformers=[\n",
							"        ('num', numeric_transformer, numeric_features),\n",
							"        ('cat', categorical_transformer, categorical_features)\n",
							"    ])\n",
							"\n",
							"# Crear el pipeline completo\n",
							"pipeline = Pipeline(steps=[\n",
							"    ('preprocessor', preprocessor),               # Paso de preprocesamiento\n",
							"    ('classifier', LGBMClassifier(random_state=42))  # Modelo\n",
							"])\n",
							"\n",
							"# Espacio de bÃºsqueda de hiperparÃ¡metros\n",
							"param_distributions = {\n",
							"    'classifier__n_estimators': randint(50, 500),\n",
							"    'classifier__learning_rate': uniform(0.01, 0.3),\n",
							"    'classifier__max_depth': randint(3, 15),\n",
							"    'classifier__num_leaves': randint(20, 150),\n",
							"    'classifier__colsample_bytree': uniform(0.5, 0.5),\n",
							"    'classifier__subsample': uniform(0.5, 0.5),\n",
							"    'classifier__lambda_l1': uniform(0.0, 1.0),\n",
							"    'classifier__lambda_l2': uniform(0.0, 1.0),\n",
							"    'classifier__min_child_samples': randint(10, 100)\n",
							"}\n",
							"\n",
							"# RandomizedSearchCV\n",
							"random_search = RandomizedSearchCV(\n",
							"    pipeline,\n",
							"    param_distributions=param_distributions,\n",
							"    n_iter=50,              # NÃºmero de iteraciones\n",
							"    scoring='accuracy',     # MÃ©trica de evaluaciÃ³n\n",
							"    cv=5,                   # NÃºmero de particiones para validaciÃ³n cruzada\n",
							"    verbose=2,\n",
							"    random_state=42,\n",
							"    n_jobs=-1               # ParalelizaciÃ³n\n",
							")\n",
							"\n",
							"# Entrenar RandomizedSearchCV\n",
							"random_search.fit(X_train, y_train)\n",
							"\n",
							"# Mejor modelo\n",
							"best_model = random_search.best_estimator_\n",
							"print(\"Mejores parÃ¡metros:\", random_search.best_params_)\n",
							"\n",
							"# Evaluar el mejor modelo\n",
							"y_pred = best_model.predict(X_test)\n",
							"print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
							"print(classification_report(y_test, y_pred))\n",
							""
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"source": [
							"X = dfTrain.drop(label, axis=1)\n",
							"y = dfTrain[label]\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) "
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "markdown",
						"source": [
							"ImputaciÃ³n de nulos"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"X_train[columnas_num] = imp_num.fit_transform(X_train[columnas_num])\n",
							"X_train[columnas_cat] = imp_cat.fit_transform(X_train[columnas_cat])\n",
							"#X_train.isnull().sum()"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"source": [
							"X_test[columnas_num] = imp_num.transform(X_test[columnas_num])\n",
							"X_test[columnas_cat] = imp_cat.transform(X_test[columnas_cat])\n",
							"#X_test.isnull().sum()"
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "markdown",
						"source": [
							"Aplicamos el encoder"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Fit en el conjunto de entrenamiento\n",
							"X_train[columnas_cat] = encoder.fit_transform(X_train[columnas_cat]) #.astype(str))\n",
							"\n",
							"# Transformar el conjunto de prueba\n",
							"X_test[columnas_cat] = encoder.transform(X_test[columnas_cat]) #.astype(str))\n",
							""
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"source": [
							"from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
							"\n",
							"scaler = StandardScaler()\n",
							"X_train_scaled_num = scaler.fit_transform(X_train[columnas_num])\n",
							"X_test_scaled_num = scaler.transform(X_test[columnas_num])\n",
							"\n",
							"# Convertir resultados escalados a DataFrame\n",
							"X_train_scaled_num = pd.DataFrame(X_train_scaled_num, columns=columnas_num, index=X_train.index)\n",
							"X_test_scaled_num = pd.DataFrame(X_test_scaled_num, columns=columnas_num, index=X_test.index)\n",
							"\n",
							"\n",
							"# Combinar los resultados\n",
							"X_train_final = pd.concat([X_train_scaled_num, X_train[columnas_cat]], axis=1)\n",
							"X_test_final = pd.concat([X_test_scaled_num, X_test[columnas_cat]], axis=1)\n",
							""
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"source": [
							"X_train_final.head()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"source": [
							"Convertimos a df de nuevo"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Entrenamos el modelo"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from sklearn.svm import SVC\n",
							"clf = SVC(gamma='auto')\n",
							"clf.fit(X_train_final, y_train)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from sklearn.ensemble import RandomForestRegressor\n",
							"clf = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
							"#clf = RandomForestRegressor()\n",
							"clf.fit(X_train_final, y_train)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from sklearn.ensemble import RandomForestClassifier\n",
							"\n",
							"from sklearn.feature_selection import SelectFromModel\n",
							"from sklearn.ensemble import RandomForestClassifier\n",
							"\n",
							"# SelectFromModel with Random Forest Classifier\n",
							"#selector = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
							"#X_selected = selector.fit_transform(X, y)\n",
							"\n",
							"#clf = RandomForestClassifier()\n",
							"selector = SelectFromModel(RandomForestClassifier(\n",
							"    n_estimators=100,           # MÃ¡s Ã¡rboles para mayor estabilidad\n",
							"    max_depth=20,               # Limitar la profundidad para evitar sobreajuste\n",
							"    min_samples_split=10,       # Evita divisiones excesivas\n",
							"    min_samples_leaf=5,         # Hoja con mÃ­nimo 5 datos\n",
							"    max_features='sqrt',        # Variables consideradas por divisiÃ³n\n",
							"    max_samples=0.8,            # Muestreo aleatorio del 80% de los datos\n",
							"    class_weight='balanced',    # Balanceo de clases\n",
							"    criterion='entropy',        # Medida de calidad de divisiÃ³n\n",
							"    bootstrap=True,             # Muestreo con reemplazo\n",
							"    random_state=42,            # Reproducibilidad\n",
							"    n_jobs=-1                   # ParalelizaciÃ³n\n",
							"))\n",
							"selector.fit(X_train_final.values, y_train.values)\n",
							"\n",
							"X_train_final = selector.transform(X_train_final)\n",
							"X_test_final = selector.transform(X_test_final)\n",
							"\n",
							"clf = RandomForestClassifier(\n",
							"    n_estimators=500,           # MÃ¡s Ã¡rboles para mayor estabilidad\n",
							"    max_depth=20,               # Limitar la profundidad para evitar sobreajuste\n",
							"    min_samples_split=10,       # Evita divisiones excesivas\n",
							"    min_samples_leaf=5,         # Hoja con mÃ­nimo 5 datos\n",
							"    max_features='sqrt',        # Variables consideradas por divisiÃ³n\n",
							"    max_samples=0.8,            # Muestreo aleatorio del 80% de los datos\n",
							"    class_weight='balanced',    # Balanceo de clases\n",
							"    criterion='entropy',        # Medida de calidad de divisiÃ³n\n",
							"    bootstrap=True,             # Muestreo con reemplazo\n",
							"    random_state=42,            # Reproducibilidad\n",
							"    n_jobs=-1                   # ParalelizaciÃ³n\n",
							")\n",
							"clf.fit(X_train_final, y_train.values)"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"source": [
							"from sklearn.model_selection import RandomizedSearchCV, KFold\n",
							"from scipy.stats import uniform\n",
							"import lightgbm as lgb\n",
							"\n",
							"# Define los hiperparÃ¡metros y sus rangos\n",
							"param_dist = {\n",
							"    'n_estimators': [100, 200, 500, 1000],         # NÃºmero de Ã¡rboles\n",
							"    'max_depth': [6, 8, 10, 12],                  # Profundidad mÃ¡xima\n",
							"    'num_leaves': [20, 50, 80, 100],              # NÃºmero de hojas\n",
							"    'learning_rate': uniform(0.01, 0.2),          # Tasa de aprendizaje\n",
							"    'colsample_bytree': uniform(0.6, 0.4),        # ProporciÃ³n de columnas por Ã¡rbol\n",
							"    'subsample': uniform(0.6, 0.4),               # ProporciÃ³n de filas\n",
							"    'lambda_l1': uniform(0.0, 1.0),               # RegularizaciÃ³n L1\n",
							"    'lambda_l2': uniform(0.0, 2.0),               # RegularizaciÃ³n L2\n",
							"    'min_child_samples': [10, 20, 30, 50]         # TamaÃ±o mÃ­nimo de datos en hojas\n",
							"}\n",
							"\n",
							"# Configura el modelo base\n",
							"clf = lgb.LGBMClassifier(random_state=42)\n",
							"\n",
							"# Configura la validaciÃ³n cruzada (KFold)\n",
							"kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
							"\n",
							"# Configura la bÃºsqueda aleatoria\n",
							"random_search = RandomizedSearchCV(\n",
							"    estimator=clf,\n",
							"    param_distributions=param_dist,\n",
							"    n_iter=50,                    # NÃºmero de combinaciones aleatorias a probar\n",
							"    scoring='accuracy',           # MÃ©trica de evaluaciÃ³n\n",
							"    cv=kfold,                     # ValidaciÃ³n cruzada\n",
							"    verbose=2,                    # Mensajes de progreso\n",
							"    random_state=42,\n",
							"    n_jobs=-1                     # Usa todos los nÃºcleos disponibles\n",
							")\n",
							"\n",
							"# Realiza la bÃºsqueda\n",
							"random_search.fit(X, y)\n",
							"\n",
							"# Imprime los mejores hiperparÃ¡metros\n",
							"print(\"Mejores hiperparÃ¡metros encontrados:\", random_search.best_params_)\n",
							"print(\"Mejor precisiÃ³n obtenida:\", random_search.best_score_)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# import lightgbm as lgb\n",
							"# #clf = lgb.LGBMClassifier(n_estimators=250, max_depth=7,num_leaves=80,learning_rate=0.02,\n",
							"# #colsample_bytree=0.8, ubsample=0.9,lambda_l1=0.2,lambda_l2=2.0,min_child_samples=40,min_split_gain=0.05)\n",
							"# param = {'classifier__colsample_bytree': 0.5066324805799333, 'classifier__lambda_l1': 0.9422017556848528, 'classifier__lambda_l2': 0.5632882178455393, 'classifier__learning_rate': 0.12562495076197483, 'classifier__max_depth': 12, 'classifier__min_child_samples': 62, 'classifier__n_estimators': 435, 'classifier__num_leaves': 103, 'classifier__subsample': 0.6205127330130058}\n",
							"# clf = lgb.LGBMClassifier(**param)\n",
							"\n",
							"# clf.fit(X_train_final, y_train)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"**PrediciÃ³n y evaluaciÃ³n**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from sklearn.metrics import confusion_matrix\n",
							"import seaborn as sns\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"# Predicciones del modelo\n",
							"prediciones_2 = clf.predict(X_test_final)\n",
							"pred_proba_2 = clf.predict_proba(X_test_final)\n",
							"\n",
							"# Crear la matriz de confusiÃ³n\n",
							"data = confusion_matrix(y_test.values, prediciones_2)\n",
							"\n",
							"# Visualizar la matriz de confusiÃ³n\n",
							"print(\"Matriz de confusiÃ³n\")\n",
							"plt.figure(figsize=(6, 5))\n",
							"sns.set(font_scale=1.4)  # Escala del texto\n",
							"sns.heatmap(data, cmap=\"Blues\", annot=True, fmt='g', annot_kws={\"size\":12})  # fmt='g' para nÃºmeros enteros\n",
							"plt.xlabel(\"Predicciones\", fontsize=14)\n",
							"plt.ylabel(\"Valores Reales\", fontsize=14)\n",
							"plt.title(\"Matriz de ConfusiÃ³n\", fontsize=16)\n",
							"plt.show()\n",
							"\n",
							"print(f\"Acurracy: {round(100*accuracy_score(y_test, prediciones_2),1)}% \\n\")\n",
							"print(classification_report(y_test, prediciones_2, digits=3, zero_division=True))"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "markdown",
						"source": [
							"Curva ROC y AUX"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"fpr, tpr, _ = roc_curve(y_test, pred_proba_2[:,1])\n",
							"roc_auc = auc(fpr, tpr)\n",
							"\n",
							"plt.figure(figsize = (6,5))\n",
							"lw = 2\n",
							"plt.plot(fpr, tpr, color='darkorange',\n",
							"         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
							"plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
							"plt.xlim([0.0, 1.0])\n",
							"plt.ylim([0.0, 1.05])\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.title('Receiver operating characteristic example')\n",
							"plt.legend(loc=\"lower right\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Pase a producciÃ³n**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Replicamos los pasos anteriores, pero sin dividir el df en test/train"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrainFull = pd.read_csv(\"MasterBI_Train.csv\", low_memory=False)\n",
							"label = \"HasDetections\""
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "code",
						"source": [
							"id_colums = [\"MachineIdentifier\"]#, \"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]\n",
							"dfTrainFull.drop(columns=id_colums, inplace=True)"
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"source": [
							"for i in dfTrainFull.columns:\n",
							"    print(f\"{i} = {guess_type(i, dfTrainFull)}\")\n",
							"    if guess_type(i, dfTrainFull) == \"categorical\":\n",
							"        dfTrainFull[i] = dfTrainFull[i].astype('category')\n",
							"        print(\"convertido\")\n",
							"    else:\n",
							"        print(\"no\")"
						],
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "code",
						"source": [
							"columnas_cat = dfTrainFull.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
							"columnas_num = dfTrainFull.select_dtypes(include=[\"float64\", \"int64\"]).columns.to_list()"
						],
						"outputs": [],
						"execution_count": 100
					},
					{
						"cell_type": "code",
						"source": [
							"columnas_cat.remove(label)"
						],
						"outputs": [],
						"execution_count": 101
					},
					{
						"cell_type": "code",
						"source": [
							"imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
							"dfTrainFull[columnas_num] = imp_num.fit_transform(dfTrainFull[columnas_num])"
						],
						"outputs": [],
						"execution_count": 102
					},
					{
						"cell_type": "code",
						"source": [
							"imp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
							"dfTrainFull[columnas_cat] = imp_cat.fit_transform(dfTrainFull[columnas_cat])"
						],
						"outputs": [],
						"execution_count": 103
					},
					{
						"cell_type": "code",
						"source": [
							"cat_num = columnas_cat+columnas_num \n",
							"dfTrainFull = dfTrainFull[cat_num+['HasDetections']]"
						],
						"outputs": [],
						"execution_count": 104
					},
					{
						"cell_type": "code",
						"source": [
							"te = TargetEncoder()"
						],
						"outputs": [],
						"execution_count": 105
					},
					{
						"cell_type": "code",
						"source": [
							"X = dfTrainFull.drop(label, axis=1)\n",
							"y = dfTrainFull[label]"
						],
						"outputs": [],
						"execution_count": 106
					},
					{
						"cell_type": "code",
						"source": [
							"X_train_prep = te.fit_transform(X, y)"
						],
						"outputs": [],
						"execution_count": 107
					},
					{
						"cell_type": "code",
						"source": [
							"columnas = dfTrain.columns.to_list()\n",
							"columnas.remove(label)"
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "code",
						"source": [
							"dfX_train_prep = pd.DataFrame(X_train_prep , columns=columnas)"
						],
						"outputs": [],
						"execution_count": 110
					},
					{
						"cell_type": "code",
						"source": [
							"model = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
							"model.fit(dfX_train_prep, y)"
						],
						"outputs": [],
						"execution_count": 111
					},
					{
						"cell_type": "markdown",
						"source": [
							"Por Ãºltimo, utilizando las funcionalidades de la librerÃ­a pickle, guardamos todo aquello ques nos va a hacer falta para la inferencia con los datos de test"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Modelo\n",
							"with open(\"output/modelo.pkl\", \"wb\") as c:\n",
							"    pickle.dump(model, c)\n",
							"\n",
							"# TransformaciÃ³n\n",
							"with open(\"output/num_imputer.pkl\", \"wb\") as c:\n",
							"    pickle.dump(imp_num, c)\n",
							"with open(\"output/cat_imputer.pkl\", \"wb\") as c:\n",
							"    pickle.dump(imp_cat, c)\n",
							"with open(\"output/encoder.pkl\", \"wb\") as c:\n",
							"    pickle.dump(te, c)\n",
							"\n",
							"# Lista de todas columnas\n",
							"todas_columnas = dfTrainFull.columns.to_list()\n",
							"with open(\"output/columns.pkl\", \"wb\") as c:\n",
							"    pickle.dump(todas_columnas, c)\n",
							"\n",
							"# Columnas Cat\n",
							"with open(\"output/cat_columns.pkl\", \"wb\") as c:\n",
							"     pickle.dump(columnas_cat, c)\n",
							"\n",
							"# Columnas Num\n",
							"with open(\"output/num_columns.pkl\", \"wb\") as c:\n",
							"     pickle.dump(columnas_num, c)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kaggle1_entrenamiento_pablo_garcia_v2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7216d10a-0a29-4e49-8c58-ab6e30e90df8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Primera entrega Kaggle\n",
							"\n",
							"**Pablo GarcÃ­a Caballero**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Importamos librerias"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"import numpy as np\n",
							"import pickle\n",
							"import re\n",
							"\n",
							"# TransformaciÃ³n de datos\n",
							"from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder #, TargetEncoder\n",
							"#import category_encoders as ce\n",
							"from category_encoders import TargetEncoder\n",
							"\n",
							"from sklearn.impute import SimpleImputer\n",
							"from sklearn.compose import ColumnTransformer\n",
							"from sklearn.preprocessing import OneHotEncoder\n",
							"\n",
							"# Modelos\n",
							"from sklearn.tree import DecisionTreeClassifier\n",
							"import lightgbm as lgb\n",
							"\n",
							"# Seleccion de variables y tuning de hiperparÃ¡metros\n",
							"from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
							"\n",
							"# MÃ©tricas para evaluar un modelo de clasificaciÃ³n\n",
							"from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve, auc, roc_curve, roc_auc_score, average_precision_score, confusion_matrix\n",
							"\n",
							"# LibrerÃ­as para visualizaciÃ³n de resultados\n",
							"import matplotlib.pyplot as plt\n",
							"import seaborn as sns\n",
							"\n",
							"pd.set_option('display.max_columns', None)\n",
							"pd.set_option('display.max_rows', None)"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "markdown",
						"source": [
							"Definimos la funciÃ³n disponible en el campus para clasificar las columnas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def guess_type(column_name:str , df:pd.DataFrame, m=None, original_df=None) -> str:\n",
							"\n",
							"    if df[column_name].dtype == 'O':\n",
							"        return 'text'\n",
							"    try:\n",
							"        if m is not None and bool(m.get_all(name=column_name)):\n",
							"            return m.get_all(name=column_name).type[0]\n",
							"        elif m is not None and bool(m.get_all(name=column_name[:-2])) and original_df is not None:\n",
							"            #lets check if this is a date:\n",
							"            original_colname = column_name[:-2]\n",
							"            atype = original_df[original_colname].dtype\n",
							"            typ = str(atype)\n",
							"            if re.search(\"^date.\", typ):\n",
							"                return 'categorical'\n",
							"            if atype == 'O':\n",
							"                row_sample = original_df[original_colname].iloc[0]\n",
							"                if is_date(row_sample):\n",
							"                    return 'categorical'\n",
							"\n",
							"        #if isinstance(row_sample, float):\n",
							"        #    return 'numerical'\n",
							"        if df[column_name].dtype in ['float64', 'float32', 'float16']:\n",
							"            # we have to check if column was wrong catalogued as numerical\n",
							"            grouped = df[column_name].unique()\n",
							"            if len(grouped) < 10:\n",
							"                #check there are no decimals\n",
							"                for v in grouped:\n",
							"                    are_decimals = v - int(v)\n",
							"                    if are_decimals > 0.0:\n",
							"                        return 'numerical'\n",
							"                return 'categorical'\n",
							"            return 'numerical'\n",
							"\n",
							"        elif df[column_name].dtype in ['int64', 'int32', 'int16', 'int8']:\n",
							"            max = df[column_name].max()\n",
							"            min = df[column_name].min()\n",
							"            diff = max - min\n",
							"            diff2 = int(max) - int(min)\n",
							"            if diff == diff2:\n",
							"                if diff < 10:\n",
							"                    grouped = df[column_name].unique()\n",
							"                    if len(grouped)< 10:\n",
							"                        return 'categorical'\n",
							"            return \"numerical\"\n",
							"        else:\n",
							"            return \"categorical\"\n",
							"    except Exception as e:\n",
							"        return 'categorical'"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "markdown",
						"source": [
							"Carga de datos"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#dfTrain = pd.read_csv(\"MasterBI_Train.csv\", low_memory=False)\n",
							"dfTrain = pd.read_csv(\"D:\\Master BI\\ML\\MasterBI_Train.csv\", low_memory=False)\n",
							"label = \"HasDetections\""
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain.head()"
						],
						"outputs": [],
						"execution_count": 94
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain[[\"Platform\", \"Processor\"]].drop_duplicates().sort_values(by=\"Platform\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain[[\"ProductName\", \"EngineVersion\", \"IsBeta\"]].drop_duplicates().sort_values(by=\"EngineVersion\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Como tienen relaciÃ³n vamos a unifircarlas, y despues a dropearlas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain[\"ProductNameIsBetaEngineVersion\"] = dfTrain[\"ProductName\"] + '_' + dfTrain[\"IsBeta\"].astype(str) + '_' + dfTrain[\"EngineVersion\"]\n",
							"\n",
							"dfTrain[\"PlatformProcessor\"] = dfTrain[\"Platform\"] + '_' + dfTrain[\"Processor\"] \n",
							"\n",
							"# Eliminar las columnas originales \n",
							"dfTrain.drop([\"ProductName\", \"EngineVersion\", \"IsBeta\", \"Platform\", \"Processor\"], axis=1, inplace=True)"
						],
						"outputs": [],
						"execution_count": 110
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain.head()"
						],
						"outputs": [],
						"execution_count": 107
					},
					{
						"cell_type": "markdown",
						"source": [
							"**EDA**. En primer lugar, realizamos un analisis exploratorio del conjunto de datos. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrain.info()"
						],
						"outputs": [],
						"execution_count": 70
					},
					{
						"cell_type": "markdown",
						"source": [
							"Agrupamos las columnas que sean identificadores para posteriormermente elimarlas del df, ya que no son utiles para el entramiento"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"id_colums = [\"MachineIdentifier\"]#, \"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]\n",
							"dfTrain.drop(columns=id_colums, inplace=True)"
						],
						"outputs": [],
						"execution_count": 111
					},
					{
						"cell_type": "markdown",
						"source": [
							"Convertimos a categoricas los campos que correspondan en funciÃ³n de la funciÃ³n guess_type"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"for i in dfTrain.columns:\n",
							"    print(f\"{i} = {guess_type(i, dfTrain)}\")\n",
							"    if guess_type(i, dfTrain) == \"categorical\":\n",
							"        dfTrain[i] = dfTrain[i].astype('category')\n",
							"        print(\"convertido\")\n",
							"    else:\n",
							"        print(\"no\")"
						],
						"outputs": [],
						"execution_count": 112
					},
					{
						"cell_type": "code",
						"source": [
							"columnas_cat = dfTrain.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
							"columnas_num = dfTrain.select_dtypes(include=[\"float64\", \"int64\"]).columns.to_list()"
						],
						"outputs": [],
						"execution_count": 113
					},
					{
						"cell_type": "code",
						"source": [
							"print(columnas_cat)\n",
							"print(columnas_num)"
						],
						"outputs": [],
						"execution_count": 114
					},
					{
						"cell_type": "markdown",
						"source": [
							"Eliminamos el label de nuestro conjunto de categÃ³ricas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"if label in columnas_cat:\n",
							"    columnas_cat.remove(label)\n",
							"    print(\"Label borrado de las cat\")\n",
							"else:\n",
							"    columnas_num.remove(label)\n",
							"    print(\"Label borrado de las num\")"
						],
						"outputs": [],
						"execution_count": 115
					},
					{
						"cell_type": "markdown",
						"source": [
							"Imputamos los nulos mediante la media para las numericas y Imputamos los nulos mediante la moda para las categoricas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# esto esta mal , primero dividir\n",
							"\n",
							"imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
							"imp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
							"#dfTrain[columnas_num] = imp_num.fit_transform(dfTrain[columnas_num])"
						],
						"outputs": [],
						"execution_count": 116
					},
					{
						"cell_type": "markdown",
						"source": [
							"Preparamos la codificaciÃ³n de variables"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"cat_cols = columnas_cat+columnas_num \n",
							"dfTrain = dfTrain[cat_cols+['HasDetections']]"
						],
						"outputs": [],
						"execution_count": 117
					},
					{
						"cell_type": "code",
						"source": [
							"te = TargetEncoder()"
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"source": [
							"preprocessor = ColumnTransformer(\n",
							"                    [('onehot', OneHotEncoder(handle_unknown='ignore'), cat_cols)],\n",
							"                    remainder='passthrough')"
						],
						"outputs": [],
						"execution_count": 118
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Modelo**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Dividimos el df en train y test"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"X = dfTrain.drop(label, axis=1)\n",
							"y = dfTrain[label]\n",
							"\n",
							"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) "
						],
						"outputs": [],
						"execution_count": 119
					},
					{
						"cell_type": "code",
						"source": [
							"X_train[columnas_num] = imp_num.fit_transform(X_train[columnas_num])\n",
							"X_train[columnas_cat] = imp_cat.fit_transform(X_train[columnas_cat])\n",
							"#X_train.isnull().sum()"
						],
						"outputs": [],
						"execution_count": 120
					},
					{
						"cell_type": "code",
						"source": [
							"X_test[columnas_num] = imp_num.transform(X_test[columnas_num])\n",
							"X_test[columnas_cat] = imp_cat.transform(X_test[columnas_cat])\n",
							"#X_test.isnull().sum()"
						],
						"outputs": [],
						"execution_count": 121
					},
					{
						"cell_type": "markdown",
						"source": [
							"Aplicamos el encoder"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(columnas_cat)\n",
							"print(columnas_num)"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"source": [
							"X_train_prep = preprocessor.fit_transform(X_train)\n",
							"X_test_prep  = preprocessor.transform(X_test)"
						],
						"outputs": [],
						"execution_count": 122
					},
					{
						"cell_type": "code",
						"source": [
							"scaler = StandardScaler(with_mean=False)\n",
							"\n",
							"\n",
							"# Ajustar y transformar el conjunto de entrenamiento\n",
							"X_train_prep = scaler.fit_transform(X_train_prep)\n",
							"\n",
							"# Transformar el conjunto de prueba (usando los mismos parÃ¡metros de entrenamiento)\n",
							"X_test_prep= scaler.transform(X_test_prep)"
						],
						"outputs": [],
						"execution_count": 123
					},
					{
						"cell_type": "code",
						"source": [
							"# X_train_prep = te.fit_transform(X_train, y_train)\n",
							"# X_test_prep = te.fit_transform(X_test, y_test)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Convertimos a df de nuevo"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Entrenamos el modelo"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"model = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
							"model.fit(X_train_prep, y_train)"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"source": [
							"import lightgbm as lgb\n",
							"import optuna\n",
							"from lightgbm import LGBMClassifier\n",
							"from sklearn.model_selection import cross_val_score\n",
							"from sklearn.datasets import make_classification\n",
							"from sklearn.model_selection import train_test_split\n",
							"\n",
							"# Generar datos de ejemplo\n",
							"X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
							"\n",
							"# Definir la funciÃ³n objetivo\n",
							"def objective(trial):\n",
							"    # Espacio de bÃºsqueda para los hiperparÃ¡metros\n",
							"    param = {\n",
							"        'objective': 'binary',\n",
							"        'metric': 'auc',\n",
							"        'boosting_type': 'gbdt',\n",
							"        'num_leaves': trial.suggest_int('num_leaves', 20, 64),\n",
							"        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
							"        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
							"        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
							"        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
							"        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
							"        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
							"        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
							"        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
							"    }\n",
							"\n",
							"    # Crear el modelo LightGBM\n",
							"    clf = LGBMClassifier(**param)\n",
							"\n",
							"    # ValidaciÃ³n cruzada para evaluar el modelo\n",
							"    score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc').mean()\n",
							"    return score\n",
							"\n",
							"# Crear el estudio con Optuna\n",
							"study = optuna.create_study(direction='maximize')  # Maximizar el AUC\n",
							"study.optimize(objective, n_trials=50)  # Ajusta el nÃºmero de pruebas (n_trials)\n",
							"\n",
							"# Imprimir los mejores resultados\n",
							"print(\"Mejores hiperparÃ¡metros:\", study.best_params)\n",
							"print(\"Mejor puntuaciÃ³n AUC:\", study.best_value)\n",
							"\n",
							"# Entrenar el modelo final con los mejores parÃ¡metros\n",
							"best_params = study.best_params\n",
							"final_model = LGBMClassifier(**best_params)\n",
							"final_model.fit(X_train, y_train)\n",
							"\n",
							"# Evaluar el modelo final\n",
							"print(\"PuntuaciÃ³n en el conjunto de prueba:\", final_model.score(X_test, y_test))\n",
							""
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"Mejores hiperparÃ¡metros:\", study.best_params)\n",
							"print(\"Mejor puntuaciÃ³n AUC:\", study.best_value)\n",
							""
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"best_params = study.best_params\n",
							"final_model = LGBMClassifier(**best_params)\n",
							"#final_model.fit(X_train, y_train)"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"source": [
							"final_model.fit(X_train_prep, y_train)"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"source": [
							"import lightgbm as lgb\n",
							"clf = lgb.LGBMClassifier()\n",
							"clf.fit(X_train_prep, y_train)\n",
							"\n",
							"# model2 = lightgbm.LGBMClassifier()\n",
							"# model2.fit(X_train_prep, y_train)\n",
							""
						],
						"outputs": [],
						"execution_count": 124
					},
					{
						"cell_type": "markdown",
						"source": [
							"**PrediciÃ³n y evaluaciÃ³n**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"prediciones = clf.predict(X = X_train_prep, ) # , )\n",
							"pred_proba = clf.predict_proba( X = X_train_prep)\n",
							"\n",
							"data = confusion_matrix(y_train, prediciones)\n",
							"\n",
							"print(\"Matriz de confusiÃ³n\")\n",
							"plt.figure(figsize=(6,5))\n",
							"sns.set(font_scale=1.4)\n",
							"sns.heatmap(data, cmap=\"Blues\", annot=True, annot_kws={\"size\":12})"
						],
						"outputs": [],
						"execution_count": 125
					},
					{
						"cell_type": "code",
						"source": [
							"prediciones_2 = clf.predict(X = X_test_prep, ) # , )\n",
							"pred_proba_2 = clf.predict_proba( X = X_test_prep)\n",
							"\n",
							"data = confusion_matrix(y_test, prediciones_2)\n",
							"\n",
							"print(\"Matriz de confusiÃ³n\")\n",
							"plt.figure(figsize=(6,5))\n",
							"sns.set(font_scale=1.4)\n",
							"sns.heatmap(data, cmap=\"Blues\", annot=True, annot_kws={\"size\":12})"
						],
						"outputs": [],
						"execution_count": 126
					},
					{
						"cell_type": "code",
						"source": [
							"print(f\"Acurracy: {round(100*accuracy_score(y_train, prediciones),1)}% \\n\")\n",
							"print(classification_report(y_train, prediciones, digits=3, zero_division=True))"
						],
						"outputs": [],
						"execution_count": 127
					},
					{
						"cell_type": "code",
						"source": [
							"print(f\"Acurracy: {round(100*accuracy_score(y_test, prediciones_2),1)}% \\n\")\n",
							"print(classification_report(y_test, prediciones_2, digits=3, zero_division=True))"
						],
						"outputs": [],
						"execution_count": 128
					},
					{
						"cell_type": "markdown",
						"source": [
							"Curva ROC y AUX"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"fpr, tpr, _ = roc_curve(y_train, pred_proba[:,1])\n",
							"roc_auc = auc(fpr, tpr)\n",
							"\n",
							"plt.figure(figsize = (6,5))\n",
							"lw = 2\n",
							"plt.plot(fpr, tpr, color='darkorange',\n",
							"         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
							"plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
							"plt.xlim([0.0, 1.0])\n",
							"plt.ylim([0.0, 1.05])\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.title('Receiver operating characteristic example')\n",
							"plt.legend(loc=\"lower right\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 129
					},
					{
						"cell_type": "code",
						"source": [
							"fpr, tpr, _ = roc_curve(y_test, pred_proba_2[:,1])\n",
							"roc_auc = auc(fpr, tpr)\n",
							"\n",
							"plt.figure(figsize = (6,5))\n",
							"lw = 2\n",
							"plt.plot(fpr, tpr, color='darkorange',\n",
							"         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
							"plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
							"plt.xlim([0.0, 1.0])\n",
							"plt.ylim([0.0, 1.05])\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.title('Receiver operating characteristic example')\n",
							"plt.legend(loc=\"lower right\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 130
					},
					{
						"cell_type": "markdown",
						"source": [
							"**Pase a producciÃ³n**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Replicamos los pasos anteriores, pero sin dividir el df en test/train"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTrainFull = pd.read_csv(\"MasterBI_Train.csv\", low_memory=False)\n",
							"label = \"HasDetections\""
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "code",
						"source": [
							"id_colums = [\"MachineIdentifier\"]#, \"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]\n",
							"dfTrainFull.drop(columns=id_colums, inplace=True)"
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"source": [
							"for i in dfTrainFull.columns:\n",
							"    print(f\"{i} = {guess_type(i, dfTrainFull)}\")\n",
							"    if guess_type(i, dfTrainFull) == \"categorical\":\n",
							"        dfTrainFull[i] = dfTrainFull[i].astype('category')\n",
							"        print(\"convertido\")\n",
							"    else:\n",
							"        print(\"no\")"
						],
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "code",
						"source": [
							"columnas_cat = dfTrainFull.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
							"columnas_num = dfTrainFull.select_dtypes(include=[\"float64\", \"int64\"]).columns.to_list()"
						],
						"outputs": [],
						"execution_count": 100
					},
					{
						"cell_type": "code",
						"source": [
							"columnas_cat.remove(label)"
						],
						"outputs": [],
						"execution_count": 101
					},
					{
						"cell_type": "code",
						"source": [
							"imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
							"dfTrainFull[columnas_num] = imp_num.fit_transform(dfTrainFull[columnas_num])"
						],
						"outputs": [],
						"execution_count": 102
					},
					{
						"cell_type": "code",
						"source": [
							"imp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
							"dfTrainFull[columnas_cat] = imp_cat.fit_transform(dfTrainFull[columnas_cat])"
						],
						"outputs": [],
						"execution_count": 103
					},
					{
						"cell_type": "code",
						"source": [
							"cat_num = columnas_cat+columnas_num \n",
							"dfTrainFull = dfTrainFull[cat_num+['HasDetections']]"
						],
						"outputs": [],
						"execution_count": 104
					},
					{
						"cell_type": "code",
						"source": [
							"te = TargetEncoder()"
						],
						"outputs": [],
						"execution_count": 105
					},
					{
						"cell_type": "code",
						"source": [
							"X = dfTrainFull.drop(label, axis=1)\n",
							"y = dfTrainFull[label]"
						],
						"outputs": [],
						"execution_count": 106
					},
					{
						"cell_type": "code",
						"source": [
							"X_train_prep = te.fit_transform(X, y)"
						],
						"outputs": [],
						"execution_count": 107
					},
					{
						"cell_type": "code",
						"source": [
							"columnas = dfTrain.columns.to_list()\n",
							"columnas.remove(label)"
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "code",
						"source": [
							"dfX_train_prep = pd.DataFrame(X_train_prep , columns=columnas)"
						],
						"outputs": [],
						"execution_count": 110
					},
					{
						"cell_type": "code",
						"source": [
							"model = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
							"model.fit(dfX_train_prep, y)"
						],
						"outputs": [],
						"execution_count": 111
					},
					{
						"cell_type": "markdown",
						"source": [
							"Por Ãºltimo, utilizando las funcionalidades de la librerÃ­a pickle, guardamos todo aquello ques nos va a hacer falta para la inferencia con los datos de test"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Modelo\n",
							"with open(\"output/modelo.pkl\", \"wb\") as c:\n",
							"    pickle.dump(model, c)\n",
							"\n",
							"# TransformaciÃ³n\n",
							"with open(\"output/num_imputer.pkl\", \"wb\") as c:\n",
							"    pickle.dump(imp_num, c)\n",
							"with open(\"output/cat_imputer.pkl\", \"wb\") as c:\n",
							"    pickle.dump(imp_cat, c)\n",
							"with open(\"output/encoder.pkl\", \"wb\") as c:\n",
							"    pickle.dump(te, c)\n",
							"\n",
							"# Lista de todas columnas\n",
							"todas_columnas = dfTrainFull.columns.to_list()\n",
							"with open(\"output/columns.pkl\", \"wb\") as c:\n",
							"    pickle.dump(todas_columnas, c)\n",
							"\n",
							"# Columnas Cat\n",
							"with open(\"output/cat_columns.pkl\", \"wb\") as c:\n",
							"     pickle.dump(columnas_cat, c)\n",
							"\n",
							"# Columnas Num\n",
							"with open(\"output/num_columns.pkl\", \"wb\") as c:\n",
							"     pickle.dump(columnas_num, c)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kaggle1_inferencia_pablo_garcia')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "607bd92e-7fd7-40b9-9b72-6769a8c704e4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Primera entrega Kaggle\n",
							"\n",
							"Inferencia \n",
							"\n",
							"**Pablo GarcÃ­a Caballero**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Importamos librerias"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"import numpy as np\n",
							"import pickle\n",
							"import re\n",
							"\n",
							"# TransformaciÃ³n de datos\n",
							"from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder #, TargetEncoder\n",
							"#import category_encoders as ce\n",
							"from category_encoders import TargetEncoder\n",
							"\n",
							"from sklearn.impute import SimpleImputer\n",
							"\n",
							"# Modelos\n",
							"from sklearn.tree import DecisionTreeClassifier\n",
							"\n",
							"# Seleccion de variables y tuning de hiperparÃ¡metros\n",
							"from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
							"\n",
							"# MÃ©tricas para evaluar un modelo de clasificaciÃ³n\n",
							"from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve, auc, roc_curve, roc_auc_score, average_precision_score, confusion_matrix\n",
							"\n",
							"# LibrerÃ­as para visualizaciÃ³n de resultados\n",
							"import matplotlib.pyplot as plt\n",
							"import seaborn as sns\n",
							"\n",
							"pd.set_option('display.max_columns', None)\n",
							"pd.set_option('display.max_rows', None)"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"source": [
							"def guess_type(column_name:str , df:pd.DataFrame, m=None, original_df=None) -> str:\n",
							"\n",
							"    if df[column_name].dtype == 'O':\n",
							"        return 'text'\n",
							"    try:\n",
							"        if m is not None and bool(m.get_all(name=column_name)):\n",
							"            return m.get_all(name=column_name).type[0]\n",
							"        elif m is not None and bool(m.get_all(name=column_name[:-2])) and original_df is not None:\n",
							"            #lets check if this is a date:\n",
							"            original_colname = column_name[:-2]\n",
							"            atype = original_df[original_colname].dtype\n",
							"            typ = str(atype)\n",
							"            if re.search(\"^date.\", typ):\n",
							"                return 'categorical'\n",
							"            if atype == 'O':\n",
							"                row_sample = original_df[original_colname].iloc[0]\n",
							"                if is_date(row_sample):\n",
							"                    return 'categorical'\n",
							"\n",
							"        #if isinstance(row_sample, float):\n",
							"        #    return 'numerical'\n",
							"        if df[column_name].dtype in ['float64', 'float32', 'float16']:\n",
							"            # we have to check if column was wrong catalogued as numerical\n",
							"            grouped = df[column_name].unique()\n",
							"            if len(grouped) < 10:\n",
							"                #check there are no decimals\n",
							"                for v in grouped:\n",
							"                    are_decimals = v - int(v)\n",
							"                    if are_decimals > 0.0:\n",
							"                        return 'numerical'\n",
							"                return 'categorical'\n",
							"            return 'numerical'\n",
							"\n",
							"        elif df[column_name].dtype in ['int64', 'int32', 'int16', 'int8']:\n",
							"            max = df[column_name].max()\n",
							"            min = df[column_name].min()\n",
							"            diff = max - min\n",
							"            diff2 = int(max) - int(min)\n",
							"            if diff == diff2:\n",
							"                if diff < 10:\n",
							"                    grouped = df[column_name].unique()\n",
							"                    if len(grouped)< 10:\n",
							"                        return 'categorical'\n",
							"            return \"numerical\"\n",
							"        else:\n",
							"            return \"categorical\"\n",
							"    except Exception as e:\n",
							"        return 'categorical'"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "markdown",
						"source": [
							"Carga de datos"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTest = pd.read_csv(\"MasterBI_Test.csv\", low_memory=False)\n",
							"label = \"HasDetections\""
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"source": [
							"dfTest.head()"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"source": [
							"**EDA**. En primer lugar, realizamos un analisis exploratorio del conjunto de datos. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTest = dfTest.set_index(\"MachineIdentifier\")"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"source": [
							"Agrupamos las columnas que sean identificadores para posteriormermente elimarlas del df, ya que no son utiles para el entramiento"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#id_colums = [\"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"#dfTest.drop(columns=id_colums, inplace=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Convertimos a categoricas los campos que correspondan en funciÃ³n de la funciÃ³n guess_type"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"for i in dfTest.columns:\n",
							"    #print(i)\n",
							"    print(f\"{i} = {guess_type(i, dfTest)}\")\n",
							"    if guess_type(i, dfTest) == \"categorical\":\n",
							"        dfTest[i] = dfTest[i].astype('category')\n",
							"        print(\"convertido\")\n",
							"    else:\n",
							"        print(\"no\")"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "markdown",
						"source": [
							"Cargamos columnas"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"with open(\"output/cat_columns.pkl\", \"rb\") as c:\n",
							"    cat_cols = pickle.load(c)"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"source": [
							"with open(\"output/num_columns.pkl\", \"rb\") as c:\n",
							"    num_cols = pickle.load(c)"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "markdown",
						"source": [
							"ImputaciÃ³n de nulos"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"with open(\"output/cat_imputer.pkl\", \"rb\") as c:\n",
							"    cat_imputer = pickle.load(c)\n",
							"\n",
							"dfTest[cat_cols] = cat_imputer.transform(dfTest[cat_cols])"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"source": [
							"with open(\"output/num_imputer.pkl\", \"rb\") as c:\n",
							"    num_imputer = pickle.load(c)\n",
							"\n",
							"dfTest[num_cols] = num_imputer.transform(dfTest[num_cols])"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "markdown",
						"source": [
							"Codificamos variables"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfTest[cat_cols].head()"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"source": [
							"with open(\"output/columns.pkl\", \"rb\") as c:\n",
							"    columnas = pickle.load(c)\n",
							"    columnas.remove(label)"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"source": [
							"with open(\"output/encoder.pkl\", \"rb\") as c:\n",
							"    encoder = pickle.load(c)\n",
							"\n",
							"dfTest[columnas] = encoder.transform(dfTest[columnas])"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "markdown",
						"source": [
							"PredicciÃ³n de resultados"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"with open(\"output/modelo.pkl\", \"rb\") as c:\n",
							"    modelo = pickle.load(c)"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"source": [
							"ordered_columns = cat_cols + num_cols\n",
							"dfTest = dfTest[ordered_columns]\n",
							""
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"source": [
							"dfTest.head()"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"source": [
							"y_hat = modelo.predict(dfTest)"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "markdown",
						"source": [
							"Generamos el dataset de resultados"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"resultados = pd.DataFrame()\n",
							"resultados[\"MachineIdentifier\"] = dfTest.reset_index()[\"MachineIdentifier\"]\n",
							"resultados[\"HasDetections\"] = y_hat"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "markdown",
						"source": [
							"Guardamos el resultado en formato csv"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"resultados.to_csv(\"output/resultadosKaggle1_pablo_garcia.csv\", index=False)"
						],
						"outputs": [],
						"execution_count": 49
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkTFM')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "francecentral"
		}
	]
}