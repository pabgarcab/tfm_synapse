{
	"name": "MergeBronzeFilesToSilver",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b8d3fdac-e4f9-403c-bb01-ee0a428c3bfd"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# path of the data lake container (bronze and silver for this example)\r\n",
					"data_lake_container = 'abfss://mdw@datalake1pgc.dfs.core.windows.net'\r\n",
					"# The ingestion folder where your parquet file are located\r\n",
					"bronze_folder = 'bronze/Landing'\r\n",
					"# The silver folder where your Delta Tables will be stored\r\n",
					"silver_folder = 'Silver'\r\n",
					"# The name of the table\r\n",
					"table_name = 'fechas'\r\n",
					"# The wildcard filter used within the bronze folder to find files\r\n",
					"source_wildcard = 'fechas*.parquet'\r\n",
					"# A comma separated string of one or more key columns (for the merge)\r\n",
					"key_columns_str = 'idfechas'"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import modules\r\n",
					"from delta.tables import DeltaTable"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Convert comma separated string with keys to array\r\n",
					"key_columns = key_columns_str.split(',')  \r\n",
					" \r\n",
					"# Convert array with keys to where-clause for merge statement\r\n",
					"conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
					" \r\n",
					"# Determine path of source files from ingest layer\r\n",
					"source_path = data_lake_container + '/' + bronze_folder + '/' + source_wildcard \r\n",
					" \r\n",
					"# Determine path of Delta Lake Table \r\n",
					"delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
					" \r\n",
					"# Read file(s) in spark data frame\r\n",
					"sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
					" \r\n",
					"# Check if the Delta Table exists\r\n",
					"if (DeltaTable.isDeltaTable(spark, delta_table_path)):\r\n",
					"    print('Existing delta table')\r\n",
					"    # Read the existing Delta Table\r\n",
					"    delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
					" \r\n",
					"    # Merge new data into existing table\r\n",
					"    delta_table.alias(\"existing\").merge(\r\n",
					"        source = sdf.alias(\"updates\"),\r\n",
					"        condition = \" AND \".join(conditions_list)\r\n",
					"         \r\n",
					"    ).whenMatchedUpdateAll(\r\n",
					"    ).whenNotMatchedInsertAll(\r\n",
					"    ).execute()\r\n",
					" \r\n",
					"    # For transactions you could do an append instead of a merge\r\n",
					"    # sdf.write.format('delta').mode('append').save(delta_table_path)\r\n",
					" \r\n",
					"else:\r\n",
					"    print('New delta table')\r\n",
					"    # Create new delta table with new data\r\n",
					"    sdf.write.format('delta').save(delta_table_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Adding the Delta Table to the Delta Database for easy querying in other notebooks or scripts within Synapse.\r\n",
					"spark.sql(f'CREATE TABLE IF NOT EXISTS Silver.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
					" \r\n",
					"# Spark SQL version\r\n",
					"#  CREATE TABLE Silver.MyTable\r\n",
					"#  USING DELTA\r\n",
					"#  LOCATION 'abfss://yourcontainer@yourdatalake.dfs.core.windows.net/Silver/MyTable'"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Actiualizar DateUpdate en el csv"
				],
				"execution_count": null
			}
		]
	}
}