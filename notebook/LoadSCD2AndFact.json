{
	"name": "LoadSCD2AndFact",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkTFM",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "abbce457-4c46-4849-9524-0b7731457c61"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
				"name": "sparkTFM",
				"type": "Spark",
				"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Import modules**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from delta.tables import DeltaTable\r\n",
					"from delta.tables import *\r\n",
					"from pyspark.sql.functions import when, lit, current_date, date_format,concat_ws,xxhash64,current_timestamp\r\n",
					"from pyspark.sql import SparkSession,Window\r\n",
					"from pyspark.sql import functions as F\r\n",
					"import json\r\n",
					"import re\r\n",
					"from datetime import datetime\r\n",
					"from azure.storage.blob import BlobServiceClient\r\n",
					"import io\r\n",
					"import pandas as pd\r\n",
					"import csv"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"USE default;\r\n",
					"DROP DATABASE  gold CASCADE ;\r\n",
					"USE default;\r\n",
					"CREATE DATABASE IF NOT EXISTS gold;"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#%%sql\r\n",
					"# USE gold;\r\n",
					"# DROP TABLE IF EXISTS fact_Sales;"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"factList = [ \r\n",
					"        {\r\n",
					"            \"name\": \"Sales\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idDesgloseVenta AS idSales,\r\n",
					"                        d.idArticulo AS idArticles,\r\n",
					"                        c.idAlmacen AS idWarehouse,\r\n",
					"                        c.idCliente AS idClient,\r\n",
					"                        c.idCodigoPostal AS idPostalCode,\r\n",
					"                        c.idDivisa AS idCurrency,\r\n",
					"                        c.idTarifa AS idTariff,\r\n",
					"                        c.idTipoOperacion AS idOperationType,\r\n",
					"                        c.idHora AS idHours,\r\n",
					"                        c.idFecha AS idDate,\r\n",
					"                        f.fecha AS  date,\r\n",
					"                        CAST(CONCAT(DATE_FORMAT(f.fecha, 'yyyy-MM-dd'), ' ', h.horaCompleta) AS TIMESTAMP) AS dateOp,\r\n",
					"                        COALESCE(c.codigoTicket, 'D') AS ticketNumber,\r\n",
					"                        COALESCE(d.Cantidad, 0) AS quantity,\r\n",
					"                        COALESCE(d.PrecioUnitario, 0)  AS unitPrice,\r\n",
					"                        COALESCE(d.CosteUnitario, 0)  AS unitCost,\r\n",
					"                        COALESCE(d.importeBruto, 0)  AS amtTotalEuros,\r\n",
					"                        COALESCE(d.importeNeto, 0)  AS amtNetEuros,\r\n",
					"                        COALESCE(d.importeDescuento, 0)  AS amtTotalEurosDiscount,\r\n",
					"                        COALESCE(d.importeNetoDescuento, 0) AS amtNetEurosDiscount,\r\n",
					"                        CASE WHEN idTarifa IN (2,3) THEN 1\r\n",
					"                            ELSE 0 \r\n",
					"                        END AS discountSale,\r\n",
					"                        CASE WHEN idTarifa = 0 THEN 0\r\n",
					"                            ELSE (d.importeBruto - d.importeDescuento) \r\n",
					"                        END AS amtTotalDiscount,\r\n",
					"                        CASE WHEN idTarifa = 0 THEN 0 \r\n",
					"                            ELSE (d.importeNeto - d.importeNetoDescuento) \r\n",
					"                        END AS amtNetDiscount,\r\n",
					"                        GREATEST(d.fechaCarga, c.fechaCarga) AS loadDate,\r\n",
					"                        GREATEST(d.fechaDelta, c.fechaDelta) AS deltaDate\r\n",
					"                    FROM silver.desgloseVenta AS d\r\n",
					"                    INNER JOIN silver.cabeceraVenta AS c\r\n",
					"                        ON d.idcabecerasVentas = c.idcabeceraVenta\r\n",
					"                    LEFT JOIN silver.Fechas  AS f\r\n",
					"                        On f.idFechas = c.idFecha\r\n",
					"                    LEFT JOIN silver.horas  AS h\r\n",
					"                        On h.idHoras = c.idHora\r\n",
					"                    WHERE GREATEST(d.fechaDelta, c.fechaDelta) >=\r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"    ]"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"lookupFact =  [ \r\n",
					"        {\r\n",
					"            \"name\": \"Sales\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT    \r\n",
					"                        s.*,\r\n",
					"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
					"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
					"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
					"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
					"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
					"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
					"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
					"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
					"                        COALESCE(d.skDate, -1) AS skDate\r\n",
					"                FROM temp_sales_view as s\r\n",
					"                LEFT JOIN gold.dim_Articles AS a\r\n",
					"                    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate AND a.isCurrent = 1\r\n",
					"                LEFT JOIN gold.dim_Client AS cl\r\n",
					"                    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate AND cl.isCurrent = 1\r\n",
					"                LEFT JOIN gold.dim_Currency AS cu\r\n",
					"                    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate AND cu.isCurrent = 1\r\n",
					"                LEFT JOIN gold.dim_Date AS d\r\n",
					"                    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate AND d.isCurrent = 1\r\n",
					"                LEFT JOIN gold.dim_Hours AS h\r\n",
					"                    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate AND h.isCurrent = 1\r\n",
					"                LEFT JOIN gold.dim_OperationType AS o\r\n",
					"                    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate AND o.isCurrent = 1\r\n",
					"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
					"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate AND p.isCurrent = 1\r\n",
					"                LEFT JOIN gold.dim_Tariff AS t\r\n",
					"                    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate AND t.isCurrent = 1\r\n",
					"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
					"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate AND w.isCurrent = 1\r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"    ]"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dimensionsList = [ \r\n",
					"        {\r\n",
					"            \"name\": \"Articles\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idArticulos                                                                                             AS idArticles,\r\n",
					"                            COALESCE(nombre, 'D')                                                                                   AS name,\r\n",
					"                            COALESCE(descripcion, 'D')                                                                              AS description,\r\n",
					"                            COALESCE(codigoReferencia, 'D')                                                                         AS externalCode,\r\n",
					"                            COALESCE(t.talla, 'D')                                                                                  AS size,\r\n",
					"                            COALESCE( t.numeroTalla, -1)                                                                            AS numSize,\r\n",
					"                            COALESCE(co.color, 'D')                                                                                 AS colour,\r\n",
					"                            COALESCE(ca.categoria, 'D')                                                                             AS category,\r\n",
					"                            COALESCE(l.codigoLinea, 'D')                                                                            AS codLine,\r\n",
					"                            COALESCE(l.Linea, 'D')                                                                                  AS line, \r\n",
					"                            CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
					"                                 WHEN a.idCategoria IN (2,4,6,7,9) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
					"                                 ELSE CAST(CONCAT('PV',CAST(a.idTemporada AS string)) AS string)\r\n",
					"                            END                                                                                                     AS season,              \r\n",
					"                            GREATEST(a.fechaCarga, t.fechaCarga, co.fechaCarga, ca.fechaCarga, l.fechaCarga)                        AS loadDate,\r\n",
					"                            GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta)                        AS deltaDate\r\n",
					"                    FROM silver.articulos AS a\r\n",
					"                        LEFT JOIN silver.talla AS t\r\n",
					"                            ON t.idTalla = a.idTalla\r\n",
					"                        LEFT JOIN silver.color AS co\r\n",
					"                            ON co.idColor = a.idColor\r\n",
					"                        LEFT JOIN silver.categoria AS ca -- select * from silver.categoria\r\n",
					"                            ON ca.idCategoria = a.idCategoria\r\n",
					"                        LEFT JOIN silver.Linea AS l\r\n",
					"                            ON l.idLinea = a.idLinea\r\n",
					"                    WHERE GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta) >= \r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Client\",\r\n",
					"            \"query\": \"\"\"              \r\n",
					"                    SELECT  idCliente                                           AS idClient,\r\n",
					"                            COALESCE(c.nombre, 'D')                             AS name,\r\n",
					"                            COALESCE(apellido1, 'D')                            AS lastName1,\r\n",
					"                            COALESCE(apellido2, 'D')                            AS lastName2, \r\n",
					"                            COALESCE(email, 'D')                                AS email,\r\n",
					"                            COALESCE(telefono, 'D')                             AS phoneNumber,\r\n",
					"                            COALESCE(CAST(cumpleanos AS STRING), '1900-01-01')  AS birthDay,       \r\n",
					"                            YEAR(current_date()) - YEAR(CAST(cumpleanos AS DATE)) \r\n",
					"                                - CASE \r\n",
					"                                    WHEN MONTH(current_date()) < MONTH(CAST(cumpleanos AS DATE)) \r\n",
					"                                        OR (MONTH(current_date()) = MONTH(CAST(cumpleanos AS DATE)) AND DAY(current_date()) < DAY(CAST(cumpleanos AS DATE)))\r\n",
					"                                    THEN 1\r\n",
					"                                    ELSE 0\r\n",
					"                            END                                                 AS age,\r\n",
					"                            CASE WHEN hombre = 1 THEN 'Hombre' \r\n",
					"                                 ELSE 'Mujer' \r\n",
					"                            END                                                 AS gender,\r\n",
					"                            COALESCE(p.nombre, 'D')                             AS country,\r\n",
					"                            COALESCE(p.codigoPais, 'D')                         AS countryCode,\r\n",
					"                            COALESCE(cp.region, 'D')                            AS region,\r\n",
					"                            COALESCE(c.Direcion, 'D')                           AS address,  \r\n",
					"                            COALESCE(codigoPostal, 'D')                         AS postalCode,\r\n",
					"                            COALESCE(activo, false)                             AS active,  -- Cambiado 0 a false aquí\r\n",
					"                            GREATEST(c.fechaCarga, p.fechaCarga, cp.fechaCarga) AS loadDate,\r\n",
					"                            GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) AS deltaDate\r\n",
					"                        FROM silver.cliente AS c\r\n",
					"                            LEFT JOIN silver.pais AS p \r\n",
					"                                ON c.idPais = p.idPais\r\n",
					"                            INNER JOIN silver.codigoPostal AS cp \r\n",
					"                                ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\r\n",
					"                        WHERE GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) >= \r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"        ,\r\n",
					"        {\r\n",
					"            \"name\": \"Currency\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idDivisa              AS idCurrency,\r\n",
					"                            COALESCE(nombre, 'D') AS name,\r\n",
					"                            COALESCE(Divisa, 'D') AS currency,\r\n",
					"                            fechaCarga            AS loadDate,\r\n",
					"                            fechaDelta            AS deltaDate\r\n",
					"                    FROM silver.divisa\r\n",
					"                    WHERE fechaDelta >=\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Date\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idFechas                                  AS idDate,\r\n",
					"                            ClaveFecha                                AS dateKey,\r\n",
					"                            Fecha                                     AS date,\r\n",
					"                            COALESCE(Mes, 'D')                        AS month,\r\n",
					"                            COALESCE(NumeroMes, -1)                   AS monthNumber,\r\n",
					"                            COALESCE(Ano, -1)                         AS year,\r\n",
					"                            COALESCE(NumeroSemana, -1)                AS weekNumber,\r\n",
					"                            COALESCE(DiaSemana, 'D')                  AS dayWeek,\r\n",
					"                            COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\r\n",
					"                            COALESCE(DiaAno, -1)                      AS yearDay,\r\n",
					"                            CASE WHEN Trimestre = 1 THEN 'Q1'\r\n",
					"                                WHEN Trimestre = 2 THEN 'Q2'\r\n",
					"                                WHEN Trimestre = 3 THEN 'Q3'\r\n",
					"                                ELSE '-1' END                         AS quarter,\r\n",
					"                            CASE WHEN Cuatrimestre = 1 THEN 'Q1'\r\n",
					"                                WHEN Cuatrimestre = 2 THEN 'Q2'\r\n",
					"                                WHEN Cuatrimestre = 3 THEN 'Q3'\r\n",
					"                                WHEN Cuatrimestre = 4 THEN 'Q4'\r\n",
					"                                ELSE '-1' \r\n",
					"                            END                                       AS quadrimester,\r\n",
					"                            CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\r\n",
					"                                WHEN Cuatrimestre IN (2,4) THEN 'S2'\r\n",
					"                                ELSE '-1' \r\n",
					"                            END                                       AS semester,\r\n",
					"                            fechaCarga                                AS loadDate,\r\n",
					"                            fechaDelta                                AS deltaDate\r\n",
					"                    FROM silver.fechas\r\n",
					"                    WHERE fechaDelta >=\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Hours\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idHoras                     AS idHours,\r\n",
					"                            COALESCE(Hora, -1)          AS hour,\r\n",
					"                            COALESCE(Minuto, -1)        AS minute,\r\n",
					"                            COALESCE(HoraCompleta, 'D') AS fullHour,\r\n",
					"                            fechaCarga                  AS loadDate,\r\n",
					"                            fechaDelta                  AS deltaDate\r\n",
					"                    FROM silver.horas\r\n",
					"                    WHERE fechaDelta >=\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"OperationType\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idTipoOperacion          AS idOperationType,\r\n",
					"                            COALESCE(Operacion, 'D') AS operation,\r\n",
					"                            fechaCarga               AS loadDate,\r\n",
					"                            fechaDelta               AS deltaDate\r\n",
					"                    FROM silver.tipoOperacion\r\n",
					"                    WHERE fechaDelta >=\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"PostalCode\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idCodigoPostal                       AS idPostalCode,\r\n",
					"                            COALESCE(codigoPostal, 'D')          AS postalCode,\r\n",
					"                            COALESCE(region, 'D')                AS region,\r\n",
					"                            COALESCE(c.codigoPais, 'D')          AS countryCode,\r\n",
					"                            COALESCE(nombre, 'D')                AS country,\r\n",
					"                            GREATEST(c.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
					"                            GREATEST(c.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
					"                    FROM silver.codigoPostal AS c\r\n",
					"                    LEFT JOIN silver.pais AS p\r\n",
					"                        ON p.codigoPais = c.codigoPais\r\n",
					"                    WHERE GREATEST(c.fechaDelta, p.fechaDelta) >= \r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Tariff\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idTarifa              AS idTariff,\r\n",
					"                            COALESCE(Tarifa, 'D') AS tariff,\r\n",
					"                            fechaCarga            AS loadDate,\r\n",
					"                            fechaDelta            AS deltaDate       \r\n",
					"                    FROM silver.tarifa\r\n",
					"                    WHERE fechaDelta >= \r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"        ,\r\n",
					"        {\r\n",
					"            \"name\": \"Warehouse\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idAlmacenes                          AS idWarehouse,\r\n",
					"                            COALESCE(a.Nombre, 'D')              AS warehouse,\r\n",
					"                            COALESCE(a.codigoAlmacen, 'D')       AS externalCode,\r\n",
					"                            COALESCE(p.codigoPais, 'D')          AS countryCode, \r\n",
					"                            COALESCE(p.nombre, 'D')              AS country,\r\n",
					"                            COALESCE(a.ciudad, 'D')              AS city,\r\n",
					"                            COALESCE(a.Direccion, 'D')           AS address,\r\n",
					"                            COALESCE(a.descripcion, 'D')         AS description,\r\n",
					"                            GREATEST(a.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
					"                            GREATEST(a.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
					"                    FROM silver.almacenes AS a\r\n",
					"                    LEFT JOIN silver.pais AS p\r\n",
					"                        ON p.idpais = a.idPais\r\n",
					"                    WHERE GREATEST(a.fechaDelta, p.fechaDelta) >= \r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"    ]"
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Configuration**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# parameter string to array\r\n",
					"tables_to_load = json.loads(tables_to_load)\r\n",
					"\r\n",
					"today_datetime = datetime.now()\r\n",
					"\r\n",
					"delta_date_filter = (today_datetime.strftime('%Y%m%d')) # 'yyyyMMdd'\r\n",
					"delta_date_filter = '19901008' # Debud\r\n",
					"\r\n",
					"delta_date_string = today_datetime.strftime('%Y-%m-%d') # 'yyyy-mm-dd'\r\n",
					"delta_date_string = '1990-10-08'\r\n",
					"\r\n",
					"# Convertir cada valor del CSV en un diccionario estructurado\r\n",
					"dict_tables_to_load = []\r\n",
					"\r\n",
					"for row in tables_to_load:\r\n",
					"    dict_tables_to_load.append({\r\n",
					"    \"SchemaName\": row[\"SchemaName\"],\r\n",
					"    \"TableName\": row[\"TableName\"],\r\n",
					"    \"UpdateDate\": row[\"UpdateDate\"],\r\n",
					"    \"Load\": row[\"Load\"]\r\n",
					"})\r\n",
					"\r\n",
					"# Variables para almacenar los datos de 'dim' y 'fact'\r\n",
					"dim_tables = []\r\n",
					"fact_tables = []\r\n",
					"fact_lockup = []\r\n",
					"\r\n",
					"# Clasificar los datos\r\n",
					"for row in dict_tables_to_load:\r\n",
					"    if row['SchemaName'] == 'dim':\r\n",
					"        dim_tables.append(row)\r\n",
					"    elif row['SchemaName'] == 'fact':\r\n",
					"        fact_tables.append(row)\r\n",
					"\r\n",
					"# Filtrar las listas de antes para obtener solo las tablas con Load='Y' y devolver una lista de diccionarios\r\n",
					"load_y_tables = {table['TableName'] for table in dim_tables if table['Load'] == 'Y'}\r\n",
					"filtered_dimensionsList = [\r\n",
					"    dim for dim in dimensionsList if dim[\"name\"] in load_y_tables\r\n",
					"]\r\n",
					"\r\n",
					"load_y_tables2 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
					"filtered_factList = [\r\n",
					"    dim for dim in factList if dim[\"name\"] in load_y_tables2\r\n",
					"]\r\n",
					"\r\n",
					"# Implementar\r\n",
					"load_y_tables3 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
					"filtered_factList_Lookup = [\r\n",
					"    dim for dim in lookupFact if dim[\"name\"] in load_y_tables3\r\n",
					"]\r\n",
					"\r\n",
					"# load_y_tables4 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
					"# filtered_createTablesFact = [\r\n",
					"#     dim for dim in createTablesFact if dim[\"name\"] in load_y_tables4\r\n",
					"# ]\r\n",
					""
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Load SCD2 Dimensions**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Carga o Merge Dimensiones\r\n",
					"results = []\r\n",
					"exception = []\r\n",
					"\r\n",
					"for file in filtered_dimensionsList:\r\n",
					"    file_result = {\r\n",
					"        'table': f\"dim_{file['name']}\",\r\n",
					"        'status': 'Incorrect',\r\n",
					"        'inserted_rows': 0,\r\n",
					"        'updated_rows': 0,\r\n",
					"        'exception': 'N/A',\r\n",
					"        'datetime': str(datetime.now()) \r\n",
					"    }\r\n",
					"\r\n",
					"    try:\r\n",
					"\r\n",
					"        table_name = file[\"name\"].split('_')[0]\r\n",
					"        #source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
					"        key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
					"\r\n",
					"        key_columns = key_columns_str.split(',')\r\n",
					"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
					"\r\n",
					"        #delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Dimensions/{table_name}\"\r\n",
					"        delta_table_path_dimensions = f\"{data_lake_container}/{golden_folder_dimensions}/{table_name}\"\r\n",
					"\r\n",
					"\r\n",
					"        df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
					"        sdf = df.cache()\r\n",
					"\r\n",
					"        if sdf.limit(1).count() == 0:\r\n",
					"            print(f\"No se procesará el archivo {file['name']} porque no contiene datos.\")\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            file_result['exception'] = \"There is no new data\"\r\n",
					"            results.append(file_result)\r\n",
					"            continue\r\n",
					"\r\n",
					"        \r\n",
					"        if DeltaTable.isDeltaTable(spark, delta_table_path_dimensions):\r\n",
					"            # Añadir las columnas necesarias\r\n",
					"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
					"\r\n",
					"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fromDate', 'toDate', 'isCurrent')]\r\n",
					"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
					"\r\n",
					"            todas_las_columnas = sdf.columns\r\n",
					"            columnas_al_principio = [f\"id{table_name}\"]\r\n",
					"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
					"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
					"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
					"            sdf = sdf.select(*nuevo_orden_columnas)\r\n",
					"\r\n",
					"            next_surrogate_key = spark.sql(f\"SELECT COALESCE(MAX(sk{table_name}), 0) + 1 AS next_key FROM gold.dim_{table_name}\").collect()[0][\"next_key\"]\r\n",
					"\r\n",
					"            sdf.createOrReplaceTempView(\"temp_view\")\r\n",
					"\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                MERGE INTO gold.dim_{table_name}  \r\n",
					"                AS existing\r\n",
					"                USING temp_view AS updates\r\n",
					"                ON {\" AND \".join([f\"existing.{key}=updates.{key}\" for key in key_columns])}\r\n",
					"                WHEN MATCHED AND existing.isCurrent = 1 AND existing.hash != updates.hash THEN\r\n",
					"                UPDATE SET\r\n",
					"                    existing.toDate = current_date(),\r\n",
					"                    existing.isCurrent = 0\r\n",
					"            \"\"\")\r\n",
					"\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                INSERT INTO gold.dim_{table_name}    \r\n",
					"                SELECT \r\n",
					"                    {next_surrogate_key} + row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS sk{table_name},  \r\n",
					"                    updates.*\r\n",
					"                FROM temp_view AS updates\r\n",
					"                LEFT JOIN gold.dim_{table_name} AS existing\r\n",
					"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
					"                WHERE \r\n",
					"                    existing.id{table_name} IS NULL OR \r\n",
					"                    (existing.isCurrent = 0 AND existing.hash != updates.hash) OR \r\n",
					"                    (existing.isCurrent = 1 AND existing.hash != updates.hash)\r\n",
					"            \"\"\")\r\n",
					"\r\n",
					"            # Almacenar conteos de filas\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.dim_{table_name}\")\r\n",
					"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
					"            last_result = last_merge_operation.collect()\r\n",
					"\r\n",
					"            if last_result:\r\n",
					"                file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
					"                file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
					"            else:\r\n",
					"                print(f\"No hay registros de operación MERGE para la tabla {file['name']}\")\r\n",
					"                file_result['inserted_rows'] = 0\r\n",
					"                file_result['updated_rows'] = 0\r\n",
					"\r\n",
					"        else: # Crear nueva tabla Delta\r\n",
					"      \r\n",
					"            last_surrogate_key = 0  # Iniciar con 0 si la tabla no existe\r\n",
					"            #sdf = sdf.withColumn(f\"sk{table_name}\", F.monotonically_increasing_id() + last_surrogate_key + 1)  # Asigna un valor único a cada fila.\r\n",
					"            sdf = sdf.withColumn(f\"sk{table_name}\", F.col(f\"id{table_name}\")) # Primera carga sk = id\r\n",
					"            # Añadir las columnas necesarias\r\n",
					"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
					"\r\n",
					"            # Hash\r\n",
					"            columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\" ,'fechaCarga', 'fechaDelta','fromDate', 'toDate', 'isCurrent')]\r\n",
					"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
					"\r\n",
					"            # Reorganizar la estructura de la tabla\r\n",
					"            todas_las_columnas = sdf.columns\r\n",
					"            columnas_al_principio = [f\"sk{table_name}\", f\"id{table_name}\"]\r\n",
					"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
					"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
					"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
					"            sdf_reordenado = sdf.select(*nuevo_orden_columnas)\r\n",
					"            \r\n",
					"            # Crear la tabla Delta\r\n",
					"            sdf_reordenado.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path_dimensions)\r\n",
					"\r\n",
					"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.dim_{table_name} USING DELTA LOCATION \\'{delta_table_path_dimensions}\\'')\r\n",
					"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
					"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
					"\r\n",
					"            # Almacenar archivo procesado correctamente\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
					"            file_result['updated_rows'] = 0\r\n",
					"\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
					"        file_result['exception'] = f\"{e}\"\r\n",
					"        results.append(file_result)\r\n",
					"        continue\r\n",
					"\r\n",
					"    results.append(file_result)"
				],
				"execution_count": 32
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Load Fact Table**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"for file, file2 in zip(filtered_factList, filtered_factList_Lookup):\r\n",
					"    file_result = {\r\n",
					"        'table': f\"fact_{file['name']}\",\r\n",
					"        'status': 'Incorrect',\r\n",
					"        'inserted_rows': 0,\r\n",
					"        'updated_rows': 0,\r\n",
					"        'exception': 'N/A',\r\n",
					"        'datetime': str(datetime.now())  \r\n",
					"    }\r\n",
					"    table_name = file[\"name\"].split('_')[0]\r\n",
					"    key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
					"\r\n",
					"    key_columns = key_columns_str.split(',')\r\n",
					"    conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
					"\r\n",
					"    # Definir la ruta de la tabla Delta\r\n",
					"    delta_table_path_fact = f\"{data_lake_container}/{golden_folder_fact}/{table_name}\"\r\n",
					"    \r\n",
					"    df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") # Hacer lo mismo más adelante con los lookups \r\n",
					"    sdf = df.cache()\r\n",
					"\r\n",
					"    try:\r\n",
					"\r\n",
					"        #source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
					"\r\n",
					"\r\n",
					"        # Verificar si el DataFrame está vacío\r\n",
					"        if sdf.limit(1).count() == 0:\r\n",
					"            print(f\"No se procesará la tabla {file['name']} porque no contiene nuevos datos.\")\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            file_result['exception'] = \"There is no new data\"\r\n",
					"            results.append(file_result)\r\n",
					"            continue\r\n",
					"\r\n",
					"        # Truncar la tabla Delta si existe\r\n",
					"        if DeltaTable.isDeltaTable(spark, delta_table_path_fact):\r\n",
					"            delta_table = DeltaTable.forPath(spark, delta_table_path_fact)\r\n",
					"            #delta_table.delete()  # Truncar la tabla Delta\r\n",
					"\r\n",
					"            #last_row number para incremental\r\n",
					"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
					"            # spark.sql(f\"\"\"\r\n",
					"            #             INSERT INTO gold.fact_{table_name} \r\n",
					"            #             {file2[\"query\"].strip()}\r\n",
					"            #            \"\"\")  # df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
					"\r\n",
					"            sdf =spark.sql(f\"\"\"\r\n",
					"                        {file2[\"query\"].strip()} \r\n",
					"                        \"\"\")\r\n",
					"\r\n",
					"            #max_skSales = 0\r\n",
					"            max_skSales = spark.sql(f\"SELECT COALESCE(MAX(sk{table_name}), 0) AS next_key FROM gold.fact_{table_name}\").collect()[0][\"next_key\"]  # +1???           \r\n",
					"            sdf = sdf.withColumn(\"skSales\", F.expr(f\"{max_skSales} + row_number() OVER (ORDER BY id{table_name})\"))\r\n",
					"\r\n",
					"            columns_to_hash = [col for col in sdf.columns if col not in ('loadDate', 'deltaDate', 'skSales')]\r\n",
					"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
					"            \r\n",
					"            #next_surrogate_key = spark.sql(f\"SELECT COALESCE(MAX(sk{table_name}), 0) + 1 AS next_key FROM gold.dim_{table_name}\").collect()[0][\"next_key\"]\r\n",
					"          \r\n",
					"            sdf.createOrReplaceTempView(\"temp_sales_view_merge\")\r\n",
					"\r\n",
					"            table_schema = spark.table(f\"gold.fact_{table_name}\").schema\r\n",
					"\r\n",
					"            # Listas de columnas para construir dinámicamente la consulta\r\n",
					"            columns = [col for col in sdf.columns if col not in ('skSales', 'hash')]\r\n",
					"\r\n",
					"            # 2. Construir las cláusulas para el INSERT\r\n",
					"            update_set = \", \".join([f\"existing.{col} = updates.{col}\" for col in columns])\r\n",
					"            insert_columns = \", \".join(columns + ['skSales', 'hash'])  # Agregar 'skSales' y 'hash' al final\r\n",
					"            insert_values = \", \".join([f\"updates.{col}\" for col in columns] + ['updates.skSales', 'updates.hash'])  # Asegurarte de que los valores estén en el mismo orden\r\n",
					"\r\n",
					"            # 3. Ejecutar el MERGE o INSERT\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                MERGE INTO gold.fact_{table_name} AS existing\r\n",
					"                USING temp_sales_view_merge AS updates\r\n",
					"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
					"\r\n",
					"                WHEN MATCHED AND existing.hash != updates.hash THEN\r\n",
					"                UPDATE SET {update_set}  -- Aquí se actualizarán las columnas existentes\r\n",
					"\r\n",
					"                WHEN NOT MATCHED THEN\r\n",
					"                INSERT ({insert_columns}) \r\n",
					"                VALUES ({insert_values})\r\n",
					"            \"\"\")\r\n",
					"            \r\n",
					"            # Almacenar conteos de filas\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.fact_{table_name}\")\r\n",
					"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
					"            last_result = last_merge_operation.collect()\r\n",
					"\r\n",
					"            if last_result:\r\n",
					"                file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
					"                file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
					"            else:\r\n",
					"                #print(f\"No hay registros de operación MERGE para la tabla {file['name']}\")\r\n",
					"                file_result['inserted_rows'] = 0\r\n",
					"                file_result['updated_rows'] = 0\r\n",
					"\r\n",
					"        else:\r\n",
					"            \r\n",
					"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
					"            sdf =spark.sql(f\"\"\"\r\n",
					"                        {file2[\"query\"].strip()} \r\n",
					"                        \"\"\")\r\n",
					"\r\n",
					"            max_skSales = 0\r\n",
					"            sdf = sdf.withColumn(\"skSales\", F.expr(f\"{max_skSales} + row_number() OVER (ORDER BY id{table_name})\"))\r\n",
					"\r\n",
					"\r\n",
					"            columns_to_hash = [col for col in sdf.columns if col not in ('loadDate', 'deltaDate', 'skSales')]\r\n",
					"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
					"            \r\n",
					"            sdf.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_fact)\r\n",
					"\r\n",
					"            # Crear la tabla Delta en caso de que no exista\r\n",
					"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.fact_{table_name} USING DELTA LOCATION \\'{delta_table_path_fact}\\'')\r\n",
					"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
					"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
					"            \r\n",
					"        # Almacenar conteos de filas\r\n",
					"        file_result['status'] = 'Correct'\r\n",
					"        file_result['inserted_rows'] = sdf.count()\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
					"        file_result['exception'] = f\"{e}\"\r\n",
					"        results.append(file_result)\r\n",
					"        continue\r\n",
					"\r\n",
					"    #results.append(file_result)"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ######\r\n",
					"# ###### Ahora mismo hardcodeado el codigo para carga Full. \r\n",
					"# ###### En proceso de desarrollar carga incremental, y el append correcto para el lookup de las Sks\r\n",
					"# ######\r\n",
					"\r\n",
					"\r\n",
					"# for file in filtered_factList:\r\n",
					"#     file_result = {\r\n",
					"#         'table': f\"fact_{file['name']}\",\r\n",
					"#         'status': 'Incorrect',\r\n",
					"#         'inserted_rows': 0,\r\n",
					"#         'updated_rows': 0,\r\n",
					"#         'exception': 'N/A',\r\n",
					"#         'datetime': str(datetime.now())  \r\n",
					"#     }\r\n",
					"#     table_name = file[\"name\"].split('_')[0]\r\n",
					"\r\n",
					"#     # Definir la ruta de la tabla Delta\r\n",
					"#     delta_table_path_fact = f\"{data_lake_container}/{golden_folder_fact}/{table_name}\"\r\n",
					"    \r\n",
					"#     df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") # Hacer lo mismo más adelante con los lookcups \r\n",
					"#     sdf = df.cache()\r\n",
					"\r\n",
					"#     try:\r\n",
					"\r\n",
					"#         # Verificar si el DataFrame está vacío\r\n",
					"#         if sdf.limit(1).count() == 0:\r\n",
					"#             print(f\"No se procesará la tabla {file['name']} porque no contiene nuevos datos.\")\r\n",
					"#             file_result['status'] = 'Correct'\r\n",
					"#             file_result['exception'] = \"There is no new data\"\r\n",
					"#             results.append(file_result)\r\n",
					"#             continue\r\n",
					"\r\n",
					"#         # Truncar la tabla Delta si existe\r\n",
					"#         if DeltaTable.isDeltaTable(spark, delta_table_path_fact):\r\n",
					"#             delta_table = DeltaTable.forPath(spark, delta_table_path_fact)\r\n",
					"#             delta_table.delete()  # Truncar la tabla Delta\r\n",
					"\r\n",
					"#             #last_row number para incremental\r\n",
					"\r\n",
					"#             sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
					"#             spark.sql(f\"\"\"\r\n",
					"#                 INSERT INTO gold.fact_{table_name}  \r\n",
					"#                 SELECT ROW_NUMBER() OVER(ORDER BY s.dateOP) AS skSales, s.*, \r\n",
					"#                         COALESCE(a.skArticles, -1) AS skArticles,\r\n",
					"#                         COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
					"#                         COALESCE(cl.skClient, -1) AS skClient, \r\n",
					"#                         COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
					"#                         COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
					"#                         COALESCE(t.skTariff, -1) AS skTariff, \r\n",
					"#                         COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
					"#                         COALESCE(h.skHours, -1) AS skHours,                     \r\n",
					"#                         COALESCE(d.skDate, -1) AS skDate\r\n",
					"#                 FROM temp_sales_view as s\r\n",
					"#                 LEFT JOIN gold.dim_Articles AS a\r\n",
					"#                     ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Client AS cl\r\n",
					"#                     ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Currency AS cu\r\n",
					"#                     ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Date AS d\r\n",
					"#                     ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Hours AS h\r\n",
					"#                     ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
					"#                 LEFT JOIN gold.dim_OperationType AS o\r\n",
					"#                     ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
					"#                 LEFT JOIN gold.dim_PostalCode AS p\r\n",
					"#                     ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Tariff AS t\r\n",
					"#                     ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Warehouse AS w\r\n",
					"#                     ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
					"#                 \"\"\")\r\n",
					"\r\n",
					"#             # Almacenar conteos de filas\r\n",
					"#             file_result['status'] = 'Correct'\r\n",
					"#             history_df = spark.sql(f\"DESCRIBE HISTORY gold.fact_{table_name}\")\r\n",
					"#             last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
					"#             last_result = last_merge_operation.collect()\r\n",
					"\r\n",
					"#             if last_result:\r\n",
					"#                 file_result['inserted_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsInserted\", 0)\r\n",
					"#                 file_result['updated_rows'] = last_result[0].operationMetrics.get(\"numTargetRowsUpdated\", 0)\r\n",
					"#             else:\r\n",
					"#                 #print(f\"No hay registros de operación MERGE para la tabla {file['name']}\")\r\n",
					"#                 file_result['inserted_rows'] = 0\r\n",
					"#                 file_result['updated_rows'] = 0\r\n",
					"\r\n",
					"#         else:\r\n",
					"#             # Hardcodeado, cambiar...\r\n",
					"#             sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
					"#             sdf = spark.sql(f\"\"\" \r\n",
					"#                 SELECT ROW_NUMBER() OVER(ORDER BY s.dateOP) AS skSales, s.*, \r\n",
					"#                         COALESCE(a.skArticles, -1) AS skArticles,\r\n",
					"#                         COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
					"#                         COALESCE(cl.skClient, -1) AS skClient, \r\n",
					"#                         COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
					"#                         COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
					"#                         COALESCE(t.skTariff, -1) AS skTariff, \r\n",
					"#                         COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
					"#                         COALESCE(h.skHours, -1) AS skHours,                     \r\n",
					"#                         COALESCE(d.skDate, -1) AS skDate\r\n",
					"#                 FROM temp_sales_view AS s\r\n",
					"#                 LEFT JOIN gold.dim_Articles AS a\r\n",
					"#                     ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Client AS cl\r\n",
					"#                     ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Currency AS cu\r\n",
					"#                     ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Date AS d\r\n",
					"#                     ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Hours AS h\r\n",
					"#                     ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
					"#                 LEFT JOIN gold.dim_OperationType AS o\r\n",
					"#                     ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
					"#                 LEFT JOIN gold.dim_PostalCode AS p\r\n",
					"#                     ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Tariff AS t\r\n",
					"#                     ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
					"#                 LEFT JOIN gold.dim_Warehouse AS w\r\n",
					"#                     ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
					"#                 \"\"\")\r\n",
					"\r\n",
					"#         # Cargar datos en la tabla Delta\r\n",
					"#             sdf.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_fact)\r\n",
					"#             #sdf.write.format(\"delta\").mode(\"append\").save(delta_table_path)  # valorar carga incremental más adelante\r\n",
					"\r\n",
					"#             # Crear la tabla Delta en caso de que no exista\r\n",
					"#             spark.sql(f'CREATE TABLE IF NOT EXISTS gold.fact_{table_name} USING DELTA LOCATION \\'{delta_table_path_fact}\\'')\r\n",
					"#             spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
					"#             spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
					"            \r\n",
					"#         # Almacenar conteos de filas\r\n",
					"#         file_result['status'] = 'Correct'\r\n",
					"#         file_result['inserted_rows'] = sdf.count()\r\n",
					"\r\n",
					"#     except Exception as e:\r\n",
					"#         print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
					"#         file_result['exception'] = f\"{e}\"\r\n",
					"#         results.append(file_result)\r\n",
					"#         continue\r\n",
					"\r\n",
					"#     results.append(file_result)"
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Guardar ficheros Logs**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Obtener la fecha actual\r\n",
					"# fecha_actual = datetime.now()\r\n",
					"# year = fecha_actual.strftime('%Y') \r\n",
					"# month = fecha_actual.strftime('%m') \r\n",
					"# day = fecha_actual.strftime('%d')  \r\n",
					"# hour = fecha_actual.strftime('%H%M%S') \r\n",
					"\r\n",
					"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
					"#archivo_nombre = f\"Log_{day}_{hour}.json\"\r\n",
					"log_file_name = f\"Log_{executionID}.json\"\r\n",
					"\r\n",
					"# Reemplaza con tu cadena de conexión a Azure\r\n",
					"\r\n",
					"# Cliente de servicio de blobs\r\n",
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
					"\r\n",
					"# Convetimos el diccionario a formato json\r\n",
					"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
					"\r\n",
					"# Crear la ruta de destino con la jerarquía Año/Mes y el nombre del archivo\r\n",
					"destination_blob_name = f\"{gold_folder_logs}/{year}/{month}/{log_file_name}\"\r\n",
					"\r\n",
					"# Crear un cliente del blob de destino\r\n",
					"destination_blob = blob_service_client.get_blob_client(container=container_name, blob=destination_blob_name)\r\n",
					"\r\n",
					"destination_blob.upload_blob(json_data, overwrite=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Actualizar fichero metadata**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Configurar la conexión\r\n",
					"\r\n",
					"#delta_date_string = '1900-01-01'\r\n",
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
					"\r\n",
					"csv_blob_name = \"ConfiguracionReporting.csv\"  # nombre de tu archivo/ se puede poner como parametro\r\n",
					"\r\n",
					"# Leer el archivo CSV desde el Data Lake\r\n",
					"blob_client = blob_service_client.get_blob_client(container=metadata_folder, blob=csv_blob_name)\r\n",
					"\r\n",
					"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
					"df = pd.read_csv(stream, sep=',')  # Especifica el separador aquí\r\n",
					"\r\n",
					"# Limpiar nombres de columnas para eliminar espacios en blanco\r\n",
					"df.columns = df.columns.str.strip()\r\n",
					"\r\n",
					"names_dimensionsList = [item['name'] for item in filtered_dimensionsList]\r\n",
					"for i in names_dimensionsList:\r\n",
					"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas según tu archivo CSV\r\n",
					"\r\n",
					"names_factList = [item['name'] for item in filtered_factList]\r\n",
					"for i in names_factList:\r\n",
					"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas según tu archivo CSV\r\n",
					"\r\n",
					"# Guardar el DataFrame modificado en un nuevo flujo de bytes\r\n",
					"output_stream = io.BytesIO()\r\n",
					"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')  # Usar line_terminator para evitar líneas en blanco\r\n",
					"output_stream.seek(0)  # Regresar al inicio del flujo\r\n",
					"\r\n",
					"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
				],
				"execution_count": 18
			}
		]
	}
}