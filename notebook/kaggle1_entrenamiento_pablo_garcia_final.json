{
	"name": "kaggle1_entrenamiento_pablo_garcia_final",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "9131a7fa-a006-40d0-b834-5d5e20e76a5b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Primera entrega Kaggle\n",
					"\n",
					"**Pablo García Caballero**"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"Importamos librerias"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import pandas as pd\n",
					"import numpy as np\n",
					"import pickle\n",
					"import re\n",
					"\n",
					"# Transformación de datos\n",
					"from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder #, TargetEncoder\n",
					"#import category_encoders as ce\n",
					"from category_encoders import TargetEncoder\n",
					"\n",
					"from sklearn.impute import SimpleImputer\n",
					"from sklearn.compose import ColumnTransformer\n",
					"\n",
					"# Modelos\n",
					"from sklearn.tree import DecisionTreeClassifier\n",
					"import lightgbm as lgb\n",
					"\n",
					"# Seleccion de variables y tuning de hiperparámetros\n",
					"from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
					"\n",
					"# Métricas para evaluar un modelo de clasificación\n",
					"from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve, auc, roc_curve, roc_auc_score, average_precision_score, confusion_matrix\n",
					"\n",
					"# Librerías para visualización de resultados\n",
					"import matplotlib.pyplot as plt\n",
					"import seaborn as sns\n",
					"\n",
					"pd.set_option('display.max_columns', None)\n",
					"pd.set_option('display.max_rows', None)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"source": [
					"Definimos la función disponible en el campus para clasificar las columnas"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def guess_type(column_name:str , df:pd.DataFrame, m=None, original_df=None) -> str:\n",
					"\n",
					"    if df[column_name].dtype == 'O':\n",
					"        return 'text'\n",
					"    try:\n",
					"        if m is not None and bool(m.get_all(name=column_name)):\n",
					"            return m.get_all(name=column_name).type[0]\n",
					"        elif m is not None and bool(m.get_all(name=column_name[:-2])) and original_df is not None:\n",
					"            #lets check if this is a date:\n",
					"            original_colname = column_name[:-2]\n",
					"            atype = original_df[original_colname].dtype\n",
					"            typ = str(atype)\n",
					"            if re.search(\"^date.\", typ):\n",
					"                return 'categorical'\n",
					"            if atype == 'O':\n",
					"                row_sample = original_df[original_colname].iloc[0]\n",
					"                if is_date(row_sample):\n",
					"                    return 'categorical'\n",
					"\n",
					"        #if isinstance(row_sample, float):\n",
					"        #    return 'numerical'\n",
					"        if df[column_name].dtype in ['float64', 'float32', 'float16']:\n",
					"            # we have to check if column was wrong catalogued as numerical\n",
					"            grouped = df[column_name].unique()\n",
					"            if len(grouped) < 10:\n",
					"                #check there are no decimals\n",
					"                for v in grouped:\n",
					"                    are_decimals = v - int(v)\n",
					"                    if are_decimals > 0.0:\n",
					"                        return 'numerical'\n",
					"                return 'categorical'\n",
					"            return 'numerical'\n",
					"\n",
					"        elif df[column_name].dtype in ['int64', 'int32', 'int16', 'int8']:\n",
					"            max = df[column_name].max()\n",
					"            min = df[column_name].min()\n",
					"            diff = max - min\n",
					"            diff2 = int(max) - int(min)\n",
					"            if diff == diff2:\n",
					"                if diff < 10:\n",
					"                    grouped = df[column_name].unique()\n",
					"                    if len(grouped)< 10:\n",
					"                        return 'categorical'\n",
					"            return \"numerical\"\n",
					"        else:\n",
					"            return \"categorical\"\n",
					"    except Exception as e:\n",
					"        return 'categorical'"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"source": [
					"Carga de datos"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#dfTrain = pd.read_csv(\"MasterBI_Train.csv\", low_memory=False)\n",
					"dfTrain = pd.read_csv(\"D:\\Master BI\\ML\\MasterBI_Train.csv\", low_memory=False)\n",
					"label = \"HasDetections\""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"dfTrain.head()"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"source": [
					"**EDA**"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"Selección de variables. Identificamos variables que son identificadores y variables que dada su información pueden ir juntas."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# id_colums = [\"MachineIdentifier\"]#, \"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]\n",
					"# dfTrain.drop(columns=id_colums, inplace=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dfTrain[[\"Census_OSEdition\", \"Census_OSSkuName\"]].drop_duplicates().head(20) #.sort_values(by=\"Census_OSEdition\")\n",
					""
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"dfTrain[[\"Platform\", \"Processor\"]].drop_duplicates().sort_values(by=\"Platform\")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"dfTrain[[\"ProductName\", \"EngineVersion\", \"IsBeta\"]].drop_duplicates().sort_values(by=\"EngineVersion\").head(20)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"Como tienen relación vamos a unifircarlas, y despues a dropearlas"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dfTrain[\"ProductNameIsBetaEngineVersion\"] = dfTrain[\"ProductName\"] + '_' + dfTrain[\"IsBeta\"].astype(str) + '_' + dfTrain[\"EngineVersion\"]\n",
					"\n",
					"dfTrain[\"PlatformProcessor\"] = dfTrain[\"Platform\"] + '_' + dfTrain[\"Processor\"] \n",
					"\n",
					"# Eliminar las columnas originales \n",
					"#dfTrain.drop([\"ProductName\", \"EngineVersion\", \"IsBeta\", \"Platform\", \"Processor\", \"Census_OSSkuName\"], axis=1, inplace=True)\n",
					"dfTrain.drop([\"MachineIdentifier\", \"ProductName\", \"EngineVersion\", \"IsBeta\", \"Platform\", \"Processor\"], axis=1, inplace=True)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"dfTrain.info()"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"Agrupamos las columnas que sean identificadores para posteriormermente elimarlas del df, ya que no son utiles para el entramiento"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# id_colums = [\"MachineIdentifier\"]#, \"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]\n",
					"# dfTrain.drop(columns=id_colums, inplace=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"Convertimos a categoricas los campos que correspondan en función de la función guess_type"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"for i in dfTrain.columns:\n",
					"    print(f\"{i} = {guess_type(i, dfTrain)}\")\n",
					"    if guess_type(i, dfTrain) == \"categorical\":\n",
					"        dfTrain[i] = dfTrain[i].astype('category')\n",
					"        print(\"convertido\")\n",
					"    else:\n",
					"        print(\"no\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"Imputación de nulos mediante la media para las numericas y mediante la moda para las categoricas"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"columnas_cat = dfTrain.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
					"columnas_num = dfTrain.select_dtypes(include=[\"float64\", \"int64\"]).columns.to_list()"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"print(\"Numericas:\", columnas_cat)\n",
					"print(\"Categorícas:\", columnas_num)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"source": [
					"Eliminamos el label de nuestro conjunto de categóricas"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"if label in columnas_cat:\n",
					"    columnas_cat.remove(label)\n",
					"    print(\"Label borrado de las cat\")\n",
					"else:\n",
					"    columnas_num.remove(label)\n",
					"    print(\"Label borrado de las num\")"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
					"imp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"source": [
					"Preparamos la codificación de variables"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"columnas_todas = columnas_cat+columnas_num \n",
					"dfTrain = dfTrain[columnas_todas+['HasDetections']]"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"source": [
					"encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)"
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Modelo**"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"Dividimos el df en train y test"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
					"from sklearn.pipeline import Pipeline\n",
					"from sklearn.compose import ColumnTransformer\n",
					"from sklearn.impute import SimpleImputer\n",
					"from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
					"from lightgbm import LGBMClassifier\n",
					"from sklearn.metrics import accuracy_score, classification_report\n",
					"from scipy.stats import randint, uniform\n",
					"\n",
					"# Dividir el conjunto de datos\n",
					"X = dfTrain.drop(label, axis=1)\n",
					"y = dfTrain[label]\n",
					"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
					"\n",
					"# Separar columnas numéricas y categóricas\n",
					"numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
					"categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
					"\n",
					"# Preprocesamiento para variables numéricas\n",
					"numeric_transformer = Pipeline(steps=[\n",
					"    ('imputer', SimpleImputer(strategy='mean')),  # Imputar con la media\n",
					"    ('scaler', StandardScaler())                 # Estandarizar\n",
					"])\n",
					"\n",
					"# Preprocesamiento para variables categóricas\n",
					"categorical_transformer = Pipeline(steps=[\n",
					"    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputar con el valor más frecuente\n",
					"    ('encoder', OneHotEncoder(handle_unknown='ignore'))    # Codificar variables categóricas\n",
					"])\n",
					"\n",
					"# Combinación de transformaciones\n",
					"preprocessor = ColumnTransformer(\n",
					"    transformers=[\n",
					"        ('num', numeric_transformer, numeric_features),\n",
					"        ('cat', categorical_transformer, categorical_features)\n",
					"    ])\n",
					"\n",
					"# Crear el pipeline completo\n",
					"pipeline = Pipeline(steps=[\n",
					"    ('preprocessor', preprocessor),               # Paso de preprocesamiento\n",
					"    ('classifier', LGBMClassifier(random_state=42))  # Modelo\n",
					"])\n",
					"\n",
					"# Espacio de búsqueda de hiperparámetros\n",
					"param_distributions = {\n",
					"    'classifier__n_estimators': randint(50, 500),\n",
					"    'classifier__learning_rate': uniform(0.01, 0.3),\n",
					"    'classifier__max_depth': randint(3, 15),\n",
					"    'classifier__num_leaves': randint(20, 150),\n",
					"    'classifier__colsample_bytree': uniform(0.5, 0.5),\n",
					"    'classifier__subsample': uniform(0.5, 0.5),\n",
					"    'classifier__lambda_l1': uniform(0.0, 1.0),\n",
					"    'classifier__lambda_l2': uniform(0.0, 1.0),\n",
					"    'classifier__min_child_samples': randint(10, 100)\n",
					"}\n",
					"\n",
					"# RandomizedSearchCV\n",
					"random_search = RandomizedSearchCV(\n",
					"    pipeline,\n",
					"    param_distributions=param_distributions,\n",
					"    n_iter=50,              # Número de iteraciones\n",
					"    scoring='accuracy',     # Métrica de evaluación\n",
					"    cv=5,                   # Número de particiones para validación cruzada\n",
					"    verbose=2,\n",
					"    random_state=42,\n",
					"    n_jobs=-1               # Paralelización\n",
					")\n",
					"\n",
					"# Entrenar RandomizedSearchCV\n",
					"random_search.fit(X_train, y_train)\n",
					"\n",
					"# Mejor modelo\n",
					"best_model = random_search.best_estimator_\n",
					"print(\"Mejores parámetros:\", random_search.best_params_)\n",
					"\n",
					"# Evaluar el mejor modelo\n",
					"y_pred = best_model.predict(X_test)\n",
					"print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
					"print(classification_report(y_test, y_pred))\n",
					""
				],
				"execution_count": 60
			},
			{
				"cell_type": "code",
				"source": [
					"X = dfTrain.drop(label, axis=1)\n",
					"y = dfTrain[label]\n",
					"\n",
					"# Dividir los datos en conjuntos de entrenamiento y prueba\n",
					"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) "
				],
				"execution_count": 57
			},
			{
				"cell_type": "markdown",
				"source": [
					"Imputación de nulos"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"X_train[columnas_num] = imp_num.fit_transform(X_train[columnas_num])\n",
					"X_train[columnas_cat] = imp_cat.fit_transform(X_train[columnas_cat])\n",
					"#X_train.isnull().sum()"
				],
				"execution_count": 58
			},
			{
				"cell_type": "code",
				"source": [
					"X_test[columnas_num] = imp_num.transform(X_test[columnas_num])\n",
					"X_test[columnas_cat] = imp_cat.transform(X_test[columnas_cat])\n",
					"#X_test.isnull().sum()"
				],
				"execution_count": 59
			},
			{
				"cell_type": "markdown",
				"source": [
					"Aplicamos el encoder"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Fit en el conjunto de entrenamiento\n",
					"X_train[columnas_cat] = encoder.fit_transform(X_train[columnas_cat]) #.astype(str))\n",
					"\n",
					"# Transformar el conjunto de prueba\n",
					"X_test[columnas_cat] = encoder.transform(X_test[columnas_cat]) #.astype(str))\n",
					""
				],
				"execution_count": 60
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
					"\n",
					"scaler = StandardScaler()\n",
					"X_train_scaled_num = scaler.fit_transform(X_train[columnas_num])\n",
					"X_test_scaled_num = scaler.transform(X_test[columnas_num])\n",
					"\n",
					"# Convertir resultados escalados a DataFrame\n",
					"X_train_scaled_num = pd.DataFrame(X_train_scaled_num, columns=columnas_num, index=X_train.index)\n",
					"X_test_scaled_num = pd.DataFrame(X_test_scaled_num, columns=columnas_num, index=X_test.index)\n",
					"\n",
					"\n",
					"# Combinar los resultados\n",
					"X_train_final = pd.concat([X_train_scaled_num, X_train[columnas_cat]], axis=1)\n",
					"X_test_final = pd.concat([X_test_scaled_num, X_test[columnas_cat]], axis=1)\n",
					""
				],
				"execution_count": 61
			},
			{
				"cell_type": "code",
				"source": [
					"X_train_final.head()"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"source": [
					"Convertimos a df de nuevo"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"Entrenamos el modelo"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.svm import SVC\n",
					"clf = SVC(gamma='auto')\n",
					"clf.fit(X_train_final, y_train)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.ensemble import RandomForestRegressor\n",
					"clf = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
					"#clf = RandomForestRegressor()\n",
					"clf.fit(X_train_final, y_train)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.ensemble import RandomForestClassifier\n",
					"\n",
					"from sklearn.feature_selection import SelectFromModel\n",
					"from sklearn.ensemble import RandomForestClassifier\n",
					"\n",
					"# SelectFromModel with Random Forest Classifier\n",
					"#selector = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
					"#X_selected = selector.fit_transform(X, y)\n",
					"\n",
					"#clf = RandomForestClassifier()\n",
					"selector = SelectFromModel(RandomForestClassifier(\n",
					"    n_estimators=100,           # Más árboles para mayor estabilidad\n",
					"    max_depth=20,               # Limitar la profundidad para evitar sobreajuste\n",
					"    min_samples_split=10,       # Evita divisiones excesivas\n",
					"    min_samples_leaf=5,         # Hoja con mínimo 5 datos\n",
					"    max_features='sqrt',        # Variables consideradas por división\n",
					"    max_samples=0.8,            # Muestreo aleatorio del 80% de los datos\n",
					"    class_weight='balanced',    # Balanceo de clases\n",
					"    criterion='entropy',        # Medida de calidad de división\n",
					"    bootstrap=True,             # Muestreo con reemplazo\n",
					"    random_state=42,            # Reproducibilidad\n",
					"    n_jobs=-1                   # Paralelización\n",
					"))\n",
					"selector.fit(X_train_final.values, y_train.values)\n",
					"\n",
					"X_train_final = selector.transform(X_train_final)\n",
					"X_test_final = selector.transform(X_test_final)\n",
					"\n",
					"clf = RandomForestClassifier(\n",
					"    n_estimators=500,           # Más árboles para mayor estabilidad\n",
					"    max_depth=20,               # Limitar la profundidad para evitar sobreajuste\n",
					"    min_samples_split=10,       # Evita divisiones excesivas\n",
					"    min_samples_leaf=5,         # Hoja con mínimo 5 datos\n",
					"    max_features='sqrt',        # Variables consideradas por división\n",
					"    max_samples=0.8,            # Muestreo aleatorio del 80% de los datos\n",
					"    class_weight='balanced',    # Balanceo de clases\n",
					"    criterion='entropy',        # Medida de calidad de división\n",
					"    bootstrap=True,             # Muestreo con reemplazo\n",
					"    random_state=42,            # Reproducibilidad\n",
					"    n_jobs=-1                   # Paralelización\n",
					")\n",
					"clf.fit(X_train_final, y_train.values)"
				],
				"execution_count": 62
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.model_selection import RandomizedSearchCV, KFold\n",
					"from scipy.stats import uniform\n",
					"import lightgbm as lgb\n",
					"\n",
					"# Define los hiperparámetros y sus rangos\n",
					"param_dist = {\n",
					"    'n_estimators': [100, 200, 500, 1000],         # Número de árboles\n",
					"    'max_depth': [6, 8, 10, 12],                  # Profundidad máxima\n",
					"    'num_leaves': [20, 50, 80, 100],              # Número de hojas\n",
					"    'learning_rate': uniform(0.01, 0.2),          # Tasa de aprendizaje\n",
					"    'colsample_bytree': uniform(0.6, 0.4),        # Proporción de columnas por árbol\n",
					"    'subsample': uniform(0.6, 0.4),               # Proporción de filas\n",
					"    'lambda_l1': uniform(0.0, 1.0),               # Regularización L1\n",
					"    'lambda_l2': uniform(0.0, 2.0),               # Regularización L2\n",
					"    'min_child_samples': [10, 20, 30, 50]         # Tamaño mínimo de datos en hojas\n",
					"}\n",
					"\n",
					"# Configura el modelo base\n",
					"clf = lgb.LGBMClassifier(random_state=42)\n",
					"\n",
					"# Configura la validación cruzada (KFold)\n",
					"kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
					"\n",
					"# Configura la búsqueda aleatoria\n",
					"random_search = RandomizedSearchCV(\n",
					"    estimator=clf,\n",
					"    param_distributions=param_dist,\n",
					"    n_iter=50,                    # Número de combinaciones aleatorias a probar\n",
					"    scoring='accuracy',           # Métrica de evaluación\n",
					"    cv=kfold,                     # Validación cruzada\n",
					"    verbose=2,                    # Mensajes de progreso\n",
					"    random_state=42,\n",
					"    n_jobs=-1                     # Usa todos los núcleos disponibles\n",
					")\n",
					"\n",
					"# Realiza la búsqueda\n",
					"random_search.fit(X, y)\n",
					"\n",
					"# Imprime los mejores hiperparámetros\n",
					"print(\"Mejores hiperparámetros encontrados:\", random_search.best_params_)\n",
					"print(\"Mejor precisión obtenida:\", random_search.best_score_)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# import lightgbm as lgb\n",
					"# #clf = lgb.LGBMClassifier(n_estimators=250, max_depth=7,num_leaves=80,learning_rate=0.02,\n",
					"# #colsample_bytree=0.8, ubsample=0.9,lambda_l1=0.2,lambda_l2=2.0,min_child_samples=40,min_split_gain=0.05)\n",
					"# param = {'classifier__colsample_bytree': 0.5066324805799333, 'classifier__lambda_l1': 0.9422017556848528, 'classifier__lambda_l2': 0.5632882178455393, 'classifier__learning_rate': 0.12562495076197483, 'classifier__max_depth': 12, 'classifier__min_child_samples': 62, 'classifier__n_estimators': 435, 'classifier__num_leaves': 103, 'classifier__subsample': 0.6205127330130058}\n",
					"# clf = lgb.LGBMClassifier(**param)\n",
					"\n",
					"# clf.fit(X_train_final, y_train)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Predición y evaluación**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.metrics import confusion_matrix\n",
					"import seaborn as sns\n",
					"import matplotlib.pyplot as plt\n",
					"\n",
					"# Predicciones del modelo\n",
					"prediciones_2 = clf.predict(X_test_final)\n",
					"pred_proba_2 = clf.predict_proba(X_test_final)\n",
					"\n",
					"# Crear la matriz de confusión\n",
					"data = confusion_matrix(y_test.values, prediciones_2)\n",
					"\n",
					"# Visualizar la matriz de confusión\n",
					"print(\"Matriz de confusión\")\n",
					"plt.figure(figsize=(6, 5))\n",
					"sns.set(font_scale=1.4)  # Escala del texto\n",
					"sns.heatmap(data, cmap=\"Blues\", annot=True, fmt='g', annot_kws={\"size\":12})  # fmt='g' para números enteros\n",
					"plt.xlabel(\"Predicciones\", fontsize=14)\n",
					"plt.ylabel(\"Valores Reales\", fontsize=14)\n",
					"plt.title(\"Matriz de Confusión\", fontsize=16)\n",
					"plt.show()\n",
					"\n",
					"print(f\"Acurracy: {round(100*accuracy_score(y_test, prediciones_2),1)}% \\n\")\n",
					"print(classification_report(y_test, prediciones_2, digits=3, zero_division=True))"
				],
				"execution_count": 64
			},
			{
				"cell_type": "markdown",
				"source": [
					"Curva ROC y AUX"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"fpr, tpr, _ = roc_curve(y_test, pred_proba_2[:,1])\n",
					"roc_auc = auc(fpr, tpr)\n",
					"\n",
					"plt.figure(figsize = (6,5))\n",
					"lw = 2\n",
					"plt.plot(fpr, tpr, color='darkorange',\n",
					"         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
					"plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
					"plt.xlim([0.0, 1.0])\n",
					"plt.ylim([0.0, 1.05])\n",
					"plt.xlabel('False Positive Rate')\n",
					"plt.ylabel('True Positive Rate')\n",
					"plt.title('Receiver operating characteristic example')\n",
					"plt.legend(loc=\"lower right\")\n",
					"plt.show()"
				],
				"execution_count": 65
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Pase a producción**"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"Replicamos los pasos anteriores, pero sin dividir el df en test/train"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dfTrainFull = pd.read_csv(\"MasterBI_Train.csv\", low_memory=False)\n",
					"label = \"HasDetections\""
				],
				"execution_count": 97
			},
			{
				"cell_type": "code",
				"source": [
					"id_colums = [\"MachineIdentifier\"]#, \"ProductName\", \"EngineVersion\", \"AppVersion\", \"AvSigVersion\"]\n",
					"dfTrainFull.drop(columns=id_colums, inplace=True)"
				],
				"execution_count": 98
			},
			{
				"cell_type": "code",
				"source": [
					"for i in dfTrainFull.columns:\n",
					"    print(f\"{i} = {guess_type(i, dfTrainFull)}\")\n",
					"    if guess_type(i, dfTrainFull) == \"categorical\":\n",
					"        dfTrainFull[i] = dfTrainFull[i].astype('category')\n",
					"        print(\"convertido\")\n",
					"    else:\n",
					"        print(\"no\")"
				],
				"execution_count": 99
			},
			{
				"cell_type": "code",
				"source": [
					"columnas_cat = dfTrainFull.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
					"columnas_num = dfTrainFull.select_dtypes(include=[\"float64\", \"int64\"]).columns.to_list()"
				],
				"execution_count": 100
			},
			{
				"cell_type": "code",
				"source": [
					"columnas_cat.remove(label)"
				],
				"execution_count": 101
			},
			{
				"cell_type": "code",
				"source": [
					"imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
					"dfTrainFull[columnas_num] = imp_num.fit_transform(dfTrainFull[columnas_num])"
				],
				"execution_count": 102
			},
			{
				"cell_type": "code",
				"source": [
					"imp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
					"dfTrainFull[columnas_cat] = imp_cat.fit_transform(dfTrainFull[columnas_cat])"
				],
				"execution_count": 103
			},
			{
				"cell_type": "code",
				"source": [
					"cat_num = columnas_cat+columnas_num \n",
					"dfTrainFull = dfTrainFull[cat_num+['HasDetections']]"
				],
				"execution_count": 104
			},
			{
				"cell_type": "code",
				"source": [
					"te = TargetEncoder()"
				],
				"execution_count": 105
			},
			{
				"cell_type": "code",
				"source": [
					"X = dfTrainFull.drop(label, axis=1)\n",
					"y = dfTrainFull[label]"
				],
				"execution_count": 106
			},
			{
				"cell_type": "code",
				"source": [
					"X_train_prep = te.fit_transform(X, y)"
				],
				"execution_count": 107
			},
			{
				"cell_type": "code",
				"source": [
					"columnas = dfTrain.columns.to_list()\n",
					"columnas.remove(label)"
				],
				"execution_count": 108
			},
			{
				"cell_type": "code",
				"source": [
					"dfX_train_prep = pd.DataFrame(X_train_prep , columns=columnas)"
				],
				"execution_count": 110
			},
			{
				"cell_type": "code",
				"source": [
					"model = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
					"model.fit(dfX_train_prep, y)"
				],
				"execution_count": 111
			},
			{
				"cell_type": "markdown",
				"source": [
					"Por último, utilizando las funcionalidades de la librería pickle, guardamos todo aquello ques nos va a hacer falta para la inferencia con los datos de test"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Modelo\n",
					"with open(\"output/modelo.pkl\", \"wb\") as c:\n",
					"    pickle.dump(model, c)\n",
					"\n",
					"# Transformación\n",
					"with open(\"output/num_imputer.pkl\", \"wb\") as c:\n",
					"    pickle.dump(imp_num, c)\n",
					"with open(\"output/cat_imputer.pkl\", \"wb\") as c:\n",
					"    pickle.dump(imp_cat, c)\n",
					"with open(\"output/encoder.pkl\", \"wb\") as c:\n",
					"    pickle.dump(te, c)\n",
					"\n",
					"# Lista de todas columnas\n",
					"todas_columnas = dfTrainFull.columns.to_list()\n",
					"with open(\"output/columns.pkl\", \"wb\") as c:\n",
					"    pickle.dump(todas_columnas, c)\n",
					"\n",
					"# Columnas Cat\n",
					"with open(\"output/cat_columns.pkl\", \"wb\") as c:\n",
					"     pickle.dump(columnas_cat, c)\n",
					"\n",
					"# Columnas Num\n",
					"with open(\"output/num_columns.pkl\", \"wb\") as c:\n",
					"     pickle.dump(columnas_num, c)"
				],
				"execution_count": null
			}
		]
	}
}