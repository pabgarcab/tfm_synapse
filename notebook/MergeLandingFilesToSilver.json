{
	"name": "MergeLandingFilesToSilver",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkTFM",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4513f2f5-24c3-4de3-b591-c9912767b4b1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
				"name": "sparkTFM",
				"type": "Spark",
				"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# path of the data lake container (bronze and silver for this example)\r\n",
					"#data_lake_container = 'abfss://mdw@datalake1pgc.dfs.core.windows.net'\r\n",
					"# The ingestion folder where your parquet file are located\r\n",
					"#bronze_landing = 'bronze/Landing/Pending'\r\n",
					"# The silver folder where your Delta Tables will be stored\r\n",
					"#silver_folder = 'silver/DeltaTables'\r\n",
					"# The name of the table\r\n",
					"#table_name = 'color'\r\n",
					"# The wildcard filter used within the bronze folder to find files\r\n",
					"#source_wildcard = 'color*.parquet'\r\n",
					"# A comma separated string of one or more key columns (for the merge)\r\n",
					"#key_columns_str = 'idColor'"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# %%sql\r\n",
					"# USE default;\r\n",
					"# DROP DATABASE  silver CASCADE ;\r\n",
					"# USE default;\r\n",
					"# CREATE DATABASE IF NOT EXISTS silver;\r\n",
					"# USE silver;"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Import modules**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import modules\r\n",
					"from delta.tables import DeltaTable\r\n",
					"from delta.tables import *\r\n",
					"from pyspark.sql.functions import when, lit, current_date, date_format, concat_ws, xxhash64\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
					"import re\r\n",
					"import json\r\n",
					"from datetime import datetime\r\n",
					"import pandas as pd\r\n",
					"import io\r\n",
					"from pyspark.sql import functions as F"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Configuration**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
					"# Fecha actual en formato yyyymmdd\r\n",
					"\r\n",
					"fecha_hoy = datetime.now()\r\n",
					"# Formato cadena 'yyyy-mm-dd'\r\n",
					"delta_date_filter = (fecha_hoy.strftime('%Y%m%d'))\r\n",
					"#delta_date_filter = '20241008'\r\n",
					"\r\n",
					"# Formato cadena 'yyyy-mm-dd'\r\n",
					"delta_date_string = fecha_hoy.strftime('%Y-%m-%d')\r\n",
					"\r\n",
					"landing_files = json.loads(landing_files)  # Convertimos en array la variable\r\n",
					"\r\n",
					"# Inicializar listas para almacenar resultados\r\n",
					"results = []\r\n",
					"correct_files = []\r\n",
					"incorrect_files = []"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Load/Merge parquet Landing files to silver**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"for file in landing_files:\r\n",
					"    file_result = {  # OPCIONAL: añadir timestamp\r\n",
					"        'table': file[\"name\"][:file[\"name\"].index('_')],  \r\n",
					"        'file_name': file['name'],\r\n",
					"        'status': 'Incorrecto',\r\n",
					"        'inserted_rows': 0,\r\n",
					"        'updated_rows': 0,\r\n",
					"        'exception': 'N/A',\r\n",
					"        'datetime': str(datetime.now())  \r\n",
					"    }\r\n",
					"\r\n",
					"    try:\r\n",
					"        table_name = file[\"name\"][:file[\"name\"].index('_')]  # Extrae la subcadena\r\n",
					"        source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
					"        key_columns_str = f\"id{table_name}\" \r\n",
					"\r\n",
					"        source_path = data_lake_container + '/' + bronze_landing + '/' + source_wildcard\r\n",
					"        delta_table_path = data_lake_container + '/' + silver_folder + '/' + table_name\r\n",
					"        sdf = spark.read.format('parquet').option(\"recursiveFileLookup\", \"true\").load(source_path)\r\n",
					"        sdf.cache()\r\n",
					"\r\n",
					"        if sdf.limit(1).count() == 0:\r\n",
					"            print(f\"No se procesará el archivo {file['name']} porque no contiene datos.\")\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            file_result['exception'] = \"File with no data\"\r\n",
					"            results.append(file_result)\r\n",
					"            correct_files.append(file['name'])\r\n",
					"            continue\r\n",
					"\r\n",
					"        key_columns = key_columns_str.split(',')\r\n",
					"        #key_columns = key_columns_str\r\n",
					"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
					"\r\n",
					"\r\n",
					"        # Leer archivo(s) en DataFrame de Spark\r\n",
					"\r\n",
					"        sdf = sdf.drop(\"fechaActualizacion\", \"pipelineID\")  # Eliminar columnas no deseadas\r\n",
					"\r\n",
					"        # Eliminar columna no deseada\r\n",
					"        #sdf = sdf.drop(\"fechaActualizacion\")\r\n",
					"\r\n",
					"        # Comprobar si la tabla Delta existe\r\n",
					"        if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
					"            delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
					"            existing_columns = delta_table.toDF().columns\r\n",
					"            \r\n",
					"            # Añadir columna fechaDelta\r\n",
					"            sdf = sdf.withColumn(\"fechaDelta\", lit(delta_date_filter))\r\n",
					"\r\n",
					"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta')]\r\n",
					"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
					"\r\n",
					"            # Crear conjunto de actualizaciones excluyendo 'fechaCarga'\r\n",
					"            update_set = {f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col != 'fechaCarga'}\r\n",
					"\r\n",
					"            # Merge new data into existing table y captura el conteo\r\n",
					"            merge_result = delta_table.alias(\"existing\").merge(\r\n",
					"                source=sdf.alias(\"updates\"),\r\n",
					"                condition=\" AND \".join(conditions_list)\r\n",
					"            ).whenMatchedUpdate(\r\n",
					"                condition=\" AND \".join(conditions_list + [\"existing.hash != updates.hash\"]),\r\n",
					"                set={\r\n",
					"                    #\"fechaCarga\": \"existing.fechaCarga\",  #updates\r\n",
					"                    \"fechaDelta\": delta_date_filter,\r\n",
					"                    **{f\"existing.{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}\r\n",
					"                }\r\n",
					"            ).whenNotMatchedInsert(\r\n",
					"                values={\r\n",
					"                    \"fechaCarga\": \"updates.fechaCarga\",\r\n",
					"                    \"fechaDelta\": delta_date_filter,\r\n",
					"                    **{f\"{col}\": f\"updates.{col}\" for col in existing_columns if col not in ('fechaCarga', 'fechaDelta')}\r\n",
					"                }\r\n",
					"            ).execute()\r\n",
					"\r\n",
					"            # Almacenar conteos de filas\r\n",
					"            # file_result['status'] = 'Correcto'\r\n",
					"            # file_result['inserted_rows'] = merge_result.get('numInserted', 0)  # Filas insertadas\r\n",
					"            # file_result['updated_rows'] = merge_result.get('numUpdated', 0)  # Filas actualizadas\r\n",
					"            #correct_files.append(file['name'])  # Agregar a la lista de archivos correctos\r\n",
					"\r\n",
					"            # Almacenar conteos de filas\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            history_df = spark.sql(f\"DESCRIBE HISTORY silver.{table_name}\")\r\n",
					"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
					"            last_result = last_merge_operation.collect()\r\n",
					"\r\n",
					"            file_result['inserted_rows'] = last_result[0].operationMetrics[\"numTargetRowsInserted\"]\r\n",
					"            file_result['updated_rows'] = last_result[0].operationMetrics[\"numTargetRowsUpdated\"]\r\n",
					"            correct_files.append(file['name'])  # Agregar a la lista de archivos correctos\r\n",
					"\r\n",
					"        else:\r\n",
					"            # Crear nueva tabla Delta\r\n",
					"            sdf = sdf.withColumn(\"fechaCarga\", sdf.fechaCarga)  # Mantener fechaCarga\r\n",
					"            sdf = sdf.withColumn(\"fechaDelta\", lit(delta_date_filter))  # Establecer fechaDelta\r\n",
					"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fechaDelta')]\r\n",
					"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
					"            sdf.write.format('delta').save(delta_table_path)\r\n",
					"\r\n",
					"            spark.sql(f'CREATE TABLE IF NOT EXISTS silver.{table_name} USING DELTA LOCATION \\'{delta_table_path}\\'')\r\n",
					"            spark.sql(f\"ALTER TABLE silver.{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
					"            spark.sql(f\"ALTER TABLE silver.{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
					"\r\n",
					"        \r\n",
					"            # Almacenar archivo procesado correctamente\r\n",
					"            file_result['status'] = 'Correcto'\r\n",
					"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
					"            file_result['updated_rows'] = 0  # Ninguna fila actualizada en la creación\r\n",
					"            correct_files.append(file['name'])  # Agregar a la lista de archivos correctos\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
					"        # Estado ya configurado como 'Incorrecto'\r\n",
					"        file_result['exception'] = f\"{e}\"\r\n",
					"        results.append(file_result)  # Agregar resultado de fallo al final\r\n",
					"        incorrect_files.append(file['name'])  # Agregar a la lista de archivos incorrectos\r\n",
					"        continue  # Continuar con la siguiente iteración\r\n",
					"\r\n",
					"    results.append(file_result)  # Agregar resultado al final del bucle\r\n",
					"\r\n",
					"\r\n",
					"# Imprimir resultados en consola\r\n",
					"for result in results:\r\n",
					"    print(f\"Archivo: {result['file_name']}, Estado: {result['status']}, Filas insertadas: {result['inserted_rows']}, Filas actualizadas: {result['updated_rows']}\")\r\n",
					"\r\n",
					"# Guardar nombres de archivos correctos e incorrectos en variables de la pipeline\r\n",
					"correct_files_variable = correct_files\r\n",
					"incorrect_files_variable = incorrect_files\r\n",
					"\r\n",
					"# Si necesitas ver las variables\r\n",
					"print(\"Archivos correctos:\", correct_files_variable)\r\n",
					"print(\"Archivos incorrectos:\", incorrect_files_variable)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Save log merge/load**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Obtener la fecha actual\r\n",
					"fecha_actual = datetime.now()\r\n",
					"year = fecha_actual.strftime('%Y')  # Año actual (por ejemplo, '2024')\r\n",
					"month = fecha_actual.strftime('%m')  # Mes actual (por ejemplo, '10')\r\n",
					"day = fecha_actual.strftime('%d')  # Día actual (por ejemplo, '24')\r\n",
					"hora = fecha_actual.strftime('%H%M%S')  # Hora actual en formato HHMMSS (por ejemplo, '235959')\r\n",
					"\r\n",
					"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
					"archivo_nombre = f\"Log_{day}_{hora}.json\"\r\n",
					"\r\n",
					"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
					"\r\n",
					"# Crear la ruta de destino con la jerarquía Año/Mes y el nombre del archivo\r\n",
					"destination_blob_name = f\"{year}/{month}/{archivo_nombre}\"\r\n",
					"\r\n",
					"# Crear un cliente del blob de destino\r\n",
					"destination_blob = blob_service_client.get_blob_client(container=silver_logs, blob=destination_blob_name)\r\n",
					"\r\n",
					"destination_blob.upload_blob(json_data, overwrite=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Move files from Pending to Processed or Error**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"results = ''\r\n",
					"error_messages = ''\r\n",
					"\r\n",
					"# Bucle para archivos correctos\r\n",
					"for i in correct_files_variable:\r\n",
					"    source_blob_name = i\r\n",
					"    name_part = i.split('_')[0]\r\n",
					"    \r\n",
					"    # Extraer fecha\r\n",
					"    date_match = re.search(r'_(\\d{8})\\.parquet$', i)\r\n",
					"    if date_match:\r\n",
					"        date_str = date_match.group(1)  # '20241008'\r\n",
					"        year = date_str[:4]  # '2024'\r\n",
					"        month = date_str[4:6]  # '10'\r\n",
					"        destination_blob_name = f\"{name_part}/{year}/{month}/{i}\"\r\n",
					"    else:\r\n",
					"        destination_blob_name = i  # Si no tiene fecha, dejar el nombre como está\r\n",
					"    \r\n",
					"    # Crear los clientes de blob\r\n",
					"    source_blob = blob_service_client.get_blob_client(container=bronze_pending, blob=source_blob_name)\r\n",
					"    destination_blob_processed = blob_service_client.get_blob_client(container=bronze_processed, blob=destination_blob_name)\r\n",
					"    \r\n",
					"    # Iniciar la copia\r\n",
					"    copy_properties = destination_blob_processed.start_copy_from_url(source_blob.url)\r\n",
					"    \r\n",
					"    # Verificar el estado de la copia\r\n",
					"    if copy_properties[\"copy_status\"] == \"success\":\r\n",
					"        source_blob.delete_blob()  # Descomentar para eliminar el blob original\r\n",
					"        results += f\"Archivo {source_blob_name} procesado exitosamente y movido a bronze/Processed. {str(datetime.now())}\\n\"\r\n",
					"    else:\r\n",
					"        error_messages += f\"Error al copiar el archivo {source_blob_name}.\\n\"\r\n",
					"\r\n",
					"# Bucle para archivos incorrectos\r\n",
					"for i in incorrect_files_variable:\r\n",
					"    source_blob_name = i\r\n",
					"    name_part = i.split('_')[0]\r\n",
					"\r\n",
					"    # Extraer fecha\r\n",
					"    date_match = re.search(r'_(\\d{8})\\.parquet$', i)\r\n",
					"    if date_match:\r\n",
					"        date_str = date_match.group(1)  # '20241008'\r\n",
					"        year = date_str[:4]  # '2024'\r\n",
					"        month = date_str[4:6]  # '10'\r\n",
					"        destination_blob_name = f\"{name_part}/{year}/{month}/{i}\"\r\n",
					"    else:\r\n",
					"        destination_blob_name = i  # Si no tiene fecha, dejar el nombre como está\r\n",
					"\r\n",
					"    source_blob = blob_service_client.get_blob_client(container=bronze_pending, blob=source_blob_name)\r\n",
					"    destination_blob_error = blob_service_client.get_blob_client(container=bronze_error, blob=destination_blob_name)\r\n",
					"\r\n",
					"    # Iniciar la copia\r\n",
					"    copy_properties = destination_blob_error.start_copy_from_url(source_blob.url)\r\n",
					"\r\n",
					"    # Verificar el estado de la copia\r\n",
					"    if copy_properties[\"copy_status\"] == \"success\":\r\n",
					"        source_blob.delete_blob()  # Descomentar para eliminar el blob original\r\n",
					"        results += f\"Archivo {source_blob_name} procesado erróneamente y movido a bronze/Landing/Error. {str(datetime.now())} \\n\"\r\n",
					"    else:\r\n",
					"        error_messages += f\"Error al procesar y copiar el archivo {source_blob_name}.\\n\"\r\n",
					"\r\n",
					"# Combina todos los resultados en un solo texto\r\n",
					"final_output = results + error_messages "
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Save logs files**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"fecha_actual = datetime.now()\r\n",
					"year = fecha_actual.strftime('%Y')  # Año actual (por ejemplo, '2024')\r\n",
					"month = fecha_actual.strftime('%m')  # Mes actual (por ejemplo, '10')\r\n",
					"day = fecha_actual.strftime('%d')  # Día actual (por ejemplo, '24')\r\n",
					"hora = fecha_actual.strftime('%H%M%S')  # Hora actual en formato HHMMSS (por ejemplo, '235959')\r\n",
					"\r\n",
					"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
					"output_blob_name = f\"Log_{day}_{hora}.json\"\r\n",
					"output_blob_name = f\"{year}/{month}/{output_blob_name}\"\r\n",
					"#output_blob_name = \"logs.txt\"\r\n",
					"output_blob = blob_service_client.get_blob_client(container=bronze_logs, blob=output_blob_name)\r\n",
					"\r\n",
					"# Subir el contenido al blob\r\n",
					"output_blob.upload_blob(final_output, overwrite=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Update Configuration CSV**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Configurar la conexión\r\n",
					"#delta_date_string = '2024-10-08'\r\n",
					"\r\n",
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
					"\r\n",
					"# Definir los nombres de los contenedores y blobs\r\n",
					"#container_name = \"etl/bronze/Configuration\"\r\n",
					"#metadata_folder = \"mdw/bronze/Configuration\"\r\n",
					"container_name = metadata_folder\r\n",
					"csv_blob_name = \"ConfiguracionOrigenes.csv\"  # Reemplaza con el nombre de tu archivo\r\n",
					"\r\n",
					"# Leer el archivo CSV desde el Data Lake\r\n",
					"blob_client = blob_service_client.get_blob_client(container=container_name, blob=csv_blob_name)\r\n",
					"\r\n",
					"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
					"df = pd.read_csv(stream, sep=',')  # Especifica el separador aquí\r\n",
					"\r\n",
					"# Limpiar nombres de columnas para eliminar espacios en blanco\r\n",
					"df.columns = df.columns.str.strip()\r\n",
					"\r\n",
					"#landing_files = [item['name'] for item in filtered_dimensionsList]\r\n",
					"for i in landing_files:\r\n",
					"    df.loc[df['FileName'] == i[\"name\"][:i[\"name\"].index('_')], 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas según tu archivo CSV\r\n",
					"\r\n",
					"# Guardar el DataFrame modificado en un nuevo flujo de bytes\r\n",
					"output_stream = io.BytesIO()\r\n",
					"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')  # Usar line_terminator para evitar líneas en blanco\r\n",
					"output_stream.seek(0)  # Regresar al inicio del flujo\r\n",
					"\r\n",
					"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
				],
				"execution_count": null
			}
		]
	}
}