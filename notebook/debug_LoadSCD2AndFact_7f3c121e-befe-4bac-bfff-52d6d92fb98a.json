{
	"name": "debug_LoadSCD2AndFact_7f3c121e-befe-4bac-bfff-52d6d92fb98a",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkTFM",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c851b305-0619-4b32-9b9c-ab58a45f8278"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7e84f9f8-73c5-4e82-9296-f2f3c4838394/resourceGroups/RG-TFM_MasterVerne_PGC/providers/Microsoft.Synapse/workspaces/asa-tfm-pgc/bigDataPools/sparkTFM",
				"name": "sparkTFM",
				"type": "Spark",
				"endpoint": "https://asa-tfm-pgc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkTFM",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters_overwritten"
					]
				},
				"source": [
					"# This cell is generated from runtime parameters. Learn more: https://go.microsoft.com/fwlink/?linkid=2161015\n",
					"data_lake_container = \"abfss://mdw@datalake1pgc.dfs.core.windows.net\"\n",
					"gold_folder_logs = \"gold/Logs\"\n",
					"container_name = \"mdw\"\n",
					"metadata_folder = \"mdw/bronze/Configuration\"\n",
					"tables_to_load = \"[{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Articles\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Currency\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Date\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Hours\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"OperationType\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"PostalCode\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Tariff\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"},{\\\"SchemaName\\\":\\\"dim\\\",\\\"TableName\\\":\\\"Warehouse\\\",\\\"UpdateDate\\\":\\\"1900-01-01\\\",\\\"Load\\\":\\\"Y\\\"}]\"\n",
					"golden_folder_dimensions = \"gold/Dimensions\"\n",
					"golden_folder_fact = \"gold/Fact\"\n",
					""
				],
				"execution_count": 73
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Import modules**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from delta.tables import DeltaTable\r\n",
					"from delta.tables import *\r\n",
					"from pyspark.sql.functions import when, lit, current_date, date_format,concat_ws,xxhash64,current_timestamp\r\n",
					"from pyspark.sql import SparkSession,Window\r\n",
					"from pyspark.sql import functions as F\r\n",
					"import json\r\n",
					"import re\r\n",
					"from datetime import datetime\r\n",
					"from azure.storage.blob import BlobServiceClient\r\n",
					"import io\r\n",
					"import pandas as pd\r\n",
					"import csv"
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# %%sql\r\n",
					"# USE default;\r\n",
					"# DROP DATABASE IF EXISTS   gold CASCADE ;\r\n",
					"# USE default;\r\n",
					"# CREATE DATABASE IF NOT EXISTS gold;\r\n",
					"# USE gold;"
				],
				"execution_count": 62
			},
			{
				"cell_type": "code",
				"source": [
					"# %%sql\r\n",
					"# USE gold;\r\n",
					"# DROP TABLE IF EXISTS fact_Sales;"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"# factList = [ \r\n",
					"#         {\r\n",
					"#             \"name\": \"Sales\",\r\n",
					"#             \"query\": \"\"\"\r\n",
					"#                     SELECT  idDesgloseVenta AS idSales,\r\n",
					"#                         d.idArticulo AS idArticles,\r\n",
					"#                         c.idAlmacen AS idWarehouse,\r\n",
					"#                         c.idCliente AS idClient,\r\n",
					"#                         c.idCodigoPostal AS idPostalCode,\r\n",
					"#                         c.idDivisa AS idCurrency,\r\n",
					"#                         c.idTarifa AS idTariff,\r\n",
					"#                         c.idTipoOperacion AS idOperationType,\r\n",
					"#                         c.idHora AS idHours,\r\n",
					"#                         c.idFecha AS idDate,\r\n",
					"#                         f.fecha AS  date,\r\n",
					"#                         CAST(CONCAT(DATE_FORMAT(f.fecha, 'yyyy-MM-dd'), ' ', h.horaCompleta) AS TIMESTAMP) AS dateOp,\r\n",
					"#                         COALESCE(c.codigoTicket, 'D') AS ticketNumber,\r\n",
					"#                         COALESCE(d.Cantidad, 0) AS quantity,\r\n",
					"#                         COALESCE(d.PrecioUnitario, 0)  AS unitPrice,\r\n",
					"#                         COALESCE(d.CosteUnitario, 0)  AS unitCost,\r\n",
					"#                         COALESCE(d.importeBruto, 0)  AS amtTotalEuros,\r\n",
					"#                         COALESCE(d.importeNeto, 0)  AS amtNetEuros,\r\n",
					"#                         COALESCE(d.importeDescuento, 0)  AS amtTotalEurosDiscount,\r\n",
					"#                         COALESCE(d.importeNetoDescuento, 0) AS amtNetEurosDiscount,\r\n",
					"#                         CASE WHEN idTarifa IN (2,3) THEN 1\r\n",
					"#                             ELSE 0 \r\n",
					"#                         END AS discountSale,\r\n",
					"#                         CASE WHEN idTarifa = 0 THEN 0\r\n",
					"#                             ELSE (d.importeBruto - d.importeDescuento) \r\n",
					"#                         END AS amtTotalDiscount,\r\n",
					"#                         CASE WHEN idTarifa = 0 THEN 0 \r\n",
					"#                             ELSE (d.importeNeto - d.importeNetoDescuento) \r\n",
					"#                         END AS amtNetDiscount,\r\n",
					"#                         GREATEST(d.fechaCarga, c.fechaCarga,) AS loadDate,\r\n",
					"#                         GREATEST(d.fechaDelta, c.fechaDelta,) AS deltaDate\r\n",
					"#                     FROM silver.desgloseVenta AS d\r\n",
					"#                     INNER JOIN silver.cabeceraVenta AS c\r\n",
					"#                         ON d.idcabecerasVentas = c.idcabeceraVenta\r\n",
					"#                     LEFT JOIN silver.Fechas  AS f\r\n",
					"#                         On f.idFechas = c.idFecha\r\n",
					"#                     LEFT JOIN silver.horas  AS h\r\n",
					"#                         On h.idHoras = c.idHora\r\n",
					"#                     WHERE GREATEST(d.fechaDelta, c.fechaDelta) >=\r\n",
					"#                      \"\"\"\r\n",
					"#         }\r\n",
					"#     ]"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"dimensionsList = [ \r\n",
					"        {\r\n",
					"            \"name\": \"Articles\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idArticulos                                                                                             AS idArticles,\r\n",
					"                            COALESCE(nombre, 'D')                                                                                   AS name,\r\n",
					"                            COALESCE(descripcion, 'D')                                                                              AS description,\r\n",
					"                            COALESCE(codigoReferencia, 'D')                                                                         AS externalCode,\r\n",
					"                            COALESCE(t.talla, 'D')                                                                                  AS size,\r\n",
					"                            COALESCE( t.numeroTalla, -1)                                                                            AS numSize,\r\n",
					"                            COALESCE(co.color, 'D')                                                                                 AS colour,\r\n",
					"                            COALESCE(ca.categoria, 'D')                                                                             AS category,\r\n",
					"                            COALESCE(l.codigoLinea, 'D')                                                                            AS codLine,\r\n",
					"                            COALESCE(l.Linea, 'D')                                                                                  AS line, \r\n",
					"                            CASE WHEN a.idCategoria IN (1,3,5,8,10) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
					"                                 WHEN a.idCategoria IN (2,4,6,7,9) THEN CAST(CONCAT('OI',CAST(a.idTemporada AS string)) AS string)\r\n",
					"                                 ELSE CAST(CONCAT('PV',CAST(a.idTemporada AS string)) AS string)\r\n",
					"                            END                                                                                                     AS season,              \r\n",
					"                            GREATEST(a.fechaCarga, t.fechaCarga, co.fechaCarga, ca.fechaCarga, l.fechaCarga)                        AS loadDate,\r\n",
					"                            GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta)                        AS deltaDate\r\n",
					"                    FROM silver.articulos AS a\r\n",
					"                        LEFT JOIN silver.talla AS t\r\n",
					"                            ON t.idTalla = a.idTalla\r\n",
					"                        LEFT JOIN silver.color AS co\r\n",
					"                            ON co.idColor = a.idColor\r\n",
					"                        LEFT JOIN silver.categoria AS ca -- select * from silver.categoria\r\n",
					"                            ON ca.idCategoria = a.idCategoria\r\n",
					"                        LEFT JOIN silver.Linea AS l\r\n",
					"                            ON l.idLinea = a.idLinea\r\n",
					"                    WHERE GREATEST(a.fechaDelta, t.fechaDelta, co.fechaDelta, ca.fechaDelta, l.fechaDelta) >= \r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        # {\r\n",
					"        #     \"name\": \"Client\",\r\n",
					"        #     \"query\": \"\"\"              \r\n",
					"        #             SELECT  idCliente                                           AS idClient,\r\n",
					"        #                     COALESCE(c.nombre, 'D')                             AS name,\r\n",
					"        #                     COALESCE(apellido1, 'D')                            AS lastName1,\r\n",
					"        #                     COALESCE(apellido2, 'D')                            AS lastName2, \r\n",
					"        #                     COALESCE(email, 'D')                                AS email,\r\n",
					"        #                     COALESCE(telefono, 'D')                             AS phoneNumber,\r\n",
					"        #                     COALESCE(CAST(cumpleanos AS STRING), '1900-01-01')  AS birthDay,       \r\n",
					"        #                     YEAR(current_date()) - YEAR(CAST(cumpleanos AS DATE)) \r\n",
					"        #                         - CASE \r\n",
					"        #                             WHEN MONTH(current_date()) < MONTH(CAST(cumpleanos AS DATE)) \r\n",
					"        #                                 OR (MONTH(current_date()) = MONTH(CAST(cumpleanos AS DATE)) AND DAY(current_date()) < DAY(CAST(cumpleanos AS DATE)))\r\n",
					"        #                             THEN 1\r\n",
					"        #                             ELSE 0\r\n",
					"        #                     END                                                 AS age,\r\n",
					"        #                     CASE WHEN hombre = 1 THEN 'Hombre' \r\n",
					"        #                          ELSE 'Mujer' \r\n",
					"        #                     END                                                 AS gender,\r\n",
					"        #                     COALESCE(p.nombre, 'D')                             AS country,\r\n",
					"        #                     COALESCE(p.codigoPais, 'D')                         AS countryCode,\r\n",
					"        #                     COALESCE(cp.region, 'D')                            AS region,\r\n",
					"        #                     COALESCE(c.Direcion, 'D')                           AS address,  \r\n",
					"        #                     COALESCE(codigoPostal, 'D')                         AS postalCode,\r\n",
					"        #                     COALESCE(activo, false)                             AS active,  -- Cambiado 0 a false aquí\r\n",
					"        #                     GREATEST(c.fechaCarga, p.fechaCarga, cp.fechaCarga) AS loadDate,\r\n",
					"        #                     GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) AS deltaDate\r\n",
					"        #                 FROM silver.cliente AS c\r\n",
					"        #                     LEFT JOIN silver.pais AS p \r\n",
					"        #                         ON c.idPais = p.idPais\r\n",
					"        #                     INNER JOIN silver.codigoPostal AS cp \r\n",
					"        #                         ON cp.idCodigoPostal = c.idCodigoPostal AND cp.codigoPais = p.codigoPais\r\n",
					"        #                 WHERE GREATEST(c.fechaDelta, p.fechaDelta, cp.fechaDelta) >= \r\n",
					"        #              \"\"\"\r\n",
					"        # }\r\n",
					"        #,\r\n",
					"        {\r\n",
					"            \"name\": \"Currency\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idDivisa              AS idCurrency,\r\n",
					"                            COALESCE(nombre, 'D') AS name,\r\n",
					"                            COALESCE(Divisa, 'D') AS currency,\r\n",
					"                            fechaCarga            AS loadDate,\r\n",
					"                            fechaDelta            AS deltaDate\r\n",
					"                    FROM silver.divisa\r\n",
					"                    WHERE fechaDelta >=\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Date\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idFechas                                  AS idDate,\r\n",
					"                            ClaveFecha                                AS dateKey,\r\n",
					"                            Fecha                                     AS date,\r\n",
					"                            COALESCE(Mes, 'D')                        AS month,\r\n",
					"                            COALESCE(NumeroMes, -1)                   AS monthNumber,\r\n",
					"                            COALESCE(Ano, -1)                         AS year,\r\n",
					"                            COALESCE(NumeroSemana, -1)                AS weekNumber,\r\n",
					"                            COALESCE(DiaSemana, 'D')                  AS dayWeek,\r\n",
					"                            COALESCE(NumeroDiaSemana, -1)             AS dayWeekNumber,\r\n",
					"                            COALESCE(DiaAno, -1)                      AS yearDay,\r\n",
					"                            CASE WHEN Trimestre = 1 THEN 'Q1'\r\n",
					"                                WHEN Trimestre = 2 THEN 'Q2'\r\n",
					"                                WHEN Trimestre = 3 THEN 'Q3'\r\n",
					"                                ELSE '-1' END                         AS quarter,\r\n",
					"                            CASE WHEN Cuatrimestre = 1 THEN 'Q1'\r\n",
					"                                WHEN Cuatrimestre = 2 THEN 'Q2'\r\n",
					"                                WHEN Cuatrimestre = 3 THEN 'Q3'\r\n",
					"                                WHEN Cuatrimestre = 4 THEN 'Q4'\r\n",
					"                                ELSE '-1' \r\n",
					"                            END                                       AS quadrimester,\r\n",
					"                            CASE WHEN Cuatrimestre IN (1,2) THEN 'S1'\r\n",
					"                                WHEN Cuatrimestre IN (2,4) THEN 'S2'\r\n",
					"                                ELSE '-1' \r\n",
					"                            END                                       AS semester,\r\n",
					"                            fechaCarga                                AS loadDate,\r\n",
					"                            fechaDelta                                AS deltaDate\r\n",
					"                    FROM silver.fechas\r\n",
					"                    WHERE fechaDelta >=\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Hours\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idHoras                     AS idHours,\r\n",
					"                            COALESCE(Hora, -1)          AS hour,\r\n",
					"                            COALESCE(Minuto, -1)        AS minute,\r\n",
					"                            COALESCE(HoraCompleta, 'D') AS fullHour,\r\n",
					"                            fechaCarga                  AS loadDate,\r\n",
					"                            fechaDelta                  AS deltaDate\r\n",
					"                    FROM silver.horas\r\n",
					"                    WHERE fechaDelta >=\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"OperationType\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idTipoOperacion          AS idOperationType,\r\n",
					"                            COALESCE(Operacion, 'D') AS operation,\r\n",
					"                            fechaCarga               AS loadDate,\r\n",
					"                            fechaDelta               AS deltaDate\r\n",
					"                    FROM silver.tipoOperacion\r\n",
					"                    WHERE fechaDelta >=\r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"PostalCode\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idCodigoPostal                       AS idPostalCode,\r\n",
					"                            COALESCE(codigoPostal, 'D')          AS postalCode,\r\n",
					"                            COALESCE(region, 'D')                AS region,\r\n",
					"                            COALESCE(c.codigoPais, 'D')          AS countryCode,\r\n",
					"                            COALESCE(nombre, 'D')                AS country,\r\n",
					"                            GREATEST(c.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
					"                            GREATEST(c.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
					"                    FROM silver.codigoPostal AS c\r\n",
					"                    LEFT JOIN silver.pais AS p\r\n",
					"                        ON p.codigoPais = c.codigoPais\r\n",
					"                    WHERE GREATEST(c.fechaDelta, p.fechaDelta) >= \r\n",
					"                     \"\"\"\r\n",
					"        },\r\n",
					"        {\r\n",
					"            \"name\": \"Tariff\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idTarifa              AS idTariff,\r\n",
					"                            COALESCE(Tarifa, 'D') AS tariff,\r\n",
					"                            fechaCarga            AS loadDate,\r\n",
					"                            fechaDelta            AS deltaDate       \r\n",
					"                    FROM silver.tarifa\r\n",
					"                    WHERE fechaDelta >= \r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"        ,\r\n",
					"        {\r\n",
					"            \"name\": \"Warehouse\",\r\n",
					"            \"query\": \"\"\"\r\n",
					"                    SELECT  idAlmacenes                          AS idWarehouse,\r\n",
					"                            COALESCE(a.Nombre, 'D')              AS warehouse,\r\n",
					"                            COALESCE(a.codigoAlmacen, 'D')       AS externalCode,\r\n",
					"                            COALESCE(p.codigoPais, 'D')          AS countryCode, \r\n",
					"                            COALESCE(p.nombre, 'D')              AS country,\r\n",
					"                            COALESCE(a.ciudad, 'D')              AS city,\r\n",
					"                            COALESCE(a.Direccion, 'D')           AS address,\r\n",
					"                            COALESCE(a.descripcion, 'D')         AS description,\r\n",
					"                            GREATEST(a.fechaCarga, p.fechaCarga) AS loadDate,\r\n",
					"                            GREATEST(a.fechaDelta, p.fechaDelta) AS deltaDate\r\n",
					"                    FROM silver.almacenes AS a\r\n",
					"                    LEFT JOIN silver.pais AS p\r\n",
					"                        ON p.idpais = a.idPais\r\n",
					"                    --WHERE a.fechaDelta <> 20241011\r\n",
					"                    --UNION \r\n",
					"                    --select 4,'Almacen Este','IT8','ES','Spain','Valencia','Calle Este 321','Almacén especializado en logística',20241010,20241010\r\n",
					"                    --UNION \r\n",
					"                    --select 20,'Almacen Sureste','I21','ES','Spain','Badajoz','Calle Alfaro','Almacén especializado',20241011,20241011\r\n",
					"                    WHERE GREATEST(a.fechaDelta, p.fechaDelta) >= \r\n",
					"                     \"\"\"\r\n",
					"        }\r\n",
					"    ]"
				],
				"execution_count": 66
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Configuration**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"factList = []"
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"source": [
					"# parameter string to array\r\n",
					"tables_to_load = json.loads(tables_to_load)\r\n",
					"\r\n",
					"today_datetime = datetime.now()\r\n",
					"\r\n",
					"delta_date_filter = (today_datetime.strftime('%Y%m%d')) # 'yyyyMMdd'\r\n",
					"#delta_date_filter = '20241008'\r\n",
					"\r\n",
					"delta_date_string = today_datetime.strftime('%Y-%m-%d') # 'yyyy-mm-dd'\r\n",
					"#delta_date_string = '2024-10-08'\r\n",
					"\r\n",
					"# Convertir cada valor del CSV en un diccionario estructurado\r\n",
					"dict_tables_to_load = []\r\n",
					"\r\n",
					"for row in tables_to_load:\r\n",
					"    #csv_data = row[\"SchemaName,TableName,UpdateDate,Load\"]\r\n",
					"    \r\n",
					"    # Usar el lector de csv para separar los campos\r\n",
					"    #reader = csv.reader(io.StringIO(csv_data))\r\n",
					"    \r\n",
					"    # for schema, table, update_date, load in reader:\r\n",
					"    #     dict_tables_to_load.append({\r\n",
					"    #         \"SchemaName\": schema,\r\n",
					"    #         \"TableName\": table,\r\n",
					"    #         \"UpdateDate\": update_date,\r\n",
					"    #         \"Load\": load\r\n",
					"    #     })\r\n",
					"    dict_tables_to_load.append({\r\n",
					"    \"SchemaName\": row[\"SchemaName\"],\r\n",
					"    \"TableName\": row[\"TableName\"],\r\n",
					"    \"UpdateDate\": row[\"UpdateDate\"],\r\n",
					"    \"Load\": row[\"Load\"]\r\n",
					"})\r\n",
					"\r\n",
					"# Variables para almacenar los datos de 'dim' y 'fact'\r\n",
					"dim_tables = []\r\n",
					"fact_tables = []\r\n",
					"\r\n",
					"# Clasificar los datos\r\n",
					"for row in dict_tables_to_load:\r\n",
					"    if row['SchemaName'] == 'dim':\r\n",
					"        dim_tables.append(row)\r\n",
					"    elif row['SchemaName'] == 'fact':\r\n",
					"        fact_tables.append(row)\r\n",
					"\r\n",
					"\r\n",
					"load_y_tables = {table['TableName'] for table in dim_tables if table['Load'] == 'Y'}\r\n",
					"# Filtrar el dimensionsList para obtener solo las tablas con Load='Y' y devolver una lista de diccionarios\r\n",
					"filtered_dimensionsList = [\r\n",
					"    dim for dim in dimensionsList if dim[\"name\"] in load_y_tables\r\n",
					"]\r\n",
					"\r\n",
					"load_y_tables2 = {table['TableName'] for table in fact_tables if table['Load'] == 'Y'}\r\n",
					"#print(load_y_tables2)\r\n",
					"# Filtrar el dimensionsList para obtener solo las tablas con Load='Y' y devolver una lista de diccionarios\r\n",
					"filtered_factList = [\r\n",
					"    dim for dim in factList if dim[\"name\"] in load_y_tables2\r\n",
					"]"
				],
				"execution_count": 68
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Load SCD2 Dimensions**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Carga o Merge Dimensiones\r\n",
					"results = []\r\n",
					"exception = []\r\n",
					"\r\n",
					"for file in dimensionsList:\r\n",
					"    file_result = {\r\n",
					"        'table': f\"fact_{file['name']}\",\r\n",
					"        'status': 'Incorrect',\r\n",
					"        'inserted_rows': 0,\r\n",
					"        'updated_rows': 0,\r\n",
					"        'exception': 'N/A',\r\n",
					"        'datetime': str(datetime.now()) \r\n",
					"    }\r\n",
					"\r\n",
					"    try:\r\n",
					"\r\n",
					"        table_name = file[\"name\"].split('_')[0]\r\n",
					"        source_wildcard = f\"{table_name}*.parquet\"   # Concatenar la subcadena\r\n",
					"        key_columns_str = f\"id{table_name.capitalize()}\" \r\n",
					"\r\n",
					"        key_columns = key_columns_str.split(',')\r\n",
					"        conditions_list = [f\"existing.{key}=updates.{key}\" for key in key_columns]\r\n",
					"\r\n",
					"        #delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Dimensions/{table_name}\"\r\n",
					"        delta_table_path_dimensions = f\"{data_lake_container}/{golden_folder_dimensions}/{table_name}\"\r\n",
					"        # Leer archivo(s) en DataFrame de Spark\r\n",
					"        # Meter filtro deltaDate\r\n",
					"\r\n",
					"        df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
					"        sdf = df.cache()\r\n",
					"\r\n",
					"        if sdf.limit(1).count() == 0:\r\n",
					"            print(f\"No se procesará el archivo {file['name']} porque no contiene datos.\")\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            file_result['exception'] = \"There is no new data\"\r\n",
					"            results.append(file_result)\r\n",
					"            continue\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"        \r\n",
					"        if DeltaTable.isDeltaTable(spark, delta_table_path_dimensions):\r\n",
					"            # Añadir las columnas necesarias\r\n",
					"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
					"\r\n",
					"            columns_to_hash = [col for col in sdf.columns if col not in ('fechaCarga', 'fromDate', 'toDate', 'isCurrent')]\r\n",
					"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
					"\r\n",
					"            todas_las_columnas = sdf.columns\r\n",
					"            columnas_al_principio = [f\"id{table_name}\"]\r\n",
					"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
					"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
					"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
					"            sdf = sdf.select(*nuevo_orden_columnas)\r\n",
					"\r\n",
					"\r\n",
					"            sdf.createOrReplaceTempView(\"temp_view\")\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                MERGE INTO gold.dim_{table_name}  \r\n",
					"                AS existing\r\n",
					"                USING temp_view AS updates\r\n",
					"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
					"                WHEN MATCHED AND existing.isCurrent = 1 AND existing.hash != updates.hash THEN\r\n",
					"                UPDATE SET\r\n",
					"                    existing.toDate = current_date(),\r\n",
					"                    existing.isCurrent = 0\r\n",
					"            \"\"\")\r\n",
					"\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                INSERT INTO gold.dim_{table_name}    \r\n",
					"                SELECT \r\n",
					"                    {next_surrogate_key} + row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS sk{table_name}, \r\n",
					"                    updates.*\r\n",
					"                FROM temp_view AS updates\r\n",
					"                LEFT JOIN gold.dim_{table_name}  \r\n",
					"                AS existing\r\n",
					"                ON {\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])}\r\n",
					"                WHERE existing.{key_columns_str} IS NULL OR existing.isCurrent = 0 OR existing.hash != updates.hash\r\n",
					"            \"\"\")\r\n",
					"\r\n",
					"            # Almacenar conteos de filas\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            history_df = spark.sql(f\"DESCRIBE HISTORY gold.dim_{table_name}\")\r\n",
					"            last_merge_operation = history_df.filter(history_df.operation == 'MERGE').orderBy(F.desc(\"timestamp\")).limit(1)\r\n",
					"            last_result = last_merge_operation.collect()\r\n",
					"\r\n",
					"            file_result['inserted_rows'] = last_result[0].operationMetrics[\"numTargetRowsInserted\"]\r\n",
					"            file_result['updated_rows'] = last_result[0].operationMetrics[\"numTargetRowsUpdated\"]\r\n",
					"\r\n",
					"\r\n",
					"        else: # Crear nueva tabla Delta\r\n",
					"      \r\n",
					"            last_surrogate_key = 0  # Iniciar con 0 si la tabla no existe\r\n",
					"            #sdf = sdf.withColumn(f\"sk{table_name}\", F.monotonically_increasing_id() + last_surrogate_key + 1)  # Asigna un valor único a cada fila.\r\n",
					"            sdf = sdf.withColumn(f\"sk{table_name}\", F.col(f\"id{table_name}\"))\r\n",
					"            # Añadir las columnas necesarias\r\n",
					"            sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit('1990-01-01 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss'))\r\n",
					"            sdf = sdf.withColumn(\"isCurrent\", F.lit(1))\r\n",
					"\r\n",
					"            # Hash\r\n",
					"            columns_to_hash = [col for col in sdf.columns if col not in (f\"sk{table_name}\" ,'fechaCarga', 'fechaDelta','fromDate', 'toDate', 'isCurrent')]\r\n",
					"            sdf = sdf.withColumn(\"hash\", xxhash64(concat_ws(\"||\", *columns_to_hash)))\r\n",
					"\r\n",
					"            # Reorganizar la estructura de la tabla\r\n",
					"            todas_las_columnas = sdf.columns\r\n",
					"            columnas_al_principio = [f\"sk{table_name}\", f\"id{table_name}\"]\r\n",
					"            columnas_al_final = [\"fromDate\", \"toDate\", \"isCurrent\", \"loadDate\", \"deltaDate\", \"hash\"]\r\n",
					"            otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio + columnas_al_final]\r\n",
					"            nuevo_orden_columnas = columnas_al_principio + otras_columnas + columnas_al_final\r\n",
					"            sdf_reordenado = sdf.select(*nuevo_orden_columnas)\r\n",
					"            \r\n",
					"            # Crear la tabla Delta\r\n",
					"            sdf_reordenado.write.format('delta').partitionBy(\"isCurrent\").save(delta_table_path_dimensions)\r\n",
					"\r\n",
					"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.dim_{table_name} USING DELTA LOCATION \\'{delta_table_path_dimensions}\\'')\r\n",
					"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
					"            #spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
					"            spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
					"\r\n",
					"            #spark.sql(f\"ALTER TABLE gold.dim_{table_name} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')\")\r\n",
					"\r\n",
					"            # Almacenar archivo procesado correctamente\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
					"            file_result['updated_rows'] = 0\r\n",
					"\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
					"        file_result['exception'] = f\"{e}\"\r\n",
					"        results.append(file_result)\r\n",
					"        continue\r\n",
					"\r\n",
					"    results.append(file_result)"
				],
				"execution_count": 69
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Load Fact Table**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Carga FULL tabla/s de hechos\r\n",
					"\r\n",
					"# Inicializar listas para almacenar resultados\r\n",
					"# results = []\r\n",
					"# exception = []\r\n",
					"\r\n",
					"\r\n",
					"for file in factList:\r\n",
					"    file_result = {\r\n",
					"        'table': f\"fact_{file['name']}\",\r\n",
					"        'status': 'Incorrect',\r\n",
					"        'inserted_rows': 0,\r\n",
					"        'updated_rows': 0,\r\n",
					"        'exception': 'N/A',\r\n",
					"        'datetime': str(datetime.now())  \r\n",
					"    }\r\n",
					"    table_name = file[\"name\"].split('_')[0]\r\n",
					"\r\n",
					"    # Definir la ruta de la tabla Delta\r\n",
					"    delta_table_path_fact = f\"{data_lake_container}/{golden_folder_fact}/{table_name}\"\r\n",
					"    #delta_table_path = f\"abfss://mdw@datalake1pgc.dfs.core.windows.net/gold/Fact/{table_name}\"\r\n",
					"    #file[\"query\"] = file[\"query\"].strip() + f\" '{delta_date_filter}'\"\r\n",
					"    #df = spark.sql(file[\"query\"]) \r\n",
					"    df = spark.sql(file[\"query\"].strip() + f\" '{delta_date_filter}'\") \r\n",
					"    sdf = df.cache()\r\n",
					"\r\n",
					"    try:\r\n",
					"        # Leer el archivo CSV en un DataFrame de Spark\r\n",
					"        #sdf = spark.read.csv(f\"gs://your-bucket/{file['name']}\", header=True, inferSchema=True)\r\n",
					"\r\n",
					"        # Verificar si el DataFrame está vacío\r\n",
					"        if sdf.limit(1).count() == 0:\r\n",
					"            print(f\"No se procesará el archivo {file['name']} porque no contiene datos.\")\r\n",
					"            file_result['status'] = 'Correct'\r\n",
					"            file_result['exception'] = \"There is no new data\"\r\n",
					"            results.append(file_result)\r\n",
					"            continue\r\n",
					"\r\n",
					"        # Añadir columnas necesarias\r\n",
					"        # sdf = sdf.withColumn(\"fromDate\", F.to_timestamp(F.lit(current_date()), 'yyyy-MM-dd HH:mm:ss')) \\\r\n",
					"        #          .withColumn(\"toDate\", F.to_timestamp(F.lit('9999-12-31 00:00:00'), 'yyyy-MM-dd HH:mm:ss')) \\\r\n",
					"        #          .withColumn(\"isCurrent\", F.lit(1))\r\n",
					"\r\n",
					"        # Truncar la tabla Delta si existe\r\n",
					"        if DeltaTable.isDeltaTable(spark, delta_table_path_fact):\r\n",
					"            delta_table = DeltaTable.forPath(spark, delta_table_path_fact)\r\n",
					"            delta_table.delete()  # Truncar la tabla Delta\r\n",
					"\r\n",
					"            #last_row number para incremental\r\n",
					"\r\n",
					"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
					"            spark.sql(f\"\"\"\r\n",
					"                INSERT INTO gold.fact_{table_name}  \r\n",
					"                SELECT ROW_NUMBER() OVER(ORDER BY s.dateOP) AS skSales, s.*, \r\n",
					"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
					"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
					"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
					"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
					"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
					"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
					"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
					"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
					"                        COALESCE(d.skDate, -1) AS skDate\r\n",
					"                FROM temp_sales_view as s\r\n",
					"                LEFT JOIN gold.dim_Articles AS a\r\n",
					"                    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
					"                LEFT JOIN gold.dim_Client AS cl\r\n",
					"                    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
					"                LEFT JOIN gold.dim_Currency AS cu\r\n",
					"                    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
					"                LEFT JOIN gold.dim_Date AS d\r\n",
					"                    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
					"                LEFT JOIN gold.dim_Hours AS h\r\n",
					"                    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
					"                LEFT JOIN gold.dim_OperationType AS o\r\n",
					"                    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
					"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
					"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
					"                LEFT JOIN gold.dim_Tariff AS t\r\n",
					"                    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
					"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
					"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
					"                \"\"\")\r\n",
					"        else:\r\n",
					"\r\n",
					"            #window_spec = Window.orderBy(\"dateOp\", \"idArticles\")\r\n",
					"            #sdf = sdf.withColumn(f\"sk{table_name}\", F.row_number().over(window_spec))\r\n",
					"            #todas_las_columnas = sdf.columns\r\n",
					"            #columnas_al_principio = [f\"sk{table_name}\"]  # Suponiendo que la tabla tiene una columna idWarehouse\r\n",
					"            #otras_columnas = [col for col in todas_las_columnas if col not in columnas_al_principio]\r\n",
					"            #nuevo_orden_columnas = columnas_al_principio + otras_columnas\r\n",
					"            #sdf = sdf.select(*nuevo_orden_columnas)\r\n",
					"\r\n",
					"            sdf.createOrReplaceTempView(\"temp_sales_view\")\r\n",
					"            sdf = spark.sql(f\"\"\" \r\n",
					"                SELECT ROW_NUMBER() OVER(ORDER BY s.dateOP) AS skSales, s.*, \r\n",
					"                        COALESCE(a.skArticles, -1) AS skArticles,\r\n",
					"                        COALESCE(w.skWarehouse, -1) AS skWarehouse,\r\n",
					"                        COALESCE(cl.skClient, -1) AS skClient, \r\n",
					"                        COALESCE(p.skPostalCode, -1) AS skPostalCode, \r\n",
					"                        COALESCE(cu.skCurrency, -1) AS skCurrency,\r\n",
					"                        COALESCE(t.skTariff, -1) AS skTariff, \r\n",
					"                        COALESCE(o.skOperationType, -1) AS skOperationType,                       \r\n",
					"                        COALESCE(h.skHours, -1) AS skHours,                     \r\n",
					"                        COALESCE(d.skDate, -1) AS skDate\r\n",
					"                FROM temp_sales_view AS s\r\n",
					"                LEFT JOIN gold.dim_Articles AS a\r\n",
					"                    ON s.idArticles = a.idArticles AND s.dateOp BETWEEN a.fromDate AND a.toDate\r\n",
					"                LEFT JOIN gold.dim_Client AS cl\r\n",
					"                    ON s.idClient = cl.idClient AND s.dateOp BETWEEN cl.fromDate AND cl.toDate\r\n",
					"                LEFT JOIN gold.dim_Currency AS cu\r\n",
					"                    ON s.idCurrency = cu.idCurrency AND s.dateOp BETWEEN cu.fromDate AND cu.toDate\r\n",
					"                LEFT JOIN gold.dim_Date AS d\r\n",
					"                    ON s.idDate = d.idDate AND s.dateOp BETWEEN d.fromDate AND d.toDate\r\n",
					"                LEFT JOIN gold.dim_Hours AS h\r\n",
					"                    ON s.idHours = h.idHours AND s.dateOp BETWEEN h.fromDate AND h.toDate\r\n",
					"                LEFT JOIN gold.dim_OperationType AS o\r\n",
					"                    ON s.idOperationType = o.idOperationType AND s.dateOp BETWEEN o.fromDate AND o.toDate\r\n",
					"                LEFT JOIN gold.dim_PostalCode AS p\r\n",
					"                    ON s.idPostalCode = p.idPostalCode AND s.dateOp BETWEEN p.fromDate AND p.toDate\r\n",
					"                LEFT JOIN gold.dim_Tariff AS t\r\n",
					"                    ON s.idTariff = t.idTariff AND s.dateOp BETWEEN t.fromDate AND t.toDate\r\n",
					"                LEFT JOIN gold.dim_Warehouse AS w\r\n",
					"                    ON s.idWarehouse = w.idWarehouse AND s.dateOp BETWEEN w.fromDate AND w.toDate\r\n",
					"                \"\"\")\r\n",
					"\r\n",
					"        # Cargar datos en la tabla Delta\r\n",
					"            sdf.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_fact)\r\n",
					"            #sdf.write.format(\"delta\").mode(\"append\").save(delta_table_path)  # valorar carga incremental más adelante\r\n",
					"\r\n",
					"            # Crear la tabla Delta en caso de que no exista\r\n",
					"            spark.sql(f'CREATE TABLE IF NOT EXISTS gold.fact_{table_name} USING DELTA LOCATION \\'{delta_table_path_fact}\\'')\r\n",
					"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\")\r\n",
					"            spark.sql(f\"ALTER TABLE gold.fact_{table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')\")\r\n",
					"            \r\n",
					"        # Almacenar conteos de filas\r\n",
					"        file_result['status'] = 'Correct'\r\n",
					"        file_result['inserted_rows'] = sdf.count()  # Contar todas las filas insertadas\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        print(f\"Error procesando el archivo {file['name']}: {e}\")\r\n",
					"        file_result['exception'] = f\"{e}\"\r\n",
					"        results.append(file_result)\r\n",
					"        continue\r\n",
					"\r\n",
					"    results.append(file_result)"
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Guardar ficheros Logs**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Obtener la fecha actual\r\n",
					"fecha_actual = datetime.now()\r\n",
					"year = fecha_actual.strftime('%Y')  # Año actual (por ejemplo, '2024')\r\n",
					"month = fecha_actual.strftime('%m')  # Mes actual (por ejemplo, '10')\r\n",
					"day = fecha_actual.strftime('%d')  # Día actual (por ejemplo, '24')\r\n",
					"hora = fecha_actual.strftime('%H%M%S')  # Hora actual en formato HHMMSS (por ejemplo, '235959')\r\n",
					"\r\n",
					"# Crear el nombre del archivo con el formato Log_<fecha>.json\r\n",
					"archivo_nombre = f\"Log_{day}_{hora}.json\"\r\n",
					"\r\n",
					"# Reemplaza con tu cadena de conexión a Azure\r\n",
					"\r\n",
					"# Cliente de servicio de blobs\r\n",
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
					"\r\n",
					"# Convetimos el diccionario a formato json\r\n",
					"json_data = json.dumps(results, indent=4, ensure_ascii=False)\r\n",
					"\r\n",
					"# Crear la ruta de destino con la jerarquía Año/Mes y el nombre del archivo\r\n",
					"destination_blob_name = f\"{gold_folder_logs}/{year}/{month}/{archivo_nombre}\"\r\n",
					"\r\n",
					"# Crear un cliente del blob de destino\r\n",
					"destination_blob = blob_service_client.get_blob_client(container=container_name, blob=destination_blob_name)\r\n",
					"\r\n",
					"destination_blob.upload_blob(json_data, overwrite=True)"
				],
				"execution_count": 74
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Actualizar fichero metadata**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Configurar la conexión\r\n",
					"\r\n",
					"#delta_date_string = '1900-01-01'\r\n",
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
					"\r\n",
					"# Definir los nombres de los contenedores y blobs\r\n",
					"#container_name = \"etl/bronze/Configuration\"\r\n",
					"#metadata_folder = \"mdw/bronze/Configuration\"\r\n",
					"#container_name = metadata_folder\r\n",
					"csv_blob_name = \"ConfiguracionReporting.csv\"  # Reemplaza con el nombre de tu archivo\r\n",
					"\r\n",
					"# Leer el archivo CSV desde el Data Lake\r\n",
					"blob_client = blob_service_client.get_blob_client(container=metadata_folder, blob=csv_blob_name)\r\n",
					"\r\n",
					"stream = io.BytesIO(blob_client.download_blob().readall())\r\n",
					"df = pd.read_csv(stream, sep=',')  # Especifica el separador aquí\r\n",
					"\r\n",
					"# Limpiar nombres de columnas para eliminar espacios en blanco\r\n",
					"df.columns = df.columns.str.strip()\r\n",
					"\r\n",
					"names_dimensionsList = [item['name'] for item in filtered_dimensionsList]\r\n",
					"for i in names_dimensionsList:\r\n",
					"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas según tu archivo CSV\r\n",
					"\r\n",
					"names_factList = [item['name'] for item in filtered_factList]\r\n",
					"for i in names_factList:\r\n",
					"    df.loc[df['TableName'] == i, 'UpdateDate'] = f'{delta_date_string}'  # Cambiar los nombres de las columnas según tu archivo CSV\r\n",
					"\r\n",
					"\r\n",
					"# Guardar el DataFrame modificado en un nuevo flujo de bytes\r\n",
					"output_stream = io.BytesIO()\r\n",
					"df.to_csv(output_stream, index=False, sep=',', lineterminator='\\n')  # Usar line_terminator para evitar líneas en blanco\r\n",
					"output_stream.seek(0)  # Regresar al inicio del flujo\r\n",
					"\r\n",
					"blob_client.upload_blob(output_stream.getvalue(), overwrite=True)"
				],
				"execution_count": 75
			},
			{
				"cell_type": "code",
				"source": [
					"        # # Comprobar si la tabla Delta existe\r\n",
					"        # if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
					"        #     delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
					"        #     existing_columns = delta_table.toDF().columns\r\n",
					"            \r\n",
					"        #     # Obtener la última surrogate key para crear nuevas\r\n",
					"        #     last_surrogate_key = delta_table.toDF().agg(F.max(f\"sk{table_name}\")).collect()[0][0] or 0\r\n",
					"        #     next_surrogate_key = last_surrogate_key + 1\r\n",
					"            \r\n",
					"        #     # Merge new data into existing table con lógica SCD Tipo 2\r\n",
					"        #     delta_table.alias(\"existing\").merge(\r\n",
					"        #         source=sdf.alias(\"updates\"),\r\n",
					"        #         condition=\" AND \".join([f\"existing.{key} = updates.{key}\" for key in key_columns])\r\n",
					"        #     ).whenMatchedUpdate(\r\n",
					"        #         condition=\"existing.isCurrent = 1 AND existing.hash != updates.hash\",\r\n",
					"        #         set={\r\n",
					"        #             \"toDate\": F.current_date(),\r\n",
					"        #             \"isCurrent\": F.lit(0)\r\n",
					"        #         }\r\n",
					"        #     ).whenNotMatchedInsert(\r\n",
					"        #         values={\r\n",
					"        #             f\"sk{table_name}\": F.expr(f\"{next_surrogate_key} + row_number() over (order by (select null)) - 1\"),  # Aumenta la surrogate key en base a la fila\r\n",
					"        #             **{f\"updates.{col}\": f\"updates.{col}\" for col in existing_columns}\r\n",
					"        #         }\r\n",
					"        #     ).execute()\r\n",
					""
				],
				"execution_count": null
			}
		]
	}
}